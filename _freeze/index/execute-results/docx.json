{
  "hash": "12cdc4e9c13bbc54b71614d27d14aab2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Unravelling mental representations in aphantasia through unsupervised alignment\nsubtitle: Project design and study simulation\nauthor: Maël Delem\nabstract-title: Summary\nabstract: |\n  Research on aphantasia is confronted with a long-standing conundrum of all research on consciousness and representations, namely the theoretical inaccessibility of subjective representations. Drawing on concepts from similarity and representation research, I endorse the view that the study of an individual’s mental representations is made possible by exploiting second-order isomorphism. The concept of second-order isomorphism means that correspondence should not be sought in the first-order relation between (a) an external object and (b) the corresponding internal representation, but in the second-order relation between (a) the perceived similarities between various external objects and (b) the similarities between their corresponding internal representations. Building on this idea, this study project report was divided into four parts. **First**, I outline the central ideas underlying similarity research and its applicability to aphantasia research. **Second**, I present a complete paradigm with an experimental design and a data analysis plan. The design will be based on multi-arrangement and inverse multidimensional scaling, a protocol that can be implemented online to conduct such large-scale research with high efficiency. The analysis plan will present a state-of-the-art method for similarity analysis, unsupervised alignment with Gromov-Wasserstein optimal transport (GWOT). **Third**, I report a data simulation I've done of a potential outcome of this study, and the successful analysis of this synthetic data using GWOT alignment. **Fourth**, I analyse the feasability of such a project given the material constraints of my thesis. I conclude with the expected utility and benefits of this project.\nbibliography: references.bib\ncsl: apa.csl\n---\n\n\n\n\\newpage\n\n# Work-In-Progress everywhere, *so look away! Shoo!* {.unlisted .unnumbered}\n\nI just wanted to try [Quarto's new manuscript format](https://quarto.org/docs/manuscripts/) with this personal project. Conclusion: it's ***incredibly cool***.\n\n::: {.callout-tip collapse=\"true\"}\n# Project inception\n\nThis project stems from several elements:\n\n1.  The long standing knowledge of the fact that internal representations seem impossible to reach due to their subjective nature.\n\n2.  The discovery of the article of @shepardSecondorderIsomorphismInternal1970 that expose the idea of \"second-order isomorphism\".\n\n3.  The discovery of state-of-the-art and accessible unsupervised analytic methods to study this principle in an astonishing way. The last two discoveries (and many more) are the fruit of amazing discussions and recommendations from Ladislas when he came to the lab on Jan. 26. These motivated me to try to implement GWOT in R on data that I wanted to create myself to emulate a study we could do.\n\n**I promise that I did this mostly on my spare time, we have too many other things to do elsewhere.**\n\n*Note: This website may seem very fancy. I wanted to take advantage of this personal project to try [Quarto's new manuscript format](https://quarto.org/docs/manuscripts/) for scientific editing. Conclusion: it's **awesome**. It is very likely that I'll end up writing my thesis using [Quarto's book format](https://quarto.org/docs/books/) (through RStudio). This will allow me to render the raw text and computations as beautifully formatted PDF and Word documents with low effort, and eventually port it as a self-contained website when I'm authorized to share it openly... All with a single command, just like I did for this website. **This also means that you can read the present report on a PDF or Word if you wish to do so, the links are in the header**. You'll freeze the nice interactive figures though. As a bonus for the curious (or the reviewer), the \"MECA Bundle\" contains absolutely everything tied to this manuscript, well sorted, from the code scripts and configuration files to the final documents in all formats. **Awesome, I tell you**.*\n:::\n\n\\newpage\n\n# Theoretical context\n\n## From similarity to second-order isomorphism\n\nWhen we try to compare our thoughts and representations with those of others, we quickly realize that the task will be really difficult, if not impossible, as we are of course incapable of \"living in someone else's head\". If we both try to imagine a dog, I can examine what goes on in my head, so can you, but apart from trying to describe our experiences verbally, we are up against a wall.\n\nNow, what if I asked you to tell me how similar you think a dog and a panther look like? Let's say, in the context of the animal kingdom as a whole. Visualize them well. Well, I could tell you that, *in my opinion, a dog and a panther look no more alike than a dog and a whale*. You might tell me: [*For this thought experiment, let's imagine two things: (1) that someone could honestly say that, and (2) that people would be rating the animals purely on the basis of their mental images, and not on categorical features (number of legs, fur, etc.), which is unfortunately almost* ***never*** *the case in reality.*]{.aside}\n\n> \"***What on earth do you imagine a dog and a panther look like? Do you also think that a dog looks nothing like a cat? What goes on in your head?***\"\n\n...And many people probably agree with you. They mentally \"see\" and compare certain items the same way you do... And just like that, *we are back on track*. We managed to better \"compare our thoughts\"! And we even felt we could dive a bit into the \"weird\" representations of someone else.\n\nThe study of individual differences in the format of representations and the attempt at understanding those of others obviously has a very rich history. It has interested many fields, in philosophy, linguistics, sociology, biology, psychology, or neuroscience, to name but a few. A myriad of ideas, concepts, models, methods, and paradigms have tried to deepen our understanding of representations and find the \"key\" to objectifying them. The principle I tried to illustrate with the thought experiment above is at the heart of one of these methods trying to unravel representations that was born in psychophysics[^mach]: the study of ***similarity.***\n\n[^mach]: Most notably in the works of @fechner1860 and @mach1890a; see also @roads2024 for an extended review.\n\n## Work-In-Progress\n\n## Psychological spaces and aphantasia\n\nWhile attempting to demonstrate the uselessness of the concept of similarity as a philosophical and scientific notion, @goodmanSevenStricturesSimilarity1972 has inadvertently expressed an aspect of similarity judgements of primary importance to us aphantasia researchers:\n\n> Comparative judgments of similarity often require not merely selection of relevant properties but a weighting of their relative importance, and variation in both relevance and importance can be rapid and enormous. Consider baggage at an airport checking station. The spectator may notice shape, size, color, material, and even make of luggage; the pilot is more concerned with weight, and the passenger with destination and ownership. Which pieces are more alike than others depends not only upon what properties they share, but upon who makes the comparison, and when. . . . Circumstances alter similarities. [*Goodman's claim was dismissed since then by propositions of robust mathematical models of similarity, e.g. @gardenforsConceptualSpacesFramework2004, @decockSimilarityGoodman2011.*]{.aside}\n\nThis can be easily reversed as an argument in favor of the **potential of similarity analyses to highlight the inter-individual differences in sensory mental representations**. For example, should we ask individuals to judge the similarities in shape or color between various objects, the *differences between the similarity structures* of individuals will be precisely the most important phenomenon for us, far less than the constancy between these structures. If we can account for the context dependence, as we will propose here with explicit instructions, clever task design, and hypothesis-neutral analysis, we could overcome the limitations of the inherently subjective nature of similarity judgements.\n\nThis idea of a difference in similarity judgements in aphantasia seems to transpire in the results of @bainbridgeQuantifyingAphantasiaDrawing2021 on their drawing study. They have shown that aphantasics had more schematic representations during recall, accurate in their spatial positioning, but with less sensory details. This difference can be seen from two perspectives: (1) a memory deficit for sensory properties; (2) a different representational structure of the items in their psychological spaces. In the latter case, aphantasics would have greater/faster abstraction of their representation of a perceived scene, reducing the amount of encoded sensory details unconsciously considered to be relevant. Both (1) and (2) can theoretically explain the same behavioural response, i.e. less sensory elements and correct spatial recall accuracy in aphantasic drawings, but **the two have drastically different consequences on how we define, characterize, and judge aphantasia.**\n\nThe dominant hypothesis seems to be that aphantasics simply have an episodic or general memory deficit. Conversely, I hypothesize that aphantasics have different representational structures than phantasics in certain dimensions of their psychological spaces (notably sensory, but potentially abstract too). More generally, I hypothesize that the concept of visual imagery evaluates in reality the continuous spectrum of representational structures in *sensory* dimensions of psychological spaces. Mirroring visual imagery, spatial imagery could also be a rough psychometric evaluation of the continuous spectrum of structural differences in *conceptual/abstract* dimensions of psychological spaces. In this view, the psychological space of aphantasics would constrain internal representations to particularly abstract forms from a very early stage, thus selectively limiting the item properties thereafter encoded in long-term memory. In other terms, **I hypothesize that aphantasia would not be characterized by an episodic memory deficit, but by an episodic memory *selectivity* caused by the specific characteristics of their representational structures and psychological spaces.** This selectivity would have, as we already hypothesized several times, benefits and drawbacks.\n\n@gardenforsConceptualSpacesFramework2004 proposed that differences in psychological (in his terms, conceptual) spaces could arise from various sources, whether innate, due to learning, or broader cultural or social differences. All these hypotheses could be coherent to explain the sources of aphantasia. Nevertheless, the study of these sources should be the subject of very large-scale or longitudinal studies, which are out of the scope of this project.\n\nHere, we shall rather attempt to **develop a method to characterize the differences in aphantasics' representational structures and psychological spaces.**\n\n## The present project\n\n# Methods\n\n@roads2024, in a recent review on the state and perspectives of similarity research, highlighted two challenges that studies in this field had to face: (1) The high cost of collecting behavioral data on a large number of stimuli; (2) The lack of software packages being a high barrier to entry, making the task of coding models difficult for the uninitiated.\n\nTo solve these problems, we present here two solutions, respectively for (1) experimental design and (2) data analysis:\n\n1.  A recent method to efficiently acquire similarity judgements, the \"multiple arrangement of items\" and \"inverse multidimensional scaling\" developed by @kriegeskorteInverseMDSInferring2012.\n2.  An accessible and robust Python toolbox provided by @sasakiToolboxGromovWassersteinOptimal2023 to conduct unsupervised alignment analysis using Gromov-Wasserstein optimal transport.\n\n## Experimental design\n\n### Multi-arrangement and inverse multidimensional scaling {#sec-ma}\n\nAssuming a geometric model of representational similarities, @kriegeskorteInverseMDSInferring2012 developed a multi-arrangement (MA) method to efficiently acquire (dis)similarity judgments for large sets of objects. The subject has to perform multiple arrangements of item subsets adaptively designed for optimal measurement efficiency and for estimating the representational dissimilarity matrix (RDM) by combining the evidence from the subset arrangements.\n\nThe procedure is illustrated in @fig-multi-arrangement.\n\n:::{.content-hidden when-format=\"pdf\"}\n![**Acquiring similarity judgements with the multi-arrangement method. (A)** Subjects are asked to arrange items according to their similarity, using mouse drag-and-drop on a computer. The similarity measure is taken as the distances between the items: similar items are closer, while dissimilar items are further apart. The upper part of the figure shows screenshots at different moments of the acquisition for one subject. Columns are trials and rows show the object arrangements over time, running from the start (top row) to the end (last row). The first trial contains all items; subsequent trials contain subsets of items that are adaptively selected to optimally estimate judged similarity for each subject. **(B)** Once acquisition of the final judgements is completed, inter-item distances in the final trial arrangements are combined over trials by rescaling and averaging to yield a single dissimilarity estimate for each object pair. The process is illustrated in this figure for two example item pairs: a boy's face and a hand (red), and carrots and a stop sign (blue). Their single-trial dissimilarity estimates (arrows) are combined into a single dissimilarity estimate, which is placed at the corresponding entry of the RDM (lower panel). Mirror-symmetric entries are indicated by lighter colors. Figure from @murHumanObjectSimilarityJudgments2013.](images/multi-arrangement-method-mur-2013.png){#fig-multi-arrangement width=\"90%\" cap-location=\"margin\"}\n:::\n\n:::{.content-visible when-format=\"pdf\"}\n![**Acquiring similarity judgements with the multi-arrangement method. (A)** Subjects are asked to arrange items according to their similarity, using mouse drag-and-drop on a computer. The similarity measure is taken as the distances between the items: similar items are closer, while dissimilar items are further apart. The upper part of the figure shows screenshots at different moments of the acquisition for one subject. Columns are trials and rows show the object arrangements over time, running from the start (top row) to the end (last row). The first trial contains all items; subsequent trials contain subsets of items that are adaptively selected to optimally estimate judged similarity for each subject. **(B)** Once acquisition of the final judgements is completed, inter-item distances in the final trial arrangements are combined over trials by rescaling and averaging to yield a single dissimilarity estimate for each object pair. The process is illustrated in this figure for two example item pairs: a boy's face and a hand (red), and carrots and a stop sign (blue). Their single-trial dissimilarity estimates (arrows) are combined into a single dissimilarity estimate, which is placed at the corresponding entry of the RDM (lower panel). Mirror-symmetric entries are indicated by lighter colors. Figure from @murHumanObjectSimilarityJudgments2013.](images/multi-arrangement-method-mur-2013.png){#fig-multi-arrangement width=\"90%\" cap-location=\"bottom\"}\n:::\n\nA key strength of this method that sets it as particularly effective is the \"adaptive\" part. The goal of the process is to acquire similarity judgements as precisely as possible while minimizing the total amount of trials. To do so, starting from the second trial, selected subsets of the items to be compared are presented to the subject: these items are the ones that were very close on-screen in previous trials and thus had their distance evaluated with lower accuracy by the subject. As the subject has to fill the entire \"arena\" with the items, these subsequent trials will necessarily increase the level of precision in the similarity judgement between pairs of items. The second key benefit of this method is the time and effort gain compared to others. For example, to compare every pair of items among 64 different items would require $\\frac{64 \\times (64-1)}{2} = 2016$ comparisons (i.e. trials). This would be extremely time-consuming, while also losing the *context-independence* afforded by the MA method due to the presence of other items around every time the subject mentally performs a pairwise comparison.\n\nHistorically, when referring to the projection of the representations of stimuli (e.g., coordinates in geometric space) from a high-dimensional space into a lower-dimensional space, inference algorithms were commonly called multidimensional scaling [@roads2024]. By analogy, the process of combining several lower-dimensional (2D) similarity judgements on-screen to form one higher dimensional similarity representation (in the RDM) can be conceptually seen as \"inverse\" multidimensional scaling, hence the name given to the method by @kriegeskorteInverseMDSInferring2012.\n\n### Principle {#sec-principle}\n\nThe idea is simple: for a given set of items that have distinct and very pictorial visual properties, we would ask a wide range of aphantasics, phantasics or hyperphantasics to imagine, mentally compare and make similarity judgements between the items. To compare these representations with actual perceptual representations, the subjects would also perform the same task afterwards, this time with actual pictures to compare. Subjects would also fill our usual psychometric imagery questionnaires.\n\n:::{.content-hidden when-format=\"pdf\"}\n![Arena layout of the MA protocol used by @murHumanObjectSimilarityJudgments2013 to acquire perceptual similarity judgements on natural images. *Click to expand.*](images/mur-spam-2.png){#fig-spam-mur .column-margin}\n:::\n\nTo \"compare imagined items\", we could use a \"word\" version of the MA paradigm. An example from @majewskaSpatialMultiarrangementClustering2020 - *who used the method to build large-scale semantic similarity resources for Natural Language Processing systems* - is represented in @fig-majewska.\n\n:::{.content-hidden when-format=\"pdf\"}\n![Arena layout of the MA protocol used by @majewskaSpatialMultiarrangementClustering2020 to acquire similarity judgements on word pairs. *Click to expand.*](images/majewska-spam-2.png){#fig-majewska .column-margin}\n:::\n\n:::{.content-visible when-format=\"pdf\"}\n\n:::{#fig-spam-arenas layout-ncol=1}\n\n![Arena layout of the MA protocol used by @murHumanObjectSimilarityJudgments2013 to acquire perceptual similarity judgements on natural images.](images/mur-spam-2.png){#fig-spam-mur width=50%}\n\n![Arena layout of the MA protocol used by @majewskaSpatialMultiarrangementClustering2020 to acquire similarity judgements on word pairs.](images/majewska-spam-2.png){#fig-majewska width=65%}\n\nExamples of arena layouts for the multi-arrangement (MA) paradigm.\n:::\n\n:::\n\n\nWe could have the stimuli rated by another set of participants on several features.\n\n> « *We deliberately did not specify which object properties to focus on, to avoid biasing participants' spontaneous mental representation of the similarities between objects. Our aim was to obtain similarity judgments that reflect the natural representation of objects without forcing participants to rely on one given dimension. However, participants were asked after having performed the task, what dimension(s) they used in judging object similarity.* » [@jozwik2016]\n\n> « ***All but one of the 16 participants reported arranging the images according to a categorical structure.*** » [@jozwik2017]\n\nThis result of @jozwik2017 suggests that we should give an explicit instruction about the features to focus on, otherwise everyone might bypass visual features and mental images in favour of concepts and categories, regardless of their mental imagery profile.\n\nIn contrast, if we ask to focus specifically on the visual features, then ask subjects about the strategy they used to evaluate the similarities, then on the subjectively felt mental format of these strategies, we might grasp better insight on the sensory representations of subjects.\n\nWe could even go for several comparisons - even though this would increase quadratically the number of trials - e.g. :\n\n-   Evaluate to what extent the **shape** *of these animals are* ***similar*** **at rest, ignoring size differences.**\n\n-   Evaluate to what extent these animals **sound like each other.**\n\n-   Etc.\n\n> *Note to be added: if you do not know the animal, just guess its placement, as this situation is quite unlikely to happen (animals chosen are fairly common knowledge).*\n\n@kawakita2023: To assess whether the color dissimilarity structures from different participants can be aligned in an unsupervised manner, we divided color pair similarity data from a large pool of 426 participants into five participant groups (85 or 86 participants per group) to obtain five independent and complete sets of pairwise dissimilarity ratings for 93 color stimuli (Fig. 3a). Each participant provided a pairwise dissimilarity judgment for a randomly allocated subset of the 4371 possible color pairs. We computed the mean of all judgments for each color pair in each group, generating five full dissimilarity matrices referred to as Group 1 to Group 5.\n\n### Stimuli\n\nWe would have a list of animal items, that would have several characteristics:\n\n:::{.content-hidden when-format=\"pdf\"}\n![Representing the characteristics of shapes with cylinders. Figure from @marr1997. *Click to expand.*](images/shapes-marr.png){#fig-marr .column-margin}\n:::\n\n:::{.content-visible when-format=\"pdf\"}\n![Representing the characteristics of shapes with cylinders. Figure from @marr1997. *Click to expand.*](images/shapes-marr.png){#fig-marr width=60%}\n:::\n\n-   A name\n\n-   A category\n\n-   A shape\n\nWe need orthogonal data:\n\n-   Each class of animal should include each shape (roughly)\n\n-   Each shape should have an animal\n\nThis would imply that category cannot be derived from shape, and vice-versa. Thus, a **sorting by shape would reveal to be innately visual** (or maybe spatial, if shape concerns this type of imagery), and a **sorting by category would reveal an abstraction** from these shapes. We expect that the two will be mixed to some degree in every subject, but that low-imagery would rather tend towards category sorting, while high-imagery would tend towards shape sorting.\n\nShapes could be very tricky stimuli to discuss. @gardenforsConceptualSpacesFramework2004 noted that we only have a very sketchy understanding of how we perceive and conceptualize things according to their shapes. The works of @marr1997 highlight this difficulty when analysing the complexity of the hierarchical judgements of shapes and volumes, as shown in @fig-marr.\n\n## Data analysis plan\n\n### Unsupervised alignment rationale\n\nVisual images can be represented as points in a multidimensional psychological space. Embedding algorithms can be used to infer latent representations from human similarity judgments. While there are an infinite number of potential visual features, an embedding algorithm can be used to identify the subset of salient features that accurately model human-perceived similarity. (*From Roads' CV*)\n\nUsing an optimization algorithm, the free parameters of a psychological space are found by maximizing goodness of fit (i.e., the loss function) to the observed data. Historically, when referring specifically to the free parameters that correspond to the representation of stimuli (e.g., coordinates in geometric space), inference algorithms were commonly called multidimensional scaling (MDS), or simply scaling, algorithms.\n\nIn the machine learning literature, analogous inference algorithms are often called embedding algorithms. The term \"embedding\" denotes a higher-dimensional representation that is embedded in a lower-dimensional space. For that reason, the inferred mental representations of a psychological space could also be called a psychological embedding.\n\nNumerous techniques exist, and each has limitations. Popular techniques for comparing representations include RSA @kriegeskorte2008 and canonical correlation analysis (CCA) (Hotelling 1936). Briefly, RSA is a method for comparing two representations that assesses the correlation between the implied pairwise similarity matrices. CCA is a method that compares two representations by finding a pair of latent variables (one for each domain) that are maximally correlated.\n\nOne might be tempted to compare two dissimilarity matrices assuming stimulus-level \"external\" correspondence: my \"red\" corresponds to your \"red\"(Fig. 1d). This type of supervised comparison between dissimilarity matrices, known as Representational Similarity Analysis (RSA), has been widely used in neuroscience to compare various similarity matrices obtained from behavioural and neural data. However, there is no guarantee that the same stimulus will necessarily evoke the same subjective experience across different participants. Accordingly, when considering which stimuli evoke which qualia for different individuals, we need to consider all possibilities of correspondence: my \"red\" might correspond to your \"red\", \"green\", \"purple\", or might lie somewhere between your \"orange\" and \"pink\"(Fig. 1e). Thus, we compare qualia structures in a purely unsupervised manner, without assuming any correspondence between individual qualia across participants.\n\n### Gromov-Wasserstein optimal transport\n\nTo account for all possible correspondences, we use an unsupervised alignment method for quantifying the degree of similarity between qualia structures. As shown in Fig. 2a, in unsupervised alignment, we do not attach any external (stimuli) labels to the qualia embeddings. Instead, we try to find the best matching between qualia structures based only on their internal relationships (see Methods). After finding the optimal alignment, we can use external labels, such as the identity of a color stimulus (Fig. 2b), to evaluate how the embeddings of different individuals relate to each other. This allows us to determine which color embeddings correspond to the same color embeddings across individuals or which do not. Checking the assumption that these external labels are consistent across individuals allows us to assess the plausibility of determining accurate inter-individual correspondences between qualia structures of different participants.\n\nTo this end, we used the Gromov-Wasserstein optimal transport (GWOT) method, which has been applied with great success in various fields. GWOT aims to find the optimal mapping between two point clouds in different domains based on the distance between points within each domain. Importantly, the distances (or correspondences) between points \"across\" different domains are not given while those \"within\" the same domain are given. GWOT aligns the point clouds according to the principle that a point in one domain should correspond to another point in the other domain that has a similar relationship to other points. The principle of the method is illustrated in @fig-gwot-kawa\n\n:::{.content-hidden when-format=\"pdf\"}\n![Gromov-Wassertein optimal transport principle. Figure from @kawakita2023. *Click to expand.*](images/kawa-gwot-2.PNG){#fig-gwot-kawa .column-margin}\n:::\n\n:::{.content-visible when-format=\"pdf\"}\n![Gromov-Wassertein optimal transport principle. Figure from @kawakita2023. *Click to expand.*](images/kawa-gwot-2.PNG){#fig-gwot-kawa width=70%}\n:::\n\nWe first computed the GWD for all pairs of the dissimilarity matrices of the 5 groups (Group 1-5) using the optimized $\\epsilon$. In Fig. 3b, we show the optimized mapping $\\Gamma*$ between Group 1 and Groups 2-5 (see Supplementary Figure S1 for the other pairs). As shown in Fig. 3b, most of the diagonal elements in $\\Gamma*$ show high values, indicating that most colors in one group correspond to the same colors in the other groups with high probability. We next performed unsupervised alignment of the vector embeddings of qualia structures. Although $\\Gamma*$ provides the rough correspondence between the embeddings of qualia structures, we should find a more precise mathematical mapping between qualia structures in terms of their vector embeddings to more accurately assess the similarity between the qualia structures. Here, we consider aligning the embeddings of all the groups in a common space.\n\nBy applying MDS, we obtained the 3-dimensional embeddings of Group 1 and Groups 2-5, referred to as X and Yi, where i = 2, ..., 5 (Fig. 3c). We then aligned Yi to X with the orthogonal rotation matrix Qi, which was obtained by solving a Procrustes-type problem using the optimized transportation plan $\\Gamma*$ obtained through GWOT (see Methods). Fig. 3d shows the aligned embed- dings of Group 2-5 (QiYi) and the embedding of Group 1 (X) plotted in the embedded space of X. Each color represents the label of a corresponding external color stimulus. Note that even though the color labels are shown in Fig. 3d, this is only for the visualization purpose and the whole alignment procedure is performed in a purely unsupervised manner without relying on the color labels. As can be seen in Fig. 3d, the embeddings of similar colors from the five groups are located close to each other, indicating that similar colors are 'correctly' aligned by the unsupervised alignment method.\n\nTo evaluate the performance of the unsupervised alignment, we computed the k-nearest color matching rate in the aligned space. If the same colors from two groups are within the k-nearest colors in the aligned space, we consider that the colors are correctly matched. We evaluated the matching rates between all the pairs of Groups 1-5. The averaged matching rates are 51% when k = 1, 83% when k = 3, and 92% when k = 5, respectively. This demonstrates the effectiveness of the GW alignment for correctly aligning the qualia structures of different participants in an unsupervised manner.\n\nHowever, as can be seen in Fig. 4b, the optimized mapping $\\Gamma*$ is not lined up diagonally unlike the optimized map- pings between color-neurotypical participants groups shown in Fig. 3b (see Supplementary Figure S1 for the other pairs). Accordingly, top k matching rate between Group 1-5 and Group 6 is 3.0% when k = 1 (Fig. 4c), which is only slightly above chance ($\\approx$ 1%). The matching rate did not improve even when we relaxed the criterion (6.9% and 11% for k = 3 and k = 5, respectively). Moreover, all of the GWD values between Group 1-5 and Group 6 are larger than any of the GWD values between color-neurotypical participant groups (Fig. 4d).\n\nThese results indicate that the difference between the qualia structures of neuro-typical and atypical participants is significantly larger than the difference between the qualia structures of neuro-typical participants.\n\n## Paradigm summary\n\nThe experimental design and data analysis plans are succinctly summarised in @fig-expe-conditions.\n\n::: {#fig-expe-conditions .column-page-inset layout-ncol=2 cap-location=\"bottom\"}\n![](images/my-protocol-1.png){#fig-expe-subject}\n\n![](images/my-protocol-2.png){#fig-expe-group}\n\nSummary schematics of the proposed experimental protocol and data analysis plan. *Click on the sub-figures to expand them.* @fig-expe-subject represents the two conditions to be completed by each subject. These two conditions will allow to compute comparisons (alignments) within a subject's own perceptual and imaginal representational structures, but also between subjects (or groups) for each modality (see the next figure's description). **(A)** The subject performs two simililarity judgement tasks using the MA paradigm presented earlier. **(B)** The low-dimensional similarity judgements are converted to a high-dimensional Representational Dissimilarity Matrix (RDM) through inverse-MDS as a follow-up to extract the results of the MA. **(C)** The RDMs are then reduced in dimensionality once again to extract relevant dimensions reflecting inferred features of the items through MDS, yielding embeddings. Three-dimensional projections of these embeddings have been chosen here for visualization purposes. **(D)** These embeddings are compared through unsupervised alignment using GWOT, which results in an estimate of the degree of alignment of the two representational structures and in coordinates of aligned embeddings. These coordinates allow us to examine the 3D visualization shown here and judge by ourselves the \"look\" of the alignment. Here the perception representation aligns with the imagination one, from which we could infer that imagined representations are made of sensory (rather than abstract properties). We expect inter-individual variability in these perception-imagination alignments, as shown in the next figure. @fig-expe-group represents the comparison between the representational structure of different cognitive profiles. In practice, all pairs of subjects will be compared to assess their representational structure alignments, independently of arbitrary groups. This is computationally heavy, but analytically very powerful. This figure also tacitly shows an idea supporting the use of unsupervised alignment: it is possible that RDMs seem to be very correlated and similar, as shown in step **(B)**, but do not align when compared without supervision. This contrasts with several supervised alignment methods [such as RSA, see @kriegeskorte2008] which usually use the RDM as-is. This difference is due to the involvement of labels for items that are already known by the researcher to correlate the RDMs, whereas unsupervised algorithms such as GWOT are only concerned with the structures. This principle is eloquently illustrated by @fig-gwot-kawa from @kawakita2023.\n:::\n\n## Hypotheses\n\n### Aphantasic and phantasic psychological spaces\n\nThe most representative members of a category are called prototypical members.\n\nPrototype theory builds on the observation that among the instances of a property, some are more representative than others. The most representative one is the prototype of the property.\n\nThus, following the concepts illustrated by @gardenforsConceptualSpacesFramework2004, we would expect that aphantasics, when doing shape similarity judgements, would be more inclined to group items close to the prototypical items due to a lower definition of the mental image. In comparison, phantasics would have a much more distributed conceptual space of item shapes due to their higher-resolution mental images of said items.\n\n### Subjective imagery and psychological spaces\n\nIn the proposed view of visual imagery as the subjective expression of a given type of psychological space, we mentioned earlier that *spatial* imagery could also constitute a subjective expression of other dimensions of psychological spaces. Hence, the *verbal* dimension of the simplified model of imagery we outlined in my thesis project could also represent different dimensions.\n\nThis conception leads to the following theoretical hypothesis: provided that our visual-spatial-verbal model correctly fits subjective imagery, the imagery profile of individuals should map on their psychological spaces.\n\nOperationally, this would be evaluated by the fact that **individuals with similar imagery profiles** (visual, spatial, verbal, or any combination of the three) **should have similar representations** in their given psychological space, **quantifiable by the degree of alignment between their similarity structures.**\n\n# Study simulation results\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# ═══ Packages ═════════════════════════════════════════════════════════════════\n\nif (!require(librarian)) install.packages(librarian) \n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nLe chargement a nécessité le package : librarian\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(librarian)                                     \n\n# now putting packages on our library's shelves:\nshelf(\n  # ─── data management ─────────────────\n  holodeck,       # simulating multivariate data\n  cluster,        # dissimilarity matrices\n  \n  # ─── modelling ───────────────────────\n  mclust,         # mixture clustering\n  \n  #  data visualization ──────────────\n  # palettes\n  viridis,        # colour-blind friendly palettes\n  # interactive\n  plotly,         # interactive plots\n  ggdendro,       # dendrograms\n  seriation,      # dissimilarity plots\n  webshot2,       # HTML screenshots for Word render\n  webshot,\n  \n  # ─── essential package collections ───\n  doParallel,     # parallel execution\n  easystats,      # data analysis ecosystem\n  reticulate,     # R to Python                    \n  tidyverse,      # modern R ecosystem\n)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n\n  The 'cran_repo' argument in shelf() was not set, so it will use\n  cran_repo = 'https://cran.r-project.org' by default.\n\n  To avoid this message, set the 'cran_repo' argument to a CRAN\n  mirror URL (see https://cran.r-project.org/mirrors.html) or set\n  'quiet = TRUE'.\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\n# ─── Global cosmetic theme ───\ntheme_set(theme_modern(base_size = 14))\n\npal_okabe_ito <- c(     # <3>                                                \n  \"#E69F00\", \"#56B4E9\", \"#009E73\",                            \n  \"#F5C710\", \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#6c0009\")      \n\n# We'll need 9 colors at some point\npal_okabe_ito_extended <- c(                                 \n  \"#E69F00\", \"#56B4E9\", \"#009E73\",                           \n  \"#F5C710\", \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#6c0009\", \"#414487FF\")\n\n# We'll need 30 colors at another moment\ncool_30_colors <- c(                                                   \n  \"#3d51b4\", \"#414487FF\", \"#003d73\", \"#440154FF\", \"#6c0009\", \"#b64e4e\",\n  \"#D55E00\", \"#E69F00\", \"#F5C710\", \"#FDE725FF\", \"#f2bb7b\", \"#f1afad\", \"#CC79A7\", \n  \"#e57774\", \"#7AD151FF\", \"#57b571\", \"#318a4a\", \"#009E73\", \"#22A884FF\", \n  \"#2A788EFF\", \"#0072B2\", \"#2da6b5\", \"#56B4E9\", \"#889be0\", \"#6677e0\",   \n  \"#3d51b4\", \"#414487FF\", \"#003d73\", \"#440154FF\", \"#6c0009\", \"#b64e4e\"  \n  ) # <3>                                                                      \n\npath = \"notebooks/data/\"\n\ndf <- read_rds(paste0(path, \"df.RDS\"))\ndf_embeds <- read_rds(paste0(path, \"df_embeds.RDS\"))\n# Categorical and visual embeddings\ndf_embeds_categ  <- read_rds(paste0(path, \"df_embeds_categ.RDS\"))\ndf_embeds_visual <- read_rds(paste0(path, \"df_embeds_visual.RDS\"))\n# Subject embeddings per sub-group\ndf_embed_c_sub  <- read_rds(paste0(path, \"df_embed_c_sub.RDS\"))\ndf_embed_cs_sub <- read_rds(paste0(path, \"df_embed_cs_sub.RDS\"))\ndf_embed_v_sub  <- read_rds(paste0(path, \"df_embed_v_sub.RDS\"))\ndf_embed_vs_sub <- read_rds(paste0(path, \"df_embed_vs_sub.RDS\"))\n# Accuracy of the unsupervised alignment (bad = not tidy data)\ndf_accuracy_all_bad <- read_rds(paste0(path, \"df_accuracy_all_bad.RDS\"))\ndf_accuracy_cat_bad <- read_rds(paste0(path, \"df_accuracy_cat_bad.RDS\"))\n# Coordinates of the aligned embeddings from the Python output\ncoordinates_aligned_embeddings <- read_rds(paste0(path, \"coordinates_aligned_embeddings.RDS\"))\n```\n:::\n\n\n\n## Visual-spatial-verbal model of cognitive profiles {#sec-osv-model-theory}\n\nOne of the objectives of the study would be to link the subjective cognitive profiles of individuals with their representational structures. To evaluate these profiles, we are going to use psychometric questionnaires evaluating the visual-object, spatial, and verbal dimensions of imagery which will yield three scores, one for each dimension.\n\nWe are going to simulate 30 participants presenting four different cognitive profiles, that I defined as, respectively, *verbal* aphantasics, *spatial* aphantasics, *spatial* phantasics, and *visual* phantasics. Their imagery abilities are summarised in @tbl-imageries.\n\nTo simulate these four sub-groups, we will generate multivariate normal distributions of scores on these three dimensions for each sub-group. For instance, verbal aphantasics have normally distributed visual imagery scores centred around a mean of 0 (normalized, so negative scores are possible), 0.4 for spatial imagery, and 0.7 for verbal style; Spatial aphantasics have means of 0 for visual, 0.75 spatial, and 0.3 for verbal; etc. The numbers are arbitrary, but have been chosen by trial-and-error to obtain a model that is both well-defined and not exaggerated. The 30 subjects' imagery profiles are represented in the three dimensional space of the visual-spatial-verbal dimensions in @fig-osv-model.\n\n| Cognitive profile  | Visual imagery | Spatial imagery | Verbal style |\n|--------------------|:--------------:|:---------------:|:------------:|\n| Verbal aphantasic  |       --       |       \\-        |      ++      |\n| Spatial aphantasic |       --       |       ++        |      \\-      |\n| Spatial phantasic  |       \\+       |       ++        |      \\-      |\n| Visual phantasic   |       ++       |       \\-        |      \\+      |\n\n: Imagery abilities of the four hypothesized cognitive profiles. {#tbl-imageries}\n\n\n\n::: {#cell-fig-osv-model .cell}\n\n```{.r .cell-code .hidden}\nplotting_osv_model <- function(df, grouping_variable, size){\n  df |> \n    plot_ly(\n      x = ~visual_imagery,\n      y = ~spatial_imagery,\n      z = ~verbal_profile,\n      color = ~df[[grouping_variable]],\n      text  = ~df[[grouping_variable]],\n      colors = c(\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F5C710\"),\n      type = \"scatter3d\",\n      mode = \"markers+text\",\n      marker = list(size = size),\n      textfont = list(size = size + 4)\n    ) |> \n    layout(\n      scene = list(\n        xaxis = list(\n          title = list(text = \"Visual imagery\", font = list(color = \"grey\")),\n          tickfont = list(color = \"grey\")\n          ),\n        yaxis = list(\n          title = list(text = \"Spatial imagery\", font = list(color = \"grey\")),\n          tickfont = list(color = \"grey\")\n          ),\n        zaxis = list(\n          title = list(text = \"Verbal profile\", font = list(color = \"grey\")),\n          tickfont = list(color = \"grey\")\n          )\n      ),\n      legend = list(title = list(text = \"Group\")),\n      paper_bgcolor = \"transparent\"\n      )\n}\n\ndf |> \n  mutate(vis_spa_group = case_when(\n    vis_spa_group == \"aph_spa_high\" ~ \"Aph. spatial\",\n    vis_spa_group == \"aph_spa_low\"  ~ \"Aph. verbal\",\n    vis_spa_group == \"phant_spa_high\" ~ \"Phant. spatial\",\n    vis_spa_group == \"phant_spa_low\"  ~ \"Phant. visual\"\n  )) |> \n  plotting_osv_model(grouping_variable = \"vis_spa_group\", size = 4)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nPhantomJS not found. You can install it with webshot::install_phantomjs(). If it is installed, please make sure the phantomjs executable can be found via the PATH variable.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![Imagery profiles generated for 30 subjects on the three object, spatial, and verbal dimensions.](index_files/figure-docx/fig-osv-model-1.png){#fig-osv-model}\n:::\n:::\n\n\n\n## Data simulation: Creating representational structures\n\n@gardenforsConceptualSpacesFramework2004 invokes two scientific concepts, to wit, prototypes and Voronoi tessellations. Prototype theory builds on the observation that among the instances of a property, some are more representative than others. The most representative one is the prototype of the property. *We hypothesize that aphantasics will be more inclined to categorize items according to prototypes than phantasics.*\n\nA Voronoi tessellation of a given space divides that space into a number of cells such that each cell has a center and consists of all and only those points that lie no closer to the center of any other cell than to its own center; the centers of the various cells are called the generator points of the tessellation. This principle will underlie our data simulation, as we will build representations in a 3D space based on distances to \"centroids\", namely, prototypes. These representations will thus be located inside of the tessellations around these prototypes, more or less close to the centroid depending on the subject's representational structures.\n\n### Generating \"prototype\" embeddings from a sphere\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# getting the centroids of each cluster\ndf_centroids <- \n  df_embeds |> \n  group_by(group) |> \n  summarise(\n    x_centroid = mean(x),\n    y_centroid = mean(y),\n    z_centroid = mean(z)\n  )\n\n# adding them to the data\ndf_embeds_2 <- left_join(df_embeds, df_centroids, by = \"group\")\n```\n:::\n\n\n\nA function will be used to generate embeddings. These spherical embeddings are displayed in [@fig-perfect-embeddings-html]{.content-hidden when-format=\"pdf\"} [@fig-perfect-embeddings-pdf]{.content-visible when-format=\"pdf\"}. We get 8 nicely distributed clusters. We'll retrieve the centroids of each cluster, which would be the \"perfect\" categories of each species group (say, generated by a computational model on categorical criteria).\n\n\n\n::: {.cell .hidden}\n\n```{.r .cell-code .hidden}\n# function for 3D plotting up to 8 groups (due to the palette)\nplotting_3d <- function(df, size, opacity){\n  df |> \n    plot_ly(\n      type = \"scatter3d\",\n      mode = \"markers\",\n      x = ~x,\n      y = ~y,\n      z = ~z,\n      color = ~paste0(\"Species group \", group),\n      colors = pal_okabe_ito,\n      marker = list(size = size, opacity = opacity)\n    ) |> \n    layout(paper_bgcolor = \"transparent\")\n}\n\nplotting_3d(df_embeds, 3, 1) |> \n  layout(legend = list(\n    yanchor = \"top\",\n    y = 1,\n    xanchor = \"right\",\n    x = 0\n    ))\n\ndf_centroids |> \n  plot_ly(\n    type = \"scatter3d\",\n    mode = \"markers+text\",\n    x = ~x_centroid,\n    y = ~y_centroid,\n    z = ~z_centroid,\n    text = ~paste0(\"Species group \", group),\n    color = ~paste0(\"Species group \", group),\n    colors = pal_okabe_ito,\n    marker = list(size = 12, opacity = 1)\n    ) |> \n  layout(\n    scene = list(\n      xaxis = list(title = \"x\"),\n      yaxis = list(title = \"y\"),\n      zaxis = list(title = \"z\")\n    ),\n    paper_bgcolor = \"transparent\",\n    showlegend = FALSE\n    )\n```\n:::\n\n\n\n::: {.content-hidden when-format=\"pdf\"}\n::: column-page-inset\n\n\n::: {#fig-perfect-embeddings-html .cell layout-ncol=\"2\"}\n\n```{.r .cell-code .hidden}\n# function for 3D plotting up to 8 groups (due to the palette)\nplotting_3d <- function(df, size, opacity){\n  df |> \n    plot_ly(\n      type = \"scatter3d\",\n      mode = \"markers\",\n      x = ~x,\n      y = ~y,\n      z = ~z,\n      color = ~paste0(\"Species group \", group),\n      colors = pal_okabe_ito,\n      marker = list(size = size, opacity = opacity)\n    ) |> \n    layout(paper_bgcolor = \"transparent\")\n}\n\nplotting_3d(df_embeds, 3, 1) |> \n  layout(legend = list(\n    yanchor = \"top\",\n    y = 1,\n    xanchor = \"right\",\n    x = 0\n    ))\n```\n\n::: {.cell-output-display}\n![Generated spherical distribution of 1000 observations grouped in 8 equal clusers with Gaussian Mixture Clustering.](index_files/figure-docx/fig-perfect-embeddings-html-1.png){#fig-perfect-embeddings-html-1}\n:::\n\n```{.r .cell-code .hidden}\ndf_centroids |> \n  plot_ly(\n    type = \"scatter3d\",\n    mode = \"markers+text\",\n    x = ~x_centroid,\n    y = ~y_centroid,\n    z = ~z_centroid,\n    text = ~paste0(\"Species group \", group),\n    color = ~paste0(\"Species group \", group),\n    colors = pal_okabe_ito,\n    marker = list(size = 12, opacity = 1)\n    ) |> \n  layout(\n    scene = list(\n      xaxis = list(title = \"x\"),\n      yaxis = list(title = \"y\"),\n      zaxis = list(title = \"z\")\n    ),\n    paper_bgcolor = \"transparent\",\n    showlegend = FALSE\n    )\n```\n\n::: {.cell-output-display}\n![Centroids of the 8 clusters created on the sphere.](index_files/figure-docx/fig-perfect-embeddings-html-2.png){#fig-perfect-embeddings-html-2}\n:::\n\nInitial random generations of 1000 points grouped in 8 clusters to represent the theoretical embeddings of 8 groups (i.e. groups of species here). ***Interact with the figures to see the details.***\n:::\n\n\n:::\n:::\n\n::: {.content-visible when-format=\"pdf\"}\n\n\n::: {#fig-perfect-embeddings-pdf .cell layout-ncol=\"2\"}\n\n```{.r .cell-code .hidden}\n# function for 3D plotting up to 8 groups (due to the palette)\nplotting_3d <- function(df, size, opacity){\n  df |> \n    plot_ly(\n      type = \"scatter3d\",\n      mode = \"markers\",\n      x = ~x,\n      y = ~y,\n      z = ~z,\n      color = ~paste0(\"Species group \", group),\n      colors = pal_okabe_ito,\n      marker = list(size = size, opacity = opacity)\n    ) |> \n    layout(paper_bgcolor = \"transparent\")\n}\n\nplotting_3d(df_embeds, 3, 1) |> \n  layout(legend = list(\n    yanchor = \"top\",\n    y = 1,\n    xanchor = \"right\",\n    x = 0\n    ))\n```\n\n::: {.cell-output-display}\n![Generated spherical distribution of 1000 observations grouped in 8 equal clusers with Gaussian Mixture Clustering.](index_files/figure-docx/fig-perfect-embeddings-pdf-1.png){#fig-perfect-embeddings-pdf-1}\n:::\n\n```{.r .cell-code .hidden}\ndf_centroids |> \n  plot_ly(\n    type = \"scatter3d\",\n    mode = \"markers+text\",\n    x = ~x_centroid,\n    y = ~y_centroid,\n    z = ~z_centroid,\n    text = ~paste0(\"Species group \", group),\n    color = ~paste0(\"Species group \", group),\n    colors = pal_okabe_ito,\n    marker = list(size = 12, opacity = 1)\n    ) |> \n  layout(\n    scene = list(\n      xaxis = list(title = \"x\"),\n      yaxis = list(title = \"y\"),\n      zaxis = list(title = \"z\")\n    ),\n    paper_bgcolor = \"transparent\",\n    showlegend = FALSE\n    )\n```\n\n::: {.cell-output-display}\n![Centroids of the 8 clusters created on the sphere.](index_files/figure-docx/fig-perfect-embeddings-pdf-2.png){#fig-perfect-embeddings-pdf-2}\n:::\n\nInitial random generations of 1000 points grouped in 8 clusters to represent the theoretical embeddings of 8 groups (i.e. groups of species here). ***Interact with the figures to see the details.***\n:::\n\n\n:::\n\nNow we want two sets of embeddings: one where the observations are very concentrated around the centroids, which would be the **categorical model**, and one where the observations are more spread out, which would be the **visual model**.\n\nWe need to select 8 observations per cluster, which would be our animals per group. These observations will be subsets of the 1000 observations we generated.\n\n\n### Categorical model embeddings\n\nThe selection procedure for the **categorical model** will consist of selecting points that are rather *close to the centroids*. Thus, we will filter the observations of the large sets to keep only points for which the distance to the centroid is inferior to a given value. That is, points for which the Euclidean norm of the vector from the observation to the centroid:\n\n$$d(centroid, observation) = \\sqrt{(x_{c} - x_{o})^{2} + (y_{c} - y_{o})^{2} + (z_{c} - z_{o})^{2}}$$\n\n\n\n::: {#cell-fig-categorical-embeddings .cell}\n\n```{.r .cell-code .hidden}\n# Plotting these observations\nplotting_3d(df_embeds_categ, 6, 1)\n```\n\n::: {.cell-output-display}\n![Selection of 64 points to represent prototypical categorical embeddings, based on the distances to each groups' centroid. These will be the bases of the verbal aphantasics' embeddings. ***Interact with the figure to see the details.***](index_files/figure-docx/fig-categorical-embeddings-1.png){#fig-categorical-embeddings}\n:::\n:::\n\n\n\n### Visual model embeddings\n\nIn the case of the **visual model**, we would like approximately evenly distributed embeddings, that could also dive *inside* the sphere, i.e. representing species that are visually close although diametrically opposed when it comes to taxonomy. To do this we can simulate multivariate normal distributions around the centroids.\n\n\n\n::: {#cell-fig-visual-embeddings .cell}\n\n```{.r .cell-code .hidden}\nplotting_3d(df_embeds_categ, 4, 1) |> \n  add_trace(\n      data = df_embeds_visual,\n      type = \"scatter3d\",\n      mode = \"markers\",\n      x = ~x,\n      y = ~y,\n      z = ~z,\n      color = ~paste0(\"Species group \", group),\n      colors = pal_okabe_ito,\n      marker = list(size = 4, opacity = 1, symbol = \"diamond\")\n  )\n```\n\n::: {.cell-output-display}\n![Selection of 64 points to represent prototypical visual embeddings, chosen randomly in multivariate distributions centered around each categorical embedding. The visual embeddings are overlaid as diamonds along with categorical ones as dots. The two distributions keep the group structure, but are pretty far apart at times. ***Interact with the figure to see the details.***](index_files/figure-docx/fig-visual-embeddings-1.png){#fig-visual-embeddings}\n:::\n:::\n\n\n\n### Intermediate embeddings\n\n\n\n```{mermaid}\n%%| label: fig-diagram-intermediate\n%%| fig-width: 5\n%%| fig-cap: \"Model of the distances between participants' representations. Note that here d is a one-dimensional distance between the representations, but it will be computed as a three-dimensional distance in our toy-model. The verbal aphantasic profile is hypothesized to be very categorical, thus diametrically opposed to the visual phantasic profile, by a given distance d. Spatial profiles are in-between: they are close to each other (10% x d), but the spatial aphantasic profile is a bit closer to the verbal aphantasic one (45% x d), and the spatial phantasic is a bit closer to the visual phantasic one (45% x d).\"\nflowchart LR\n    A(Aph. verbal) -----|0.45 x d| B(Aph. spatial) ---|0.10 x d| C(Phant. spatial) ------|0.45 x d| D(Phant. visual)\n    A ---|d| D\n```\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# ─── Generating visual embeddings based on multivariate distributions around \n# the categorical embeddings ───\n\ndist_c = 0.45\ndist_v = 0.55\n\ndf_embeddings <-\n  df_embeds_categ |>\n  rename(\n    group_c = group,\n    x_c = x,\n    y_c = y,\n    z_c = z\n  ) |>\n  bind_cols(df_embeds_visual) |>\n  rename(\n    group_v = group,\n    x_v = x,\n    y_v = y,\n    z_v = z\n  ) |> \n  select(!group_v) |> \n  rename(group = group_c) |> \n  mutate(\n    x_cs = x_c + dist_c*(x_v - x_c),\n    y_cs = y_c + dist_c*(y_v - y_c),\n    z_cs = z_c + dist_c*(z_v - z_c),\n    x_vs = x_c + dist_v*(x_v - x_c),\n    y_vs = y_c + dist_v*(y_v - y_c),\n    z_vs = z_c + dist_v*(z_v - z_c)\n  )\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nNew names:\n• `species` -> `species...2`\n• `species` -> `species...7`\n```\n\n\n:::\n:::\n\n::: {#cell-fig-intermediate-embeddings .cell}\n\n```{.r .cell-code .hidden}\nsize = 3\n\ndf_embeddings |> \n  plot_ly(\n    type = \"scatter3d\", \n    mode = \"marker\",\n    color  = ~paste0(\"Species group \", group),\n    colors = ~pal_okabe_ito\n    ) |> \n  add_markers(\n    x = ~x_c, y = ~y_c, z = ~z_c, \n    mode = \"marker\",\n    marker = list(size = size, symbol = \"circle\")\n  ) |> \n  add_markers(\n    x = ~x_v, y = ~y_v, z = ~z_v, \n    mode = \"marker\",\n    marker = list(size = size, symbol = \"diamond\")\n  ) |> \n  add_markers(\n    x = ~x_cs, y = ~y_cs, z = ~z_cs, \n    mode = \"marker\",\n    marker = list(size = size, symbol = \"circle-open\")\n  ) |> \n  add_markers(\n    x = ~x_vs, y = ~y_vs, z = ~z_vs, \n    mode = \"marker\",\n    marker = list(size = size, symbol = \"diamond-open\")\n  ) |> \n  layout(\n    scene = list(\n      xaxis = list(title = \"x\"),\n      yaxis = list(title = \"y\"),\n      zaxis = list(title = \"z\")\n    ),\n    paper_bgcolor = \"transparent\")\n```\n\n::: {.cell-output-display}\n![Space of embeddings with 128 additional points based on the euclidean distances between the visual and categorical embeddings. The empty dots are the *aphantasics-spatial* ones, and the empty diamonds are the *phantasic-spatial* ones. Some can be very close together, and sometimes further apart due to the various pairs of visual and categorical points used to create them. A network-like structure seems to appear, with empty points seemingly 'connecting' the dots and diamonds. ***Interact with the figure to see the details.***](index_files/figure-docx/fig-intermediate-embeddings-1.png){#fig-intermediate-embeddings}\n:::\n:::\n\n\n\nThe distributions created are still gathered around the centroids of each group, but they are much more widespread, each group getting close to each other and even reaching inside the sphere.\n\nPerfect! Now we have two 3D embeddings per animal, in a categorical or a visual description of their features. Thus, we have four sets of coherent coordinates, around which we will simulate the embeddings of the 30 participants, depending on their groups.\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# ─── Labeling each species in each group ───\n\ndf_embeddings <- \n  df_embeddings |> \n  mutate(\n    group = case_when(\n    group == 1 ~ \"a\", \n    group == 2 ~ \"b\",\n    group == 3 ~ \"c\",\n    group == 4 ~ \"d\",\n    group == 5 ~ \"e\",\n    group == 6 ~ \"f\",\n    group == 7 ~ \"g\",\n    group == 8 ~ \"h\",\n    TRUE ~ group\n    )\n  ) |> \n  group_by(group) |> \n  mutate(\n    species = paste0(\"species_\", group, 1:8),\n    species = as.factor(species),\n    group   = as.factor(group)\n    ) |> \n  select(group, species, everything())\n```\n:::\n\n\n\n\n### Generating the subject embeddings\n\nWe have four \"reference\" sets of embeddings which represent animals either judged according to their similarity in categorical terms (namely, species), or in visual terms (namely shape or color similarities, assuming that these similarities are more evenly distributed, e.g. the crab looks like a spider, but is also pretty close to a scorpion, etc.).\n\nTo generate the embeddings of each subject in each condition, we will start from these reference embeddings and generate random noise around *each item*, i.e. for all 64 animals. For 100 subjects, we would thus generate 100 noisy points around each animal, each point corresponding to a given subject.\n\nThe visual and verbal groups will be generated with slightly more intra-group variance, so as to try to make the spatial groups as coherent as possible (and avoid blurring everything and making the groups disappear in noise).\n\nAlthough the groups and species in [@fig-subject-embeddings-html-1]{.content-hidden when-format=\"pdf\"} [@fig-subject-embeddings-pdf-1]{.content-visible when-format=\"pdf\"} look fairly obvious when we colour the embeddings using the knowledge about how we built them, the algorithm will only be fed with the data for each subject, without any labelling or additional information. Thus, [@fig-subject-embeddings-html-2]{.content-hidden when-format=\"pdf\"} [@fig-subject-embeddings-pdf-2]{.content-visible when-format=\"pdf\"} is what the algorithm will actually \"see\" (and what it will try to decrypt). Said otherwise, its objective will be to find all the correct colours and shapes in [@fig-subject-embeddings-html-1]{.content-hidden when-format=\"pdf\"} [@fig-subject-embeddings-pdf-1]{.content-visible when-format=\"pdf\"} using only 30 sub-datasets (one for each subject) that are illustrated in [@fig-subject-embeddings-html-2]{.content-hidden when-format=\"pdf\"} [@fig-subject-embeddings-pdf-2]{.content-visible when-format=\"pdf\"}. Admittedly, that looks a lot more complicated.\n\n\n\n::: {.cell .hidden}\n\n```{.r .cell-code .hidden}\nsize = 2\n\nplot_ly(\n    type = \"scatter3d\", \n    mode = \"marker\",\n    colors = ~pal_okabe_ito\n    ) |> \n  add_markers(\n    data = df_embed_c_sub, \n    x = ~x, y = ~y, z = ~z, color  = ~paste0(\"Species group \", group),\n    mode = \"marker\",\n    marker = list(size = size, symbol = \"circle\")\n  ) |> \n  add_markers(\n    data = df_embed_v_sub, \n    x = ~x, y = ~y, z = ~z, color  = ~paste0(\"Species group \", group),\n    mode = \"marker\",\n    marker = list(size = size, symbol = \"diamond\")\n  ) |> \n  add_markers(\n    data = df_embed_cs_sub, \n    x = ~x, y = ~y, z = ~z, color  = ~paste0(\"Species group \", group), \n    mode = \"marker\",\n    marker = list(size = size, symbol = \"circle-open\")\n  ) |> \n  add_markers(\n    data = df_embed_vs_sub, \n    x = ~x, y = ~y, z = ~z, color  = ~paste0(\"Species group \", group), \n    mode = \"marker\",\n    marker = list(size = size, symbol = \"diamond-open\")\n  ) |> \n  layout(legend = list(\n    yanchor = \"top\",\n    y = 1,\n    xanchor = \"right\",\n    x = 0\n    ),\n    paper_bgcolor = \"transparent\",\n    showlegend = FALSE\n    )\n\nbind_rows(\n  # aph_spatial\n  df_embed_cs_sub |> \n    separate_wider_delim(\n      subject,\n      delim = \"_\",\n      names = c(\"subject\", \"number\")\n    ) |> \n    mutate(subject = paste0(subject,\"_a_aph_s\", number), .keep = \"unused\"),\n  \n  # aph_verbal\n  df_embed_c_sub  |>\n    separate_wider_delim(\n      subject,\n      delim = \"_\",\n      names = c(\"subject\", \"number\")\n    ) |> \n    mutate(subject = paste0(subject,\"_b_aph_v\", number), .keep = \"unused\"),\n  \n  # phant spatial\n  df_embed_vs_sub  |>\n    separate_wider_delim(\n      subject,\n      delim = \"_\",\n      names = c(\"subject\", \"number\")\n    ) |> \n    mutate(subject = paste0(subject,\"_c_phant_s\", number), .keep = \"unused\"),\n  \n  # phant visual\n  df_embed_v_sub  |>\n    separate_wider_delim(\n      subject,\n      delim = \"_\",\n      names = c(\"subject\", \"number\")\n    ) |> \n    mutate(subject = paste0(subject,\"_d_phant_v\", number), .keep = \"unused\")\n  ) |>\n  plot_ly() |> \n  add_markers(\n      x = ~x, y = ~y, z = ~z,\n      color = ~subject,\n      colors = cool_30_colors,\n      marker = list(size = 3)\n    ) |> \n  layout(\n    paper_bgcolor = \"transparent\",\n    showlegend = FALSE\n    )\n```\n:::\n\n\n\n:::{.content-hidden when-format=\"pdf\"}\n::: column-page-inset\n\n\n::: {#fig-subject-embeddings-html .cell layout-ncol=\"2\"}\n\n```{.r .cell-code .hidden}\nsize = 2\n\nplot_ly(\n    type = \"scatter3d\", \n    mode = \"marker\",\n    colors = ~pal_okabe_ito\n    ) |> \n  add_markers(\n    data = df_embed_c_sub, \n    x = ~x, y = ~y, z = ~z, color  = ~paste0(\"Species group \", group),\n    mode = \"marker\",\n    marker = list(size = size, symbol = \"circle\")\n  ) |> \n  add_markers(\n    data = df_embed_v_sub, \n    x = ~x, y = ~y, z = ~z, color  = ~paste0(\"Species group \", group),\n    mode = \"marker\",\n    marker = list(size = size, symbol = \"diamond\")\n  ) |> \n  add_markers(\n    data = df_embed_cs_sub, \n    x = ~x, y = ~y, z = ~z, color  = ~paste0(\"Species group \", group), \n    mode = \"marker\",\n    marker = list(size = size, symbol = \"circle-open\")\n  ) |> \n  add_markers(\n    data = df_embed_vs_sub, \n    x = ~x, y = ~y, z = ~z, color  = ~paste0(\"Species group \", group), \n    mode = \"marker\",\n    marker = list(size = size, symbol = \"diamond-open\")\n  ) |> \n  layout(legend = list(\n    yanchor = \"top\",\n    y = 1,\n    xanchor = \"right\",\n    x = 0\n    ),\n    paper_bgcolor = \"transparent\",\n    showlegend = FALSE\n    )\n```\n\n::: {.cell-output-display}\n![Distribution of the embeddings of the 30 subjects,***colored by the species groups*** <br>they represent. The symbols represent the four imagery groups (Aph. verbal, spatial, etc.)](index_files/figure-docx/fig-subject-embeddings-html-1.png){#fig-subject-embeddings-html-1}\n:::\n\n```{.r .cell-code .hidden}\nbind_rows(\n  # aph_spatial\n  df_embed_cs_sub |> \n    separate_wider_delim(\n      subject,\n      delim = \"_\",\n      names = c(\"subject\", \"number\")\n    ) |> \n    mutate(subject = paste0(subject,\"_a_aph_s\", number), .keep = \"unused\"),\n  \n  # aph_verbal\n  df_embed_c_sub  |>\n    separate_wider_delim(\n      subject,\n      delim = \"_\",\n      names = c(\"subject\", \"number\")\n    ) |> \n    mutate(subject = paste0(subject,\"_b_aph_v\", number), .keep = \"unused\"),\n  \n  # phant spatial\n  df_embed_vs_sub  |>\n    separate_wider_delim(\n      subject,\n      delim = \"_\",\n      names = c(\"subject\", \"number\")\n    ) |> \n    mutate(subject = paste0(subject,\"_c_phant_s\", number), .keep = \"unused\"),\n  \n  # phant visual\n  df_embed_v_sub  |>\n    separate_wider_delim(\n      subject,\n      delim = \"_\",\n      names = c(\"subject\", \"number\")\n    ) |> \n    mutate(subject = paste0(subject,\"_d_phant_v\", number), .keep = \"unused\")\n  ) |>\n  plot_ly() |> \n  add_markers(\n      x = ~x, y = ~y, z = ~z,\n      color = ~subject,\n      colors = cool_30_colors,\n      marker = list(size = 3)\n    ) |> \n  layout(\n    paper_bgcolor = \"transparent\",\n    showlegend = FALSE\n    )\n```\n\n::: {.cell-output-display}\n![Distribution of the embeddings of the 30 subjects, ***colored by subject***.<br>This is the only information the unsupervised algorithm will have to work with.](index_files/figure-docx/fig-subject-embeddings-html-2.png){#fig-subject-embeddings-html-2}\n:::\n\nFinal distribution of the 64 embeddings of all the 30 subjects, amounting to 1920 points total. ***Interact with the figures to see the details.***\n:::\n\n\n:::\n:::\n\n:::{.content-visible when-format=\"pdf\"}\n\n\n::: {#fig-subject-embeddings-pdf .cell layout-ncol=\"2\"}\n\n```{.r .cell-code .hidden}\nsize = 2\n\nplot_ly(\n    type = \"scatter3d\", \n    mode = \"marker\",\n    colors = ~pal_okabe_ito\n    ) |> \n  add_markers(\n    data = df_embed_c_sub, \n    x = ~x, y = ~y, z = ~z, color  = ~paste0(\"Species group \", group),\n    mode = \"marker\",\n    marker = list(size = size, symbol = \"circle\")\n  ) |> \n  add_markers(\n    data = df_embed_v_sub, \n    x = ~x, y = ~y, z = ~z, color  = ~paste0(\"Species group \", group),\n    mode = \"marker\",\n    marker = list(size = size, symbol = \"diamond\")\n  ) |> \n  add_markers(\n    data = df_embed_cs_sub, \n    x = ~x, y = ~y, z = ~z, color  = ~paste0(\"Species group \", group), \n    mode = \"marker\",\n    marker = list(size = size, symbol = \"circle-open\")\n  ) |> \n  add_markers(\n    data = df_embed_vs_sub, \n    x = ~x, y = ~y, z = ~z, color  = ~paste0(\"Species group \", group), \n    mode = \"marker\",\n    marker = list(size = size, symbol = \"diamond-open\")\n  ) |> \n  layout(legend = list(\n    yanchor = \"top\",\n    y = 1,\n    xanchor = \"right\",\n    x = 0\n    ),\n    paper_bgcolor = \"transparent\",\n    showlegend = FALSE\n    )\n```\n\n::: {.cell-output-display}\n![Distribution of the embeddings of the 30 subjects,***colored by the species groups*** <br>they represent. The symbols represent the four imagery groups (Aph. verbal, spatial, etc.)](index_files/figure-docx/fig-subject-embeddings-pdf-1.png){#fig-subject-embeddings-pdf-1}\n:::\n\n```{.r .cell-code .hidden}\nbind_rows(\n  # aph_spatial\n  df_embed_cs_sub |> \n    separate_wider_delim(\n      subject,\n      delim = \"_\",\n      names = c(\"subject\", \"number\")\n    ) |> \n    mutate(subject = paste0(subject,\"_a_aph_s\", number), .keep = \"unused\"),\n  \n  # aph_verbal\n  df_embed_c_sub  |>\n    separate_wider_delim(\n      subject,\n      delim = \"_\",\n      names = c(\"subject\", \"number\")\n    ) |> \n    mutate(subject = paste0(subject,\"_b_aph_v\", number), .keep = \"unused\"),\n  \n  # phant spatial\n  df_embed_vs_sub  |>\n    separate_wider_delim(\n      subject,\n      delim = \"_\",\n      names = c(\"subject\", \"number\")\n    ) |> \n    mutate(subject = paste0(subject,\"_c_phant_s\", number), .keep = \"unused\"),\n  \n  # phant visual\n  df_embed_v_sub  |>\n    separate_wider_delim(\n      subject,\n      delim = \"_\",\n      names = c(\"subject\", \"number\")\n    ) |> \n    mutate(subject = paste0(subject,\"_d_phant_v\", number), .keep = \"unused\")\n  ) |>\n  plot_ly() |> \n  add_markers(\n      x = ~x, y = ~y, z = ~z,\n      color = ~subject,\n      colors = cool_30_colors,\n      marker = list(size = 3)\n    ) |> \n  layout(\n    paper_bgcolor = \"transparent\",\n    showlegend = FALSE\n    )\n```\n\n::: {.cell-output-display}\n![Distribution of the embeddings of the 30 subjects, ***colored by subject***.<br>This is the only information the unsupervised algorithm will have to work with.](index_files/figure-docx/fig-subject-embeddings-pdf-2.png){#fig-subject-embeddings-pdf-2}\n:::\n\nFinal distribution of the 64 embeddings of all the 30 subjects, amounting to 1920 points total. ***Interact with the figures to see the details.***\n:::\n\n\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# ─── Grouping the 64 embeddings of each subject in their own dataframe ───\n\ndf_embeddings_sub <-\n  bind_rows(\n    # aphantasic spatial\n    df_embed_cs_sub |> \n      separate_wider_delim(\n        subject,\n        delim = \"_\",\n        names = c(\"subject\", \"number\")\n      ) |> \n      mutate(subject = paste0(subject,\"_a_aph_s\", number), .keep = \"unused\"),\n    \n    # aphantasic verbal\n    df_embed_c_sub  |>\n      separate_wider_delim(\n        subject,\n        delim = \"_\",\n        names = c(\"subject\", \"number\")\n      ) |> \n      mutate(subject = paste0(subject,\"_b_aph_v\", number), .keep = \"unused\"),\n    \n    # phantasic spatial\n    df_embed_vs_sub  |>\n      separate_wider_delim(\n        subject,\n        delim = \"_\",\n        names = c(\"subject\", \"number\")\n      ) |> \n      mutate(subject = paste0(subject,\"_c_phant_s\", number), .keep = \"unused\"),\n    \n    # phantasic visual\n    df_embed_v_sub  |>\n      separate_wider_delim(\n        subject,\n        delim = \"_\",\n        names = c(\"subject\", \"number\")\n      ) |> \n      mutate(subject = paste0(subject,\"_d_phant_v\", number), .keep = \"unused\")\n  ) |> \n  mutate(subject = as.factor(subject)) |> \n  select(!c(group, species)) |> \n  group_by(subject) |> \n  nest() |> \n  rename(embedding = data) |> \n  rowwise() |> \n  mutate(embedding = list(as.matrix(embedding)))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# number of available cores\nn_cores <- as.integer(parallel::detectCores() - 1)\n\n# all the categories of animals\nanimal_categories <- paste0(\"genre_\", letters[1:8])\n\n# the ids of each animal\nanimal_names <- \n  paste0(\"species_\",  letters[1:8]) |> \n  paste(rep(seq(1, 8, 1), each = 8), sep = \"\")\n\n# the indexes of each animal, linking them to the categories\nanimal_indexes <- list(\n  rep(0:7), \n  rep(8:15),\n  rep(16:23), \n  rep(24:31), \n  rep(32:39), \n  rep(40:47), \n  rep(48:55), \n  rep(56:63) \n  ) |> \n  lapply(as.integer) |> \n  lapply(as.array)\n\n# number of animals per category\nanimal_n_per_cat <- rep(8, each = 8) |> as.integer()\n\n# matrix of the category per animal\nanimal_matrix <-\n  tibble(\n  genre     = rep(animal_categories, each = 8), \n  species   = sort(animal_names),\n  belonging = rep(1, 64)\n  ) |> \n  pivot_wider(\n    names_from  = genre,\n    values_from = belonging\n  ) |> \n  mutate(across(everything(), ~replace_na(.x, 0))) |> \n  as.data.frame() |> \n  select(!species)\n\nrow.names(animal_matrix) <- animal_names |> sort()\n\n# These structures will be loaded right away when we'll transition to Python.\n```\n:::\n\n\n\n## Data analysis: Aligning representational structures\n\nFor all this section, we need to adapt a simple version of the explanations from @kawakita2023 and @sasakiToolboxGromovWassersteinOptimal2023 and avoid any technical aspects in the main manuscript.\n\nFrom there on, most of the code follows the instructions from the open-source scientific toolbox by @sasakiToolboxGromovWassersteinOptimal2023. I added a few explanations on the purpose of each step, without diving into unnecessary details.\n\n### Step 1: Importing the embeddings in the Python instances {.unnumbered}\n\n### Step 2: Setting the parameters for the optimization of GWOT {.unnumbered}\n\n### Step 3: Gromov-Wasserstein Optimal Transport (GWOT) between Representations {.unnumbered}\n\n### Step 4: Evaluation and Visualization\n\n#### Clustering the subjects by alignment accuracy\n\nFirst, we evaluate the accuracy per subject and group the subjects based on the alignment accuracy via hierarchical clustering. This procedure is represented in @fig-hclust. Second, we evaluate the accuracy of the alignment between these clusters.\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\ngroup_aph_s  <- paste0(\"Subject_\", seq(1, 7, 1))\ngroup_aph_v   <- paste0(\"Subject_\", seq(8, 15, 1))\ngroups_aph    <- paste0(\"Subject_\", seq(1, 15, 1))\n\ngroup_phant_s  <- paste0(\"Subject_\", seq(16, 22, 1))\ngroup_phant_v   <- paste0(\"Subject_\", seq(23, 30, 1))\ngroups_phant    <- paste0(\"Subject_\", seq(16, 30, 1))\n\nsummarise_accuracy <- function(df, name){\n  df <- \n    df |> \n    mutate(k = c(1, 5, 10)) |> \n    pivot_longer(\n      !k,\n      names_to  = \"comparisons\",\n      values_to = \"accuracy\" \n    ) |> \n    pivot_wider(\n      names_from = \"k\",\n      values_from = \"accuracy\"\n    ) |> \n    rename_with(.fn = ~paste0(\"k_\", name, \"_\", .x), .cols = c(2, 3, 4)\n    ) |> \n    separate_wider_delim(\n      comparisons, \n      delim = \"_vs_\",\n      names = c(\"group_a\", \"group_b\")\n      )\n  \n  return(df)\n}\n\ndf_accuracy_all <- \n  summarise_accuracy(df_accuracy_all_bad, \"all\") |>\n  mutate(mean_acc_all = (k_all_1 + k_all_5 + k_all_10)/3)\n\ndf_accuracy_cat <- \n  summarise_accuracy(df_accuracy_cat_bad, \"cat\") |>\n  mutate(mean_acc_cat = (k_cat_1 + k_cat_5 + k_cat_10)/3) \n\ndf_accuracy <-\n  left_join(\n    df_accuracy_all, \n    df_accuracy_cat, \n    by = c(\"group_a\", \"group_b\")\n    ) |> \n  mutate(mean_acc = (mean_acc_all + mean_acc_cat) /2) |> \n  select(group_a, group_b, mean_acc)\n\ndist_matrix <-\n  structure(\n    100 - df_accuracy$mean_acc, \n    Size = 30, \n    Labels = 1:30, \n    method = \"user\", \n    class = \"dist\"\n    )\n\nclustering <- hclust(dist_matrix)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nclusters <- clustering |> cutree(k = 9)\n\n# 9 Cluster solution\ncluster_1 <- paste0(\"Subject_\", c(1,14))\ncluster_2 <- paste0(\"Subject_\", c(3,6,7))\ncluster_3 <- paste0(\"Subject_\", c(5,15))\ncluster_4 <- paste0(\"Subject_\", c(8,9,10,11))\ncluster_5 <- paste0(\"Subject_\", c(12,13))\ncluster_6 <- paste0(\"Subject_\", c(16,17,23,25,30))\ncluster_7 <- paste0(\"Subject_\", c(18,19,26,28))\ncluster_8 <- paste0(\"Subject_\", c(20,21,22,24,27,29))\ncluster_9 <- paste0(\"Subject_\", c(2,4))\n\ndf_9_cluster_accuracy <-\n  df_accuracy |> \n  mutate(\n    group_a = case_when(\n      group_a %in% cluster_1 ~ \"Cluster 1\",\n      group_a %in% cluster_2 ~ \"Cluster 2\",\n      group_a %in% cluster_3 ~ \"Cluster 3\",\n      group_a %in% cluster_4 ~ \"Cluster 4\",\n      group_a %in% cluster_5 ~ \"Cluster 5\",\n      group_a %in% cluster_6 ~ \"Cluster 6\",\n      group_a %in% cluster_7 ~ \"Cluster 7\",\n      group_a %in% cluster_8 ~ \"Cluster 8\",\n      group_a %in% cluster_9 ~ \"Cluster 9\"\n    ),\n    group_b = case_when(\n      group_b %in% cluster_1 ~ \"Cluster 1\",\n      group_b %in% cluster_2 ~ \"Cluster 2\",\n      group_b %in% cluster_3 ~ \"Cluster 3\",\n      group_b %in% cluster_4 ~ \"Cluster 4\",\n      group_b %in% cluster_5 ~ \"Cluster 5\",\n      group_b %in% cluster_6 ~ \"Cluster 6\",\n      group_b %in% cluster_7 ~ \"Cluster 7\",\n      group_b %in% cluster_8 ~ \"Cluster 8\",\n      group_b %in% cluster_9 ~ \"Cluster 9\"\n    )\n  ) |> \n  rowwise() |>\n  mutate(alignment = paste0(sort(c(group_a,group_b))[1], \" - \", sort(c(group_a,group_b))[2])) |>\n  group_by(alignment) |>\n  summarise(accuracy = mean(mean_acc)) |> \n  separate_wider_delim(alignment, \" - \", names = c(\"cluster_a\", \"cluster_b\"))\n```\n:::\n\n::: {#cell-fig-hclust .cell}\n\n```{.r .cell-code .hidden}\ndendro <-\n  clustering |>\n  as.dendrogram() |> \n  dendro_data(type = \"rectangle\")\n\ndf_plot_cluster <- \n  data.frame(\n    label = names(clusters), \n    cluster = factor(clusters)\n    )\n\ndendro[[\"labels\"]] <- merge(dendro[[\"labels\"]], df_plot_cluster, by = \"label\")\n\nggplot() +\n  geom_segment(\n    data = dendro$segments,\n    aes(x = x, y = y, xend = xend, yend = yend),\n    color = \"white\"\n    ) +\n  geom_hline(yintercept = 75, color = \"grey\", linetype = 2, linewidth = .5) +\n  geom_text(\n    data = dendro$labels,\n    aes(x = x, y = y, label = label, color = cluster),\n    size = 5,\n    nudge_y = -7\n  ) +\n  labs(y = \"Height\", x = \"Subject\") +\n  scale_color_manual(\n    values = c(\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F5C710\", \n               \"#7AD151FF\", \"#D55E00\", \"#CC79A7\", \"#6c0009\", \"#414487FF\"), \n    name = \"Cluster\"\n  ) +\n  theme_blackboard() +\n  theme(\n    axis.line.y = element_blank(),\n    axis.line.x = element_blank(),\n    axis.text.x = element_blank(),\n    plot.background = element_rect(color = \"black\")\n  )\n```\n\n::: {.cell-output-display}\n![Hierachical clustering of the 30 subjects based on their representational alignment.](index_files/figure-docx/fig-hclust-1.png){#fig-hclust width=100%}\n:::\n:::\n\n::: {#cell-fig-clusters-accuracy .cell}\n\n```{.r .cell-code .hidden}\ndf_9_cluster_accuracy |> \n  ggplot(aes(x = cluster_a, y = cluster_b, fill = accuracy)) +\n  geom_tile(color = \"black\", linewidth = .5) +\n  geom_text(aes(label = round(accuracy/100, digits  =2)), color = \"black\") +\n  scale_fill_viridis(guide = \"none\") +\n  theme_blackboard() +\n  theme(\n    axis.title.x = element_blank(),\n    axis.title.y = element_blank(),\n    axis.line  = element_blank(),\n    plot.background = element_rect(color = \"black\")\n  )\n```\n\n::: {.cell-output-display}\n![Accuracy of the alignments between the subject's embeddings in each cluster. An alignment of a cluster with itself (e.g. Cluster 7 - Cluster 7) is the evaluation of the alignment of the subjects ***inside*** the cluster.](index_files/figure-docx/fig-clusters-accuracy-1.png){#fig-clusters-accuracy}\n:::\n:::\n\n\n\n#### Evaluating the clusters in light of our theoretical OSV model\n\nLet's see the composition of the clusters in light of our initial O-S-V model. The cognitive profiles of the subjects are represented in @fig-osv-clusters, and the distribution of the cognitive profiles in the clusters is represented in @fig-clusters-distribution.\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\ndf_coordinates <- coordinates_aligned_embeddings |> lapply(as.data.frame)\n\ndf_aligned_clustered <-\n  tibble(\n    embedding = list(df_coordinates)\n  ) |> \n  unnest(embedding) |> \n  unnest(embedding) |>\n  mutate(\n    species = rep(animal_names, 30),\n    species_group = paste0(\"species_group_\", rep(rep(1:8, each = 8), 30)),\n    subject = rep(pull(df_embeddings_sub, subject), each = 64),\n    group   = rep(pull(df, group), each = 64),\n    vis_spa_group = rep(pull(df, vis_spa_group), each = 64)\n  ) |> \n  rename(x = V1, y = V2, z = V3) |> \n  select(\n    subject,\n    group,\n    vis_spa_group,\n    species,\n    species_group,\n    x, y, z) |> \n  mutate(clusters = as.factor(rep(clusters, each = 64))) |> \n  select(subject, group, clusters, everything())\n\n# analyzing subject assignations\ndf_aligned_subjects <-\n  df_aligned_clustered |> \n  select(subject:vis_spa_group) |> \n  group_by(subject) |> \n  unique()\n\ndf_clustered <- bind_cols(df_aligned_subjects, df[,c(6:8)])\n```\n:::\n\n::: {#cell-fig-osv-clusters .cell}\n\n```{.r .cell-code .hidden}\ndf_clustered |> \n  plot_ly(\n    x = ~visual_imagery,\n    y = ~spatial_imagery,\n    z = ~verbal_profile,\n    color = ~paste0(\"Cluster \", clusters),\n    text = ~paste0(\"Cluster \", clusters),\n    colors = c(\n      \"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F5C710\", \n      \"#7AD151FF\", \"#D55E00\", \"#CC79A7\", \"#6c0009\", \"#414487FF\"),\n    type = \"scatter3d\",\n    mode = \"markers+text\",\n    marker = list(size = 6),\n    textfont = list(size = 11)\n  ) |> \n  layout(\n    scene = list(\n      xaxis = list(\n        title = list(text = \"Visual imagery\", font = list(color = \"grey\")),\n        tickfont = list(color = \"grey\")\n        ),\n      yaxis = list(\n        title = list(text = \"Spatial imagery\", font = list(color = \"grey\")),\n        tickfont = list(color = \"grey\")\n        ),\n      zaxis = list(\n        title = list(text = \"Verbal profile\", font = list(color = \"grey\")),\n        tickfont = list(color = \"grey\")\n        )\n    ),\n    # legend = list(title = list(text = \"Group\")),\n    paper_bgcolor = \"transparent\",\n    showlegend = FALSE\n    )\n```\n\n::: {.cell-output-display}\n![Imagery profiles of the nine identified clusters on the three object, spatial, and verbal dimensions. ***Interact with the figure to see the details.***](index_files/figure-docx/fig-osv-clusters-1.png){#fig-osv-clusters}\n:::\n:::\n\n::: {#cell-fig-clusters-distribution .cell}\n\n```{.r .cell-code .hidden}\nnames = c(\"Aph. spatial\", \"Aph. verbal\", \"Phant. spatial\", \"Phant. visual\")\n\ndf_clustered |> \n  ggplot(aes(x = clusters, fill = vis_spa_group, color = vis_spa_group)) +\n  geom_bar(alpha = .5, position = position_dodge()) +\n  labs(x = \"Cluster\", y = \"Count\") +\n  scale_color_manual(values = pal_okabe_ito, labels = names, name = \"Group\") +\n  scale_fill_manual(values = pal_okabe_ito, labels = names, name = \"Group\") +\n  theme_blackboard() +\n  theme(plot.background = element_rect(color = \"black\"))\n```\n\n::: {.cell-output-display}\n![Repartion of our intial O-S-V groups in the clusters created by the unsupervised alignment.](index_files/figure-docx/fig-clusters-distribution-1.png){#fig-clusters-distribution}\n:::\n:::\n\n\n\nNow let's visualize the embeddings of the subjects in each cluster to get a visual idea of their representational structures and the intra-cluster alignment between the subjects.\n\n\n\n::: {.cell .hidden}\n\n```{.r .cell-code .hidden}\nsize = 4\n\n# Cluster 1\nplot_ly(\n  type = \"scatter3d\", \n  mode = \"marker+text\", \n  colors = c(\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F5C710\", \n             \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#6c0009\")) |> \n  add_markers(\n    data = df_aligned_clustered |> filter(clusters == \"1\"),\n    x = ~x, y = ~y, z = ~z,\n    color = ~paste0(\"Species group \", species_group),\n    marker = list(size = size, symbol = \"circle\")\n  ) |> \n  layout(\n    paper_bgcolor = \"transparent\",\n    showlegend = FALSE\n    )\n\n# Cluster 2\nplot_ly(\n  type = \"scatter3d\", \n  mode = \"marker+text\", \n  colors = c(\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F5C710\", \n             \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#6c0009\")) |> \n  add_markers(\n    data = df_aligned_clustered |> filter(clusters == \"2\"),\n    x = ~x, y = ~y, z = ~z,\n    color = ~paste0(\"Species group \", species_group),\n    marker = list(size = size, symbol = \"circle\")\n  ) |> \n  layout(\n    paper_bgcolor = \"transparent\",\n    showlegend = FALSE\n    )\n\n# Cluster 3\nplot_ly(\n  type = \"scatter3d\", \n  mode = \"marker+text\", \n  colors = c(\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F5C710\", \n             \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#6c0009\")) |> \n  add_markers(\n    data = df_aligned_clustered |> filter(clusters == \"3\"),\n    x = ~x, y = ~y, z = ~z,\n    color = ~paste0(\"Species group \", species_group),\n    marker = list(size = size, symbol = \"circle\")\n  ) |> \n  layout(\n    paper_bgcolor = \"transparent\",\n    showlegend = FALSE\n    )\n\n# Cluster 4\nplot_ly(\n  type = \"scatter3d\", \n  mode = \"marker+text\", \n  colors = c(\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F5C710\", \n             \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#6c0009\")) |> \n  add_markers(\n    data = df_aligned_clustered |> filter(clusters == \"4\"),\n    x = ~x, y = ~y, z = ~z,\n    color = ~species_group,\n    marker = list(size = size, symbol = \"circle\")\n  ) |> \n  layout(\n    paper_bgcolor = \"transparent\",\n    showlegend = FALSE\n    )\n\n# Cluster 5\nplot_ly(\n  type = \"scatter3d\", \n  mode = \"marker+text\", \n  colors = c(\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F5C710\", \n             \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#6c0009\")) |> \n  add_markers(\n    data = df_aligned_clustered |> filter(clusters == \"5\"),\n    x = ~x, y = ~y, z = ~z,\n    color = ~species_group,\n    marker = list(size = size, symbol = \"circle\")\n  ) |> \n  layout(\n    paper_bgcolor = \"transparent\",\n    showlegend = FALSE\n    )\n\n# Cluster 6\nplot_ly(\n  type = \"scatter3d\", \n  mode = \"marker+text\", \n  colors = c(\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F5C710\", \n             \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#6c0009\")) |> \n  add_markers(\n    data = df_aligned_clustered |> filter(clusters == \"6\"),\n    x = ~x, y = ~y, z = ~z,\n    color = ~species_group,\n    marker = list(size = size, symbol = \"circle\")\n  ) |> \n  layout(\n    paper_bgcolor = \"transparent\",\n    showlegend = FALSE\n    )\n\n# Cluster 7\nplot_ly(\n  type = \"scatter3d\", \n  mode = \"marker+text\", \n  colors = c(\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F5C710\", \n             \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#6c0009\")) |> \n  add_markers(\n    data = df_aligned_clustered |> filter(clusters == \"7\"),\n    x = ~x, y = ~y, z = ~z,\n    color = ~species_group,\n    marker = list(size = size, symbol = \"circle\")\n  ) |> \n  layout(\n    paper_bgcolor = \"transparent\",\n    showlegend = FALSE\n    )\n\n# Cluster 8\nplot_ly(\n  type = \"scatter3d\", \n  mode = \"marker+text\", \n  colors = c(\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F5C710\", \n             \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#6c0009\")) |> \n  add_markers(\n    data = df_aligned_clustered |> filter(clusters == \"8\"),\n    x = ~x, y = ~y, z = ~z,\n    color = ~species_group,\n    marker = list(size = size, symbol = \"circle\")\n  ) |> \n  layout(\n    paper_bgcolor = \"transparent\",\n    showlegend = FALSE\n    )\n\n# Cluster 9\nplot_ly(\n  type = \"scatter3d\", \n  mode = \"marker+text\", \n  colors = c(\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F5C710\", \n             \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#6c0009\")) |> \n  add_markers(\n    data = df_aligned_clustered |> filter(clusters == \"9\"),\n    x = ~x, y = ~y, z = ~z,\n    color = ~species_group,\n    marker = list(size = size, symbol = \"circle\")\n  ) |> \n  layout(\n    paper_bgcolor = \"transparent\",\n    showlegend = FALSE\n    )\n```\n:::\n\n\n\nLet's try referencing [@fig-clusters-final-html-1]{.content-hidden when-format=\"pdf\"} [@fig-clusters-final-pdf-1]{.content-visible when-format=\"pdf\"}, or [@fig-clusters-final-html-9]{.content-hidden when-format=\"pdf\"} [@fig-clusters-final-pdf-9]{.content-visible when-format=\"pdf\"}, the last one.\n\n:::{.content-hidden when-format=\"pdf\"}\n::: column-page-inset\n\n\n::: {#fig-clusters-final-html .cell .fig-cap-location-top layout-nrow=\"3\" layout-ncol=\"3\"}\n\n```{.r .cell-code .hidden}\nsize = 4\n\n# Cluster 1\nplot_ly(\n  type = \"scatter3d\", \n  mode = \"marker+text\", \n  colors = c(\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F5C710\", \n             \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#6c0009\")) |> \n  add_markers(\n    data = df_aligned_clustered |> filter(clusters == \"1\"),\n    x = ~x, y = ~y, z = ~z,\n    color = ~paste0(\"Species group \", species_group),\n    marker = list(size = size, symbol = \"circle\")\n  ) |> \n  layout(\n    paper_bgcolor = \"transparent\",\n    showlegend = FALSE\n    )\n```\n\n::: {.cell-output-display}\n![**Cluster 1** embeddings (Aphantasic cluster).<br>Within-cluster aligment accuracy = 95.83%.](index_files/figure-docx/fig-clusters-final-html-1.png){#fig-clusters-final-html-1}\n:::\n\n```{.r .cell-code .hidden}\n# Cluster 2\nplot_ly(\n  type = \"scatter3d\", \n  mode = \"marker+text\", \n  colors = c(\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F5C710\", \n             \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#6c0009\")) |> \n  add_markers(\n    data = df_aligned_clustered |> filter(clusters == \"2\"),\n    x = ~x, y = ~y, z = ~z,\n    color = ~paste0(\"Species group \", species_group),\n    marker = list(size = size, symbol = \"circle\")\n  ) |> \n  layout(\n    paper_bgcolor = \"transparent\",\n    showlegend = FALSE\n    )\n```\n\n::: {.cell-output-display}\n![**Cluster 2** embeddings (Spatial aphantasic cluster).<br>Within-cluster aligment accuracy = 97.66%.](index_files/figure-docx/fig-clusters-final-html-2.png){#fig-clusters-final-html-2}\n:::\n\n```{.r .cell-code .hidden}\n# Cluster 3\nplot_ly(\n  type = \"scatter3d\", \n  mode = \"marker+text\", \n  colors = c(\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F5C710\", \n             \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#6c0009\")) |> \n  add_markers(\n    data = df_aligned_clustered |> filter(clusters == \"3\"),\n    x = ~x, y = ~y, z = ~z,\n    color = ~paste0(\"Species group \", species_group),\n    marker = list(size = size, symbol = \"circle\")\n  ) |> \n  layout(\n    paper_bgcolor = \"transparent\",\n    showlegend = FALSE\n    )\n```\n\n::: {.cell-output-display}\n![**Cluster 3** embeddings (Spatial aphantasic cluster).<br>Within-cluster aligment accuracy = 94.01%.](index_files/figure-docx/fig-clusters-final-html-3.png){#fig-clusters-final-html-3}\n:::\n\n```{.r .cell-code .hidden}\n# Cluster 4\nplot_ly(\n  type = \"scatter3d\", \n  mode = \"marker+text\", \n  colors = c(\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F5C710\", \n             \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#6c0009\")) |> \n  add_markers(\n    data = df_aligned_clustered |> filter(clusters == \"4\"),\n    x = ~x, y = ~y, z = ~z,\n    color = ~species_group,\n    marker = list(size = size, symbol = \"circle\")\n  ) |> \n  layout(\n    paper_bgcolor = \"transparent\",\n    showlegend = FALSE\n    )\n```\n\n::: {.cell-output-display}\n![**Cluster 4** embeddings (Aphantasic cluster).<br>Within-cluster aligment accuracy = 85.03%.](index_files/figure-docx/fig-clusters-final-html-4.png){#fig-clusters-final-html-4}\n:::\n\n```{.r .cell-code .hidden}\n# Cluster 5\nplot_ly(\n  type = \"scatter3d\", \n  mode = \"marker+text\", \n  colors = c(\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F5C710\", \n             \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#6c0009\")) |> \n  add_markers(\n    data = df_aligned_clustered |> filter(clusters == \"5\"),\n    x = ~x, y = ~y, z = ~z,\n    color = ~species_group,\n    marker = list(size = size, symbol = \"circle\")\n  ) |> \n  layout(\n    paper_bgcolor = \"transparent\",\n    showlegend = FALSE\n    )\n```\n\n::: {.cell-output-display}\n![**Cluster 5** embeddings (Verbal aphantasic cluster).<br>Within-cluster aligment accuracy = 96.88%.](index_files/figure-docx/fig-clusters-final-html-5.png){#fig-clusters-final-html-5}\n:::\n\n```{.r .cell-code .hidden}\n# Cluster 6\nplot_ly(\n  type = \"scatter3d\", \n  mode = \"marker+text\", \n  colors = c(\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F5C710\", \n             \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#6c0009\")) |> \n  add_markers(\n    data = df_aligned_clustered |> filter(clusters == \"6\"),\n    x = ~x, y = ~y, z = ~z,\n    color = ~species_group,\n    marker = list(size = size, symbol = \"circle\")\n  ) |> \n  layout(\n    paper_bgcolor = \"transparent\",\n    showlegend = FALSE\n    )\n```\n\n::: {.cell-output-display}\n![**Cluster 6** embeddings (Verbal aphantasic cluster).<br>Within-cluster aligment accuracy = 81.80%.](index_files/figure-docx/fig-clusters-final-html-6.png){#fig-clusters-final-html-6}\n:::\n\n```{.r .cell-code .hidden}\n# Cluster 7\nplot_ly(\n  type = \"scatter3d\", \n  mode = \"marker+text\", \n  colors = c(\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F5C710\", \n             \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#6c0009\")) |> \n  add_markers(\n    data = df_aligned_clustered |> filter(clusters == \"7\"),\n    x = ~x, y = ~y, z = ~z,\n    color = ~species_group,\n    marker = list(size = size, symbol = \"circle\")\n  ) |> \n  layout(\n    paper_bgcolor = \"transparent\",\n    showlegend = FALSE\n    )\n```\n\n::: {.cell-output-display}\n![**Cluster 7** embeddings (Phantasic cluster).<br>Within-cluster aligment accuracy = 98.23%.](index_files/figure-docx/fig-clusters-final-html-7.png){#fig-clusters-final-html-7}\n:::\n\n```{.r .cell-code .hidden}\n# Cluster 8\nplot_ly(\n  type = \"scatter3d\", \n  mode = \"marker+text\", \n  colors = c(\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F5C710\", \n             \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#6c0009\")) |> \n  add_markers(\n    data = df_aligned_clustered |> filter(clusters == \"8\"),\n    x = ~x, y = ~y, z = ~z,\n    color = ~species_group,\n    marker = list(size = size, symbol = \"circle\")\n  ) |> \n  layout(\n    paper_bgcolor = \"transparent\",\n    showlegend = FALSE\n    )\n```\n\n::: {.cell-output-display}\n![**Cluster 8** embeddings (Phantasic cluster).<br>Within-cluster aligment accuracy = 74.64%.](index_files/figure-docx/fig-clusters-final-html-8.png){#fig-clusters-final-html-8}\n:::\n\n```{.r .cell-code .hidden}\n# Cluster 9\nplot_ly(\n  type = \"scatter3d\", \n  mode = \"marker+text\", \n  colors = c(\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F5C710\", \n             \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#6c0009\")) |> \n  add_markers(\n    data = df_aligned_clustered |> filter(clusters == \"9\"),\n    x = ~x, y = ~y, z = ~z,\n    color = ~species_group,\n    marker = list(size = size, symbol = \"circle\")\n  ) |> \n  layout(\n    paper_bgcolor = \"transparent\",\n    showlegend = FALSE\n    )\n```\n\n::: {.cell-output-display}\n![**Cluster 9** embeddings (Phantasic cluster).<br>Within-cluster aligment accuracy = 42.45%.](index_files/figure-docx/fig-clusters-final-html-9.png){#fig-clusters-final-html-9}\n:::\n\nPsychological spaces (embeddings) of the 30 subjects, aligned and clustered with other subjects having the most similar representations. The eight colors represent the initial eight groups of species that each subject had to 'represent' with imagery (legends for these colors have been taken out for display clarity purposes). ***Interact with the figures to see the details.***\n:::\n\n\n:::\n:::\n\n:::{.content-visible when-format=\"pdf\"}\n\n\n::: {#fig-clusters-final-pdf .cell .fig-cap-location-top layout-nrow=\"3\" layout-ncol=\"3\"}\n\n```{.r .cell-code .hidden}\nsize = 4\n\n# Cluster 1\nplot_ly(\n  type = \"scatter3d\", \n  mode = \"marker+text\", \n  colors = c(\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F5C710\", \n             \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#6c0009\")) |> \n  add_markers(\n    data = df_aligned_clustered |> filter(clusters == \"1\"),\n    x = ~x, y = ~y, z = ~z,\n    color = ~paste0(\"Species group \", species_group),\n    marker = list(size = size, symbol = \"circle\")\n  ) |> \n  layout(\n    paper_bgcolor = \"transparent\",\n    showlegend = FALSE\n    )\n```\n\n::: {.cell-output-display}\n![**Cluster 1** embeddings (Aphantasic cluster).<br>Within-cluster aligment accuracy = 95.83%.](index_files/figure-docx/fig-clusters-final-pdf-1.png){#fig-clusters-final-pdf-1}\n:::\n\n```{.r .cell-code .hidden}\n# Cluster 2\nplot_ly(\n  type = \"scatter3d\", \n  mode = \"marker+text\", \n  colors = c(\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F5C710\", \n             \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#6c0009\")) |> \n  add_markers(\n    data = df_aligned_clustered |> filter(clusters == \"2\"),\n    x = ~x, y = ~y, z = ~z,\n    color = ~paste0(\"Species group \", species_group),\n    marker = list(size = size, symbol = \"circle\")\n  ) |> \n  layout(\n    paper_bgcolor = \"transparent\",\n    showlegend = FALSE\n    )\n```\n\n::: {.cell-output-display}\n![**Cluster 2** embeddings (Spatial aphantasic cluster).<br>Within-cluster aligment accuracy = 97.66%.](index_files/figure-docx/fig-clusters-final-pdf-2.png){#fig-clusters-final-pdf-2}\n:::\n\n```{.r .cell-code .hidden}\n# Cluster 3\nplot_ly(\n  type = \"scatter3d\", \n  mode = \"marker+text\", \n  colors = c(\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F5C710\", \n             \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#6c0009\")) |> \n  add_markers(\n    data = df_aligned_clustered |> filter(clusters == \"3\"),\n    x = ~x, y = ~y, z = ~z,\n    color = ~paste0(\"Species group \", species_group),\n    marker = list(size = size, symbol = \"circle\")\n  ) |> \n  layout(\n    paper_bgcolor = \"transparent\",\n    showlegend = FALSE\n    )\n```\n\n::: {.cell-output-display}\n![**Cluster 3** embeddings (Spatial aphantasic cluster).<br>Within-cluster aligment accuracy = 94.01%.](index_files/figure-docx/fig-clusters-final-pdf-3.png){#fig-clusters-final-pdf-3}\n:::\n\n```{.r .cell-code .hidden}\n# Cluster 4\nplot_ly(\n  type = \"scatter3d\", \n  mode = \"marker+text\", \n  colors = c(\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F5C710\", \n             \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#6c0009\")) |> \n  add_markers(\n    data = df_aligned_clustered |> filter(clusters == \"4\"),\n    x = ~x, y = ~y, z = ~z,\n    color = ~species_group,\n    marker = list(size = size, symbol = \"circle\")\n  ) |> \n  layout(\n    paper_bgcolor = \"transparent\",\n    showlegend = FALSE\n    )\n```\n\n::: {.cell-output-display}\n![**Cluster 4** embeddings (Aphantasic cluster).<br>Within-cluster aligment accuracy = 85.03%.](index_files/figure-docx/fig-clusters-final-pdf-4.png){#fig-clusters-final-pdf-4}\n:::\n\n```{.r .cell-code .hidden}\n# Cluster 5\nplot_ly(\n  type = \"scatter3d\", \n  mode = \"marker+text\", \n  colors = c(\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F5C710\", \n             \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#6c0009\")) |> \n  add_markers(\n    data = df_aligned_clustered |> filter(clusters == \"5\"),\n    x = ~x, y = ~y, z = ~z,\n    color = ~species_group,\n    marker = list(size = size, symbol = \"circle\")\n  ) |> \n  layout(\n    paper_bgcolor = \"transparent\",\n    showlegend = FALSE\n    )\n```\n\n::: {.cell-output-display}\n![**Cluster 5** embeddings (Verbal aphantasic cluster).<br>Within-cluster aligment accuracy = 96.88%.](index_files/figure-docx/fig-clusters-final-pdf-5.png){#fig-clusters-final-pdf-5}\n:::\n\n```{.r .cell-code .hidden}\n# Cluster 6\nplot_ly(\n  type = \"scatter3d\", \n  mode = \"marker+text\", \n  colors = c(\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F5C710\", \n             \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#6c0009\")) |> \n  add_markers(\n    data = df_aligned_clustered |> filter(clusters == \"6\"),\n    x = ~x, y = ~y, z = ~z,\n    color = ~species_group,\n    marker = list(size = size, symbol = \"circle\")\n  ) |> \n  layout(\n    paper_bgcolor = \"transparent\",\n    showlegend = FALSE\n    )\n```\n\n::: {.cell-output-display}\n![**Cluster 6** embeddings (Verbal aphantasic cluster).<br>Within-cluster aligment accuracy = 81.80%.](index_files/figure-docx/fig-clusters-final-pdf-6.png){#fig-clusters-final-pdf-6}\n:::\n\n```{.r .cell-code .hidden}\n# Cluster 7\nplot_ly(\n  type = \"scatter3d\", \n  mode = \"marker+text\", \n  colors = c(\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F5C710\", \n             \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#6c0009\")) |> \n  add_markers(\n    data = df_aligned_clustered |> filter(clusters == \"7\"),\n    x = ~x, y = ~y, z = ~z,\n    color = ~species_group,\n    marker = list(size = size, symbol = \"circle\")\n  ) |> \n  layout(\n    paper_bgcolor = \"transparent\",\n    showlegend = FALSE\n    )\n```\n\n::: {.cell-output-display}\n![**Cluster 7** embeddings (Phantasic cluster).<br>Within-cluster aligment accuracy = 98.23%.](index_files/figure-docx/fig-clusters-final-pdf-7.png){#fig-clusters-final-pdf-7}\n:::\n\n```{.r .cell-code .hidden}\n# Cluster 8\nplot_ly(\n  type = \"scatter3d\", \n  mode = \"marker+text\", \n  colors = c(\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F5C710\", \n             \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#6c0009\")) |> \n  add_markers(\n    data = df_aligned_clustered |> filter(clusters == \"8\"),\n    x = ~x, y = ~y, z = ~z,\n    color = ~species_group,\n    marker = list(size = size, symbol = \"circle\")\n  ) |> \n  layout(\n    paper_bgcolor = \"transparent\",\n    showlegend = FALSE\n    )\n```\n\n::: {.cell-output-display}\n![**Cluster 8** embeddings (Phantasic cluster).<br>Within-cluster aligment accuracy = 74.64%.](index_files/figure-docx/fig-clusters-final-pdf-8.png){#fig-clusters-final-pdf-8}\n:::\n\n```{.r .cell-code .hidden}\n# Cluster 9\nplot_ly(\n  type = \"scatter3d\", \n  mode = \"marker+text\", \n  colors = c(\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F5C710\", \n             \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#6c0009\")) |> \n  add_markers(\n    data = df_aligned_clustered |> filter(clusters == \"9\"),\n    x = ~x, y = ~y, z = ~z,\n    color = ~species_group,\n    marker = list(size = size, symbol = \"circle\")\n  ) |> \n  layout(\n    paper_bgcolor = \"transparent\",\n    showlegend = FALSE\n    )\n```\n\n::: {.cell-output-display}\n![**Cluster 9** embeddings (Phantasic cluster).<br>Within-cluster aligment accuracy = 42.45%.](index_files/figure-docx/fig-clusters-final-pdf-9.png){#fig-clusters-final-pdf-9}\n:::\n\nPsychological spaces (embeddings) of the 30 subjects, aligned and clustered with other subjects having the most similar representations. The eight colors represent the initial eight groups of species that each subject had to 'represent' with imagery (legends for these colors have been taken out for display clarity purposes). ***Interact with the figures to see the details.***\n:::\n\n\n:::\n\n------------------------------------------------------------------------\n\n## Summary of the simulation analysis\n\n-   We generated subject data on subjective imagery based on the Object-Spatial-Verbal model of cognitive styles and representations\n\n-   We translated this model into a model of the distances between participants' mental representations\n\n-   We generated random data based on this theoretical model, on the representations of 64 items with categorical and visual properties by 30 participants\n\n-   We used an unsupervised alignment algorithm to judge the similarity between the representations of the subjects without any knowledge of their initial groups and relations\n\n-   The algorithm aligned with high precision 9 clusters of participants, which were coherent with the initial model we created, with several differences and unexpected alignments due to the randomization.\n\n-   The 9 clusters revealed a distinction between verbal aphantasics, spatial aphantasics, and phantasics in general. This interesting result shows that even though we tried to model spatial aphants and phantasics closer to each other, they all ended up separated based on visual imagery. This unexpected outcome, that went besides our initial intentions, shows that such an unsupervised method could reveal coherent patterns of representations that we did not expect, even with a relevant psychometric model.\n\nThis simulation motivates the idea that, should the imagery of participants be accurately fitted by our OSV model (or any other model to be tested), this paradigm and analytic method would be able to align the representations of participants with the same subjective imagery abilities.\n\n<!-- @kawakita2023: These results indicate that the difference between the qualia structures of neuro-typical and atypical participants is significantly larger than the difference between the qualia structures of neuro-typical participants. -->\n\n<!-- A notable difference is that greenish colors and reddish colors are close in the embedding space of color atypical participants while they are distant in the embedding space of color neurotypical participants. This structural difference is likely to prevent the unsupervised alignment between the embeddings of color-neurotypical and atypical participants even though the correlation coefficient between the dissimilarity matrices of color neuro-typical and atypical participants is reasonably high. -->\n\n<!-- For a long time, assessing the similarity of subjective experiences across participants has been challenging. To address this problem, we proposed the \"qualia structure\" paradigm, which focuses on quantitative structural comparisons of subjective experiences. Using an unsupervised alignment method, we were able to match the qualia structures of colors and natural objects of different groups of participants based only on the way the qualia relate to each other, without using any external labels. -->\n\n<!-- Our results on color qualia structures are consistent with an idea that the relational properties of color qualia are universally shared by color-neurotypical individuals. Intriguingly, our results also suggest that individuals with color-atypical vision may have a different structure of their color experiences, rather than just failing to experience a certain subset of colors. Longstanding thought experiments that challenge the feasibility of inter-subjective color comparisons, such as individuals with color qualia inversion, should be resolvable with our relational unsupervised approach. Beyond traditional measures such as Pearson's correlation coefficient, our method provides a more fundamental structural characterization of how two structures are similar or different, which will be crucial for future investigations of qualia structures across psychological, neuroscientific, and computational fields. -->\n\nI insist on a key finding : I did not use the data of the OSV model presented in @sec-osv-model-theory to generate the subject embeddings. The only hypothesis that guided how I simulated the subjects' embeddings was the distance model I envisioned, which is represented in @fig-diagram-intermediate. Thus, **the algorithm managed to reverse-engineer my logic**, to find the subjects groups I simulated with this logic, and it so happens that these groups' matched the cognitive profiles groups I built in the beginning.\n\nIn other words, the algorithm managed to find the common pattern - which was the groups pattern - between two models built differently, ***a common pattern that existed originally only in my head.***\n\nI think this \"mind-reading\"[^1] further argues for the potential of this method to reveal hidden patterns of inter-individual differences in subjective experience. These patterns could help build models of subjective mental imagery, one of the most challenging tasks in cognitive psychology to date.\n\n[^1]: Or, less prettily put, \"this unsupervised extraction of hidden representational features\".\n\n# Feasibility\n\n## Stimuli and study design\n\nThe simulation study presented here focused on aligning the representations ***between*** various participants, but a real study should go further and also analyse the similarities of representations ***within*** participants, for instance with a perception and an imagery condition. This was the basis of the study of @shepardSecondorderIsomorphismInternal1970, and remains a good starting point to design our own.\n\n## Online study materials {#sec-online-study-materials}\n\n## Analytic methods and collaborations\n\nI have proved (*mostly to myself*) that I was capable of implementing unsupervised GWOT alignment analysis in Python and R using the open-source toolbox provided by @sasakiToolboxGromovWassersteinOptimal2023, firstly to demonstrate that this key feasibility aspect was not out of reach (and that I was prepared to handle it). This toolbox is very recent (the associated article was posted on bioR$\\chi$iv last September) but is based on long-standing theory on similarity and cutting-edge topology analysis research. Consequently, I may not be confident enough on my expertise in these fields to state with confidence that my analyses of this project's data would be solid, which is a very important aspect for me. I need to be convinced by analytical choices, which are often taken for granted, to believe in their results and implications. I'm convinced of the relevance of what I've done here, but in the context a real application, even more expertise (and other analytical points of view) would be most welcome.\n\nTherefore, I think that this project could be the opportunity to collaborate with several teams working in these fields that have great data analysis expertise.\n\n-   Starting of course with [**Ladislas Nalborczyk**](https://lnalborczyk.github.io/) (who gave me this idea), who works on synesthesia and inner speech aphantasia at the *Paris Brain Institute* with **Laurent Cohen** and **Stanislas Dehaene**.\n\n-   [**Nikolaus Kriegeskorte**](https://zuckermaninstitute.columbia.edu/nikolaus-kriegeskorte-phd), one of the creators of the famous Representational Similarity Analysis [RSA, another *supervised* alignment method, see @kriegeskorte2008] and his colleagues could be precious collaborators for alignment analyses and study materials.\n\n-   The Japanese team of [**Masafumi Oizumi**](https://sites.google.com/a/g.ecc.u-tokyo.ac.jp/oizumi-lab/home/member/masafumi_oizumi?pli=1) behind the GWOT toolbox is of course also very knowledgeable on the subject, the method, and its technical implementation.\n\n-   They are collaborating with the Australia-based team of [**Naotsugu Tsuchiya**](https://research.monash.edu/en/persons/nao-tsuchiya), with whom they recently published several very interesting articles on similarity as a concept and method for perception research [e.g. @tsuchiyaEnrichedCategoryModel2022; @kawakita2023; @kawakitaComparingColorSimilarity2023; @zeleznikow-johnstonAreColorExperiences2023].\n\n-   Visiting [Tsuchiya's webpage](https://research.monash.edu/en/persons/nao-tsuchiya), I also found an amazing chain of connections that lead us to his team. Tsuchiya also works on sleep and dreams and has collaborated several times with [**Thomas Andrillon**](https://www.movit.paris/team-pi/thomas-andrillon), who works at the *Paris Brain Institute*, thus close to Dehaene, Cohen, Bartolomeo, and Nalborczyk, and is very technically knowledgeable. Even more interestingly, Andrillon is a co-author of [one of Alexei Dawes' most famous papers on aphantasia](https://www.nature.com/articles/s41598-020-65705-7), probably because they surveyed aphantasics about dreams (looking at the author contributions, he apparently took part in the study concept, data analysis, and critical revisions). Further, Tsuchiya and Andrillon are co-directors of [**Nicolas Decat**](https://www.movit.paris/team-members/decat-nicolas), whom I met at the *Immersion and Synesthesia Conference* where he gave another talk. So Tsuchiya and Andrillon might even have indirectly heard of our work! (Provided that my talk was noticeable enough for Nicolas - or anyone else - to tell them about it...)\n\n<!-- Modern psychology builds on the relativistic framework of philosophy, accepting that humans cannot know reality in an absolute sense. Focusing on relative comparisons, or similarity, is more than a clever philosophical work-around. similarity is a common currency of perception and cognition. In addition to operating at all levels of cognition, similarity---or, more accurately, the second-order isomorphism defined by a set of similarity relations---has been a powerful tool for analyzing and comparing psychological spaces. -->\n\n# Conclusion {.unnumbered}\n\nUsing the unsupervised alignment method that we exposed and tested in this report, Kawakita have shown that relational properties of color representations were universally shared by color-neurotypical individuals, but structurally different from color-atypical individuals. Yet intriguingly, their results also support the hypothesis that color-atypical individuals have a different structure of their color representations, rather than simply failing to experience certain colors. This observation on color-atypical individuals, which emerges primarily from the novel consideration of color representation in a psychological space, foreshadows of the potential of this technique to demystify aphantasia.Such perspectives open up unexpected avenues of research to address the impossibility of comparing subjective experiences using psychophysical science.\n\nI tried to show in this project report that this type of (very simple) paradigm focusing on similarities between participants' subjective representations, combined with a state-of-the-art unsupervised alignment method that I was able to implement using an open-source Python scientific toolbox, can be extremely promising for objectifying the difference (or lack of difference) between people's representational formats. This objectification is intrinsically tied to the idea of a link between similarities and representations, but we have good evidence to support this hypothesis. So, provided we create a good study design, this project would enable us to make robust inferences about the contents of analog representations (visual, spatial, auditory-verbal) of aphantasics and phantasics, independent of any subjective assumptions or relationships.\n",
    "supporting": [
      "index_files\\figure-docx"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}