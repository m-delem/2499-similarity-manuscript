{
  "hash": "fb5cfdfd95232f6d7f668c427aa6935b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Study simulation\"\nauthor: \"Maël Delem\"\n# code reading options\ncode-fold: false\n---\n\n\n::: {.callout-note collapse=\"true\"}\n## Packages and setup\n\nDown below is the code to load necessary packages used for the simulation and analysis, along with some setups for the whole document (*hover over the numbers on the far right for additional explanation of code and mechanics*).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#| output: false\n#| echo: true\n\n# ═══ Packages ═════════════════════════════════════════════════════════════════\n\nif (!require(librarian)) install.packages(librarian)   # <1>\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nLe chargement a nécessité le package : librarian\n```\n\n\n:::\n\n```{.r .cell-code}\n#| output: false\n#| echo: true\n\nlibrary(librarian)                                     # <1>      \n\n# now putting packages on our library's shelves:\nshelf(\n  # ─── data management ─────────────────\n  holodeck,       # simulating multivariate data\n  cluster,        # dissimilarity matrices\n  \n  # ─── modelling ───────────────────────\n  mclust,         # mixture clustering\n  \n  #  data visualization ──────────────\n  # palettes\n  viridis,        # colour-blind friendly palettes\n  # interactive\n  plotly,         # interactive plots\n  ggdendro,       # dendrograms\n  seriation,      # dissimilarity plots\n  webshot2,       # HTML screenshots for Word render\n  webshot,\n  \n  # ─── essential package collections ───\n  doParallel,     # parallel execution\n  easystats,      # data analysis ecosystem\n  reticulate,     # R to Python                       # <2>\n  tidyverse,      # modern R ecosystem\n)\n\n# ─── Global cosmetic theme ───\ntheme_set(theme_modern(base_size = 14))\n\npal_okabe_ito <- c(     # <3>                                                \n  \"#E69F00\", \"#56B4E9\", \"#009E73\",                            \n  \"#F5C710\", \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#6c0009\")      \n\n# We'll need 9 colors at some point\npal_okabe_ito_extended <- c(                                 \n  \"#E69F00\", \"#56B4E9\", \"#009E73\",                           \n  \"#F5C710\", \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#6c0009\", \"#414487FF\")\n\n# We'll need 30 colors at another moment\ncool_30_colors <- c(                                                   \n  \"#3d51b4\", \"#414487FF\", \"#003d73\", \"#440154FF\", \"#6c0009\", \"#b64e4e\",\n  \"#D55E00\", \"#E69F00\", \"#F5C710\", \"#FDE725FF\", \"#f2bb7b\", \"#f1afad\", \"#CC79A7\", \n  \"#e57774\", \"#7AD151FF\", \"#57b571\", \"#318a4a\", \"#009E73\", \"#22A884FF\", \n  \"#2A788EFF\", \"#0072B2\", \"#2da6b5\", \"#56B4E9\", \"#889be0\", \"#6677e0\",   \n  \"#3d51b4\", \"#414487FF\", \"#003d73\", \"#440154FF\", \"#6c0009\", \"#b64e4e\"  \n  ) # <3>                                                                      \n\npath = \"data/\"\n\ndf <- read_rds(paste0(path, \"df.RDS\")) # <4>\ndf_embeds <- read_rds(paste0(path, \"df_embeds.RDS\"))\n# Categorical and visual embeddings\ndf_embeds_categ  <- read_rds(paste0(path, \"df_embeds_categ.RDS\"))\ndf_embeds_visual <- read_rds(paste0(path, \"df_embeds_visual.RDS\"))\n# Subject embeddings per sub-group\ndf_embed_c_sub  <- read_rds(paste0(path, \"df_embed_c_sub.RDS\"))\ndf_embed_cs_sub <- read_rds(paste0(path, \"df_embed_cs_sub.RDS\"))\ndf_embed_v_sub  <- read_rds(paste0(path, \"df_embed_v_sub.RDS\"))\ndf_embed_vs_sub <- read_rds(paste0(path, \"df_embed_vs_sub.RDS\"))\n# Accuracy of the unsupervised alignment (bad = not tidy data)\ndf_accuracy_all_bad <- read_rds(paste0(path, \"df_accuracy_all_bad.RDS\"))\ndf_accuracy_cat_bad <- read_rds(paste0(path, \"df_accuracy_cat_bad.RDS\"))\n# Coordinates of the aligned embeddings from the Python output\ncoordinates_aligned_embeddings <- read_rds(paste0(path, \"coordinates_aligned_embeddings.RDS\")) # <4>\n```\n:::\n\n1.  The package `librairian` eases package management with the \"shelf\" function, which automatically: (1) checks if a package is installed; (2) installs it if need be; (3) loads the package like the \"library()\" function would.\n2.  `reticulate` allows to translate and transfer objects and functions from R to Python and vice-versa, and was thus of primary importance for the successful use of the Python toolbox on our simulated data.\n3.  These are personal custom color palettes meant to extend my favourite palette, the color-atypical friendly Okabe-Ito color palette. The palette originally has only eight colors, but I will need nine, then up to 30 for later graphs, so I extended it with a hand-picked selection of mine.\n4. These are R objects that were the results of a previous run of the simulation.\n:::\n\n## Visual-spatial-verbal model of cognitive profiles\n\nWe are going to simulate 30 participants presenting four different cognitive profiles, that I defined as, respectively, *verbal* aphantasics, *spatial* aphantasics, *spatial* phantasics, and *visual* phantasics. Their imagery abilities are summarised in @tbl-imageries.\n\nTo simulate these four sub-groups, we use the `holodeck` R package to generate multivariate normal distributions of scores on these three dimensions for each sub-group. For instance, verbal aphantasics have normally distributed visual imagery scores centered around a mean of 0 (normalized, so negative scores are possible), 0.4 for spatial imagery, and 0.7 for verbal style; Spatial aphantasics have means of 0 for visual, 0.75 spatial, and 0.3 for verbal; etc. The numbers are arbitrary, but have been chosen by trial-and-error to obtain a model that is both well-defined and not exaggerated.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Generating hypothetical imagery values for 30 subjects\"}\n#| code-summary: \"Generating hypothetical imagery values for 30 subjects\"\n#| label: osv-model\n#| echo: true\n#| eval: false\n\n# The function takes the variance and covariance of the imagery distributions\n# as arguments\ngenerate_osv_model <- function(var, cov){\n  df <- \n    tibble(group = rep(c(\"aph\", \"phant\"), each = 8)) |> \n    group_by(group) |> \n    mutate(\n      spatial_group = c(rep(\"spa_low\", 4), rep(\"spa_high\", 4)),\n      vis_spa_group = paste0(group, \"_\", spatial_group),\n      verbal_group = \"verbal_low\",\n      verbal_group  = case_when(\n        vis_spa_group == \"aph_spa_low\" ~ \"verbal_high\", \n        vis_spa_group == \"phant_spa_low\" ~ \"verbal_mid\",\n        TRUE ~ verbal_group)\n    ) |> \n    group_by(vis_spa_group) |> \n    # ─── visual ───\n    sim_discr(\n      n_vars = 1, \n      var = var, \n      cov = cov, \n      # aph_s, aph_v, phant_s, phant_v\n      group_means = c(0, 0, 0.6, 0.87), \n      name = \"v\") |> \n    # ─── spatial ───\n    sim_discr(\n      n_vars = 1,  \n      var = var, \n      cov = cov, \n      # aph_s, aph_v, phant_s, phant_v\n      group_means = c(0.75, 0.4, 0.7, 0.3), \n      name = \"s\") |>\n    # ─── verbal ───\n    sim_discr(\n      n_vars = 1,  \n      var = var, \n      cov = cov, \n      # aph_s, aph_v, phant_s, phant_v\n      group_means = c(0.3, 0.7, 0.3, 0.5), \n      name = \"i\") |>\n    rename(\n      visual_imagery  = v_1,\n      spatial_imagery = s_1,\n      verbal_profile  = i_1\n      )\n}\n\ndf <- generate_osv_model(0.03, 0)\n```\n:::\n\n\n### Generating \"prototype\" embeddings from a sphere\n\nProposal from [StackExchange](https://stats.stackexchange.com/questions/7977/how-to-generate-uniformly-distributed-points-on-the-surface-of-the-3-d-unit-sphe) to generate points on a sphere:\n\nLet's use a function to generate embeddings. We get 8 nicely distributed clusters. We'll retrieve the centroids of each cluster, which would be the \"perfect\" categories of each species group (say, generated by a computational model on categorical criteria).\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Generating a sphere of 1000 points and 8 equally distributed clusters\"}\n#| code-summary: \"Generating a sphere of 1000 points and 8 equally distributed clusters\"\n#| label: generating-sphere\n#| echo: true\n#| eval: false\n\ngenerate_sphere <- function(n){\n  z     <- 2*runif(n) - 1          # uniform on [-1, 1]\n  theta <- 2*pi*runif(n) - pi      # uniform on [-pi, pi]\n  x     <- sin(theta)*sqrt(1-z^2)  # based on angle\n  y     <- cos(theta)*sqrt(1-z^2) \n  \n  df <- tibble(x = x, y = y, z = z)\n  \n  return(df)\n}\n\n# 1000 random observations with embeddings uniformly distributed on a sphere\ndf_embeds <- generate_sphere(1000)\n\n# Clustering the observations in 8 groups based on their coordinates\nclusters <- Mclust(df_embeds, G = 8)\n\n# adding the classification to the data\ndf_embeds <- df_embeds |> mutate(group = as.factor(clusters$classification))\n\n# getting the centroids of each cluster\ndf_centroids <- \n  df_embeds |> \n  group_by(group) |> \n  summarise(\n    x_centroid = mean(x),\n    y_centroid = mean(y),\n    z_centroid = mean(z)\n  )\n\n# adding them to the data\ndf_embeds_2 <- left_join(df_embeds, df_centroids, by = \"group\")\n```\n:::\n\n\n### Categorical model embeddings\n\nThe selection procedure for the **categorical model** will consist of selecting points that are rather *close to the centroids*. Thus, we will filter the observations of the large sets to keep only points for which the distance to the centroid is inferior to a given value. That is, points for which the Euclidean norm of the vector from the observation to the centroid:\n\n$$d(centroid, observation) = \\sqrt{(x_{c} - x_{o})^{2} + (y_{c} - y_{o})^{2} + (z_{c} - z_{o})^{2}}$$\n\nThis can be done using the function `norm(coordinates, type = \"2\")` in R.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Selecting categorical embeddings based on distances to the centroids\"}\n#| code-summary: \"Selecting categorical embeddings based on distances to the centroids\"\n#| label: categorical-embeddings\n#| echo: true\n#| eval: false\n\n# Function to filter points of the sphere based on the distance to the centroids\ngenerate_embeddings <- function(df, n_embeddings, distance_quantile){\n  df <- \n    df |> \n    # computing the euclidean distance to the centroids for each observation\n    rowwise() |> \n    mutate(\n      distance = norm(\n        c((x_centroid - x), (y_centroid - y), (z_centroid - z)),\n        type = \"2\")\n      ) |> \n    # filtering by distance to the centroid by group\n    group_by(group) |> \n    # selecting the X% closest (specified with \"distance_quantile\")\n    filter(distance < quantile(distance, probs = distance_quantile)) |> \n    # selecting X random observations per cluster in these \n    # (specified with \"n_embeddings\")\n    slice(1:n_embeddings) |> \n    select(group, x, y, z) |>\n    ungroup()\n}\n\ndf_embeds_categ <- generate_embeddings(df_embeds_2, 8, 0.5)\n```\n:::\n\n\n### Visual model embeddings\n\nIn the case of the **visual model**, we would like approximately evenly distributed embeddings, that could also dive *inside* the sphere, i.e. representing species that are visually close although diametrically opposed when it comes to taxonomy. To do this we could try to simulate multivariate normal distributions around the centroids[^2]. This can be done with the `holodeck` package.\n\n[^2]: A simpler alternative would be generating the visual embeddings with the same code as the categorical ones, selecting 8 points per cluster but much more spread out (e.g. selecting 8 among the 90% closest to the centroids, which would create more variability than the categorical one set to 60%). I chose otherwise because this wouldn't have had points reaching *inside* the sphere.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Generating visual embeddings based on multivariate distributions around the categorical embeddings\"}\n#| code-summary: \"Generating visual embeddings based on multivariate distributions around the categorical embeddings\"\n#| label: visual-embeddings\n#| echo: true\n#| eval: false\n\n# defining the variance and covariance of the distributions\nvar2 <- 0.05\ncov2 <- 0\n\n# generating multivariate distributions around the categorical 3D means\ndf_embeds_visual <-\n  tibble(\n    id = as.factor(seq(1,6400)),\n    category = as.factor(rep(seq(1:64), each = 100))\n  )|> \n  group_by(category) |> \n  sim_discr(\n    n_vars = 1, \n    var = var2, \n    cov = cov2, \n    group_means = df_embeds_categ$x, \n    name = \"x\") |> \n  sim_discr(\n    n_vars = 1, \n    var = var2, \n    cov = cov2, \n    group_means = df_embeds_categ$y, \n    name = \"y\") |> \n  sim_discr(\n    n_vars = 1, \n    var = var2, \n    cov = cov2, \n    group_means = df_embeds_categ$z, \n    name = \"z\") |> \n  # keeping only 8 points per distribution\n  slice(1) |> \n  ungroup() |> \n  mutate(group = as.factor(rep(seq(1, 8), each = 8))) |> \n  rename(x = x_1, y = y_1, z = z_1) |> \n  select(group, x, y, z)\n```\n:::\n\n\n### Generating the subject embeddings\n\nWe have four \"reference\" sets of embeddings which represent animals either judged according to their similarity in categorical terms (namely, species), or in visual terms (namely shape or color similarities, assuming that these similarities are more evenly distributed, e.g. the crab looks like a spider, but is also pretty close to a scorpion, etc.).\n\nTo generate the embeddings of each subject in each condition, we will start from these reference embeddings and generate random noise around *each item*, i.e. for all 64 animals. For 100 subjects, we would thus generate 100 noisy points around each animal, each point corresponding to a given subject.\n\nThe visual and verbal groups will be generated with slightly more intra-group variance, so as to try to make the spatial groups as coherent as possible (and avoid blurring everything and making the groups disappear in noise).\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Generating unique embeddings for each subject based on the models\"}\n#| code-summary: \"Generating unique embeddings for each subject based on the models\"\n#| label: subject-embeddings\n#| echo: true\n#| eval: false\n\n# creating dfs with participants\ndf_subjects_7 <- \n  tibble(subject = seq(1, 7, 1)) |> \n  mutate(subject = paste0(\"subject_\", subject))\n\ndf_subjects_8 <- \n  tibble(subject = seq(1, 8, 1)) |> \n  mutate(subject = paste0(\"subject_\", subject))\n\n# splitting df_embeddings\ndf_embed_c  <- df_embeddings |> select(group, species,  x_c:z_c)\ndf_embed_cs <- df_embeddings |> select(group, species, x_cs:z_cs)\ndf_embed_vs <- df_embeddings |> select(group, species, x_vs:z_vs)\ndf_embed_v  <- df_embeddings |> select(group, species,  x_v:z_v)\n\n# function to create embeddings per subject with normal random noise\ngenerate_subject_embeddings <- function(df, df_subjects, var){\n  df <-\n    df |> \n    mutate(subject = list(df_subjects)) |> \n    unnest(subject) |> \n    group_by(species) |> \n    # simulating x coordinates\n    sim_discr(\n      n_vars = 1, \n      var = var, \n      cov = 0, \n      group_means = pull(df, 3), \n      name = \"x\") |> \n    # simulating y coordinates\n    sim_discr(\n      n_vars = 1, \n      var = var, \n      cov = 0, \n      group_means = pull(df, 4), \n      name = \"y\") |> \n    # simulating z coordinates\n    sim_discr(\n      n_vars = 1, \n      var = var, \n      cov = 0, \n      group_means = pull(df, 5), \n      name = \"z\") |>\n  select(group, species, subject, 7:9) |> \n  rename(x = 4, y = 5, z = 6) |> \n  ungroup()\n  \n  return(df)\n}\n\nvar_s1 = 0.001\nvar_s2 = 0.0005\n\ndf_embed_c_sub  <- generate_subject_embeddings(df_embed_c,  df_subjects_4, var_s1)\ndf_embed_cs_sub <- generate_subject_embeddings(df_embed_cs, df_subjects_4, var_s2)\ndf_embed_vs_sub <- generate_subject_embeddings(df_embed_vs, df_subjects_4, var_s2)\ndf_embed_v_sub  <- generate_subject_embeddings(df_embed_v,  df_subjects_4, var_s1)\n```\n:::",
    "supporting": [
      "simulation-code_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}