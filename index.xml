<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving
and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">

<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">

<front>


<article-meta>


<title-group>
<article-title>Unravelling mental representations in aphantasia through
unsupervised alignment</article-title>
<subtitle>Project design and study simulation</subtitle>
</title-group>

<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Delem</surname>
<given-names>Maël</given-names>
</name>
<string-name>Maël Delem</string-name>

</contrib>
</contrib-group>









<history></history>


<abstract>
<p>Research on aphantasia is confronted with a long-standing conundrum
of all research on consciousness and representations, namely the
theoretical inaccessibility of subjective representations. Drawing on
concepts from similarity and representation research, I endorse the view
that the study of an individual’s mental representations is made
possible by exploiting second-order isomorphism. The concept of
second-order isomorphism means that correspondence should not be sought
in the first-order relation between (a) an external object and (b) the
corresponding internal representation, but in the second-order relation
between (a) the perceived similarities between various external objects
and (b) the similarities between their corresponding internal
representations. Building on this idea, this study project report was
divided into four parts. <bold>First</bold>, I outline the central ideas
underlying similarity research and its applicability to aphantasia
research. <bold>Second</bold>, I present a complete paradigm with an
experimental design and a data analysis plan. The design will be based
on multi-arrangement and inverse multidimensional scaling, a protocol
that can be implemented online to conduct such large-scale research with
high efficiency. The analysis plan will present a state-of-the-art
method for similarity analysis, unsupervised alignment with
Gromov-Wasserstein optimal transport (GWOT). <bold>Third</bold>, I
report a data simulation I’ve done of a potential outcome of this study,
and the successful analysis of this synthetic data using GWOT alignment.
<bold>Fourth</bold>, I analyse the feasability of such a project given
the material constraints of my thesis. I conclude with the expected
utility and benefits of this project.</p>
</abstract>




</article-meta>

</front>

<body>
<p></p>
<sec id="work-in-progress-everywhere-so-look-away-shoo">
  <title>Work-In-Progress everywhere, <italic>so look away!
  Shoo!</italic></title>
  <p>I just wanted to try
  <ext-link ext-link-type="uri" xlink:href="https://quarto.org/docs/manuscripts/">Quarto’s
  new manuscript format</ext-link> with this personal project.
  Conclusion: it’s <bold><italic>incredibly cool</italic></bold>.</p>
  <boxed-text>
  <p><bold>Project inception</bold></p>
  <p>This project stems from several elements:</p>
  <list list-type="order">
    <list-item>
      <p>The long standing knowledge of the fact that internal
      representations seem impossible to reach due to their subjective
      nature.</p>
    </list-item>
    <list-item>
      <p>The discovery of the article of Shepard &amp; Chipman
      (<xref alt="1970" rid="ref-shepardSecondorderIsomorphismInternal1970" ref-type="bibr">1970</xref>)
      that expose the idea of “second-order isomorphism”.</p>
    </list-item>
    <list-item>
      <p>The discovery of state-of-the-art and accessible unsupervised
      analytic methods to study this principle in an astonishing way.
      The last two discoveries (and many more) are the fruit of amazing
      discussions and recommendations from Ladislas when he came to the
      lab on Jan. 26. These motivated me to try to implement GWOT in R
      on data that I wanted to create myself to emulate a study we could
      do.</p>
    </list-item>
  </list>
  <p><bold>I promise that I did this mostly on my spare time, we have
  too many other things to do elsewhere.</bold></p>
  <p><italic>Note: This website may seem very fancy. I wanted to take
  advantage of this personal project to try
  <ext-link ext-link-type="uri" xlink:href="https://quarto.org/docs/manuscripts/">Quarto’s
  new manuscript format</ext-link> for scientific editing. Conclusion:
  it’s <bold>awesome</bold>. It is very likely that I’ll end up writing
  my thesis using
  <ext-link ext-link-type="uri" xlink:href="https://quarto.org/docs/books/">Quarto’s
  book format</ext-link> (through RStudio). This will allow me to render
  the raw text and computations as beautifully formatted PDF and Word
  documents with low effort, and eventually port it as a self-contained
  website when I’m authorized to share it openly… All with a single
  command, just like I did for this website. <bold>This also means that
  you can read the present report on a PDF or Word if you wish to do so,
  the links are in the header</bold>. You’ll freeze the nice interactive
  figures though. As a bonus for the curious (or the reviewer), the
  “MECA Bundle” contains absolutely everything tied to this manuscript,
  well sorted, from the code scripts and configuration files to the
  final documents in all formats. <bold>Awesome, I tell
  you</bold>.</italic></p>
  </boxed-text>
  <p></p>
</sec>
<sec id="theoretical-context">
  <title>1. Theoretical context</title>
  <p>When we try to compare our thoughts and representations with those
  of others, we quickly realize that the task will be really difficult,
  if not impossible, as we are of course incapable of “living in someone
  else’s head”. If we both try to imagine a dog, I can examine what goes
  on in my head, so can you, but apart from trying to describe our
  experiences verbally, we are up against a wall.</p>
  <p>Now, what if I asked you to tell me how similar you think a dog and
  a panther look like? Let’s say, in the context of the animal kingdom
  as a whole. Visualize them well. Well, I could tell you that,
  <italic>in my opinion, a dog and a panther look no more alike than a
  dog and a whale</italic>. You might tell me:
  <named-content content-type="column-margin"><italic>For this thought
  experiment, let’s imagine two things: (1) that someone could honestly
  say that, and (2) that people would be rating the animals purely on
  the basis of their mental images, and not on categorical features
  (number of legs, fur, etc.), which is unfortunately almost</italic>
  <bold><italic>never</italic></bold> <italic>the case in
  reality.</italic></named-content></p>
  <disp-quote>
    <p>“<bold><italic>What on earth do you imagine a dog and a panther
    look like? Do you also think that a dog looks nothing like a cat?
    What goes on in your head?</italic></bold>”</p>
  </disp-quote>
  <p>…And many people probably agree with you. They mentally “see” and
  compare certain items the same way you do… And just like that,
  <italic>we are back on track</italic>. We managed to better “compare
  our thoughts”! And we even felt we could dive a bit into the “weird”
  representations of someone else.</p>
  <p>The study of individual differences in the format of
  representations and the attempt at understanding those of others
  obviously has a very rich history. It has interested many fields, in
  philosophy, linguistics, sociology, biology, psychology, or
  neuroscience, to name but a few. A myriad of ideas, concepts, models,
  methods, and paradigms have tried to deepen our understanding of
  representations and find the “key” to objectifying them. The principle
  I tried to illustrate with the thought experiment above is at the
  heart of one of these methods trying to unravel representations that
  was born in psychophysics<xref ref-type="fn" rid="fn1">1</xref>: the
  study of <bold><italic>similarity.</italic></bold></p>
  <sec id="from-similarity-to-second-order-isomorphism">
    <title>1.1 From similarity to second-order isomorphism</title>
  </sec>
  <sec id="psychological-spaces-and-aphantasia">
    <title>1.2 Psychological spaces and aphantasia</title>
    <p>While attempting to demonstrate the uselessness of the concept of
    similarity as a philosophical and scientific notion, Goodman
    (<xref alt="1972" rid="ref-goodmanSevenStricturesSimilarity1972" ref-type="bibr">1972</xref>)
    has inadvertently expressed an aspect of similarity judgements of
    primary importance to us aphantasia researchers:</p>
    <disp-quote>
      <p>Comparative judgments of similarity often require not merely
      selection of relevant properties but a weighting of their relative
      importance, and variation in both relevance and importance can be
      rapid and enormous. Consider baggage at an airport checking
      station. The spectator may notice shape, size, color, material,
      and even make of luggage; the pilot is more concerned with weight,
      and the passenger with destination and ownership. Which pieces are
      more alike than others depends not only upon what properties they
      share, but upon who makes the comparison, and when. . . .
      Circumstances alter similarities.
      <named-content content-type="column-margin"><italic>Goodman’s
      claim was dismissed since then by propositions of robust
      mathematical models of similarity, e.g. Gardenfors
      (<xref alt="2004a" rid="ref-gardenforsConceptualSpacesFramework2004" ref-type="bibr">2004a</xref>),
      Decock &amp; Douven
      (<xref alt="2011" rid="ref-decockSimilarityGoodman2011" ref-type="bibr">2011</xref>).</italic></named-content></p>
    </disp-quote>
    <p>This can be easily reversed as an argument in favor of the
    <bold>potential of similarity analyses to highlight the
    inter-individual differences in sensory mental
    representations</bold>. For example, should we ask individuals to
    judge the similarities in shape or color between various objects,
    the <italic>differences between the similarity structures</italic>
    of individuals will be precisely the most important phenomenon for
    us, far less than the constancy between these structures. If we can
    account for the context dependence, as we will propose here with
    explicit instructions, clever task design, and hypothesis-neutral
    analysis, we could overcome the limitations of the inherently
    subjective nature of similarity judgements.</p>
    <p>This idea of a difference in similarity judgements in aphantasia
    seems to transpire in the results of Bainbridge et al.
    (<xref alt="2021" rid="ref-bainbridgeQuantifyingAphantasiaDrawing2021" ref-type="bibr">2021</xref>)
    on their drawing study. They have shown that aphantasics had more
    schematic representations during recall, accurate in their spatial
    positioning, but with less sensory details. This difference can be
    seen from two perspectives: (1) a memory deficit for sensory
    properties; (2) a different representational structure of the items
    in their psychological spaces. In the latter case, aphantasics would
    have greater/faster abstraction of their representation of a
    perceived scene, reducing the amount of encoded sensory details
    unconsciously considered to be relevant. Both (1) and (2) can
    theoretically explain the same behavioural response, i.e. less
    sensory elements and correct spatial recall accuracy in aphantasic
    drawings, but <bold>the two have drastically different consequences
    on how we define, characterize, and judge aphantasia.</bold></p>
    <p>The dominant hypothesis seems to be that aphantasics simply have
    an episodic or general memory deficit. Conversely, I hypothesize
    that aphantasics have different representational structures than
    phantasics in certain dimensions of their psychological spaces
    (notably sensory, but potentially abstract too). More generally, I
    hypothesize that the concept of visual imagery evaluates in reality
    the continuous spectrum of representational structures in
    <italic>sensory</italic> dimensions of psychological spaces.
    Mirroring visual imagery, spatial imagery could also be a rough
    psychometric evaluation of the continuous spectrum of structural
    differences in <italic>conceptual/abstract</italic> dimensions of
    psychological spaces. In this view, the psychological space of
    aphantasics would constrain internal representations to particularly
    abstract forms from a very early stage, thus selectively limiting
    the item properties thereafter encoded in long-term memory. In other
    terms, <bold>I hypothesize that aphantasia would not be
    characterized by an episodic memory deficit, but by an episodic
    memory <italic>selectivity</italic> caused by the specific
    characteristics of their representational structures and
    psychological spaces.</bold> This selectivity would have, as we
    already hypothesized several times, benefits and drawbacks.</p>
    <p>Gardenfors
    (<xref alt="2004a" rid="ref-gardenforsConceptualSpacesFramework2004" ref-type="bibr">2004a</xref>)
    proposed that differences in psychological (in his terms,
    conceptual) spaces could arise from various sources, whether innate,
    due to learning, or broader cultural or social differences. All
    these hypotheses could be coherent to explain the sources of
    aphantasia. Nevertheless, the study of these sources should be the
    subject of very large-scale or longitudinal studies, which are out
    of the scope of this project.</p>
    <p>Here, we shall rather attempt to <bold>develop a method to
    characterize the differences in aphantasics’ representational
    structures and psychological spaces.</bold></p>
  </sec>
</sec>
<sec id="methods">
  <title>2. Methods</title>
  <p>Roads &amp; Love
  (<xref alt="2024" rid="ref-roads2024" ref-type="bibr">2024</xref>), in
  a recent review on the state and perspectives of similarity research,
  highlighted two challenges that studies in this field had to face: (1)
  The high cost of collecting behavioral data on a large number of
  stimuli; (2) The lack of software packages being a high barrier to
  entry, making the task of coding models difficult for the
  uninitiated.</p>
  <p>To solve these problems, we present here two solutions,
  respectively for (1) experimental design and (2) data analysis:</p>
  <list list-type="order">
    <list-item>
      <p>A recent method to efficiently acquire similarity judgements,
      the “multiple arrangement of items” and “inverse multidimensional
      scaling” developed by Kriegeskorte &amp; Mur
      (<xref alt="2012" rid="ref-kriegeskorteInverseMDSInferring2012" ref-type="bibr">2012</xref>).</p>
    </list-item>
    <list-item>
      <p>An accessible and robust Python toolbox provided by Sasaki et
      al.
      (<xref alt="2023" rid="ref-sasakiToolboxGromovWassersteinOptimal2023" ref-type="bibr">2023</xref>)
      to conduct unsupervised alignment analysis using
      Gromov-Wasserstein optimal transport.</p>
    </list-item>
  </list>
  <sec id="experimental-design">
    <title>2.1 Experimental design</title>
    <sec id="sec-ma">
      <title>Multi-arrangement and inverse multidimensional
      scaling</title>
      <p>Assuming a geometric model of representational similarities,
      Kriegeskorte &amp; Mur
      (<xref alt="2012" rid="ref-kriegeskorteInverseMDSInferring2012" ref-type="bibr">2012</xref>)
      developed a multi-arrangement (MA) method to efficiently acquire
      (dis)similarity judgments for large sets of objects. The subject
      has to perform multiple arrangements of item subsets adaptively
      designed for optimal measurement efficiency and for estimating the
      representational dissimilarity matrix (RDM) by combining the
      evidence from the subset arrangements.</p>
      <p>The procedure is illustrated in
      <xref alt="Figure 2.1" rid="fig-multi-arrangement">Figure 2.1</xref>.</p>
      <fig id="fig-multi-arrangement">
        <caption><p>Figure 2.1: <bold>Acquiring similarity judgements
        with the multi-arrangement method. (A)</bold> Subjects are asked
        to arrange items according to their similarity, using mouse
        drag-and-drop on a computer. The similarity measure is taken as
        the distances between the items: similar items are closer, while
        dissimilar items are further apart. The upper part of the figure
        shows screenshots at different moments of the acquisition for
        one subject. Columns are trials and rows show the object
        arrangements over time, running from the start (top row) to the
        end (last row). The first trial contains all items; subsequent
        trials contain subsets of items that are adaptively selected to
        optimally estimate judged similarity for each subject.
        <bold>(B)</bold> Once acquisition of the final judgements is
        completed, inter-item distances in the final trial arrangements
        are combined over trials by rescaling and averaging to yield a
        single dissimilarity estimate for each object pair. The process
        is illustrated in this figure for two example item pairs: a
        boy’s face and a hand (red), and carrots and a stop sign (blue).
        Their single-trial dissimilarity estimates (arrows) are combined
        into a single dissimilarity estimate, which is placed at the
        corresponding entry of the RDM (lower panel). Mirror-symmetric
        entries are indicated by lighter colors. Figure from Mur et al.
        (<xref alt="2013" rid="ref-murHumanObjectSimilarityJudgments2013" ref-type="bibr">2013</xref>).</p></caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="images/multi-arrangement-method-mur-2013.png" />
      </fig>
      <p>A key strength of this method that sets it as particularly
      effective is the “adaptive” part. The goal of the process is to
      acquire similarity judgements as precisely as possible while
      minimizing the total amount of trials. To do so, starting from the
      second trial, selected subsets of the items to be compared are
      presented to the subject: these items are the ones that were very
      close on-screen in previous trials and thus had their distance
      evaluated with lower accuracy by the subject. As the subject has
      to fill the entire “arena” with the items, these subsequent trials
      will necessarily increase the level of precision in the similarity
      judgement between pairs of items. The second key benefit of this
      method is the time and effort gain compared to others. For
      example, to compare every pair of items among 64 different items
      would require <inline-formula><alternatives>
      <tex-math><![CDATA[\frac{64 \times (64-1)}{2} = 2016]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>64</mml:mn><mml:mo>×</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mn>64</mml:mn><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>=</mml:mo><mml:mn>2016</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
      comparisons (i.e. trials). This would be extremely time-consuming,
      while also losing the <italic>context-independence</italic>
      afforded by the MA method due to the presence of other items
      around every time the subject mentally performs a pairwise
      comparison.</p>
      <p>Historically, when referring to the projection of the
      representations of stimuli (e.g., coordinates in geometric space)
      from a high-dimensional space into a lower-dimensional space,
      inference algorithms were commonly called multidimensional scaling
      (<xref alt="Roads &amp; Love, 2024" rid="ref-roads2024" ref-type="bibr">Roads
      &amp; Love, 2024</xref>). By analogy, the process of combining
      several lower-dimensional (2D) similarity judgements on-screen to
      form one higher dimensional similarity representation (in the RDM)
      can be conceptually seen as “inverse” multidimensional scaling,
      hence the name given to the method by Kriegeskorte &amp; Mur
      (<xref alt="2012" rid="ref-kriegeskorteInverseMDSInferring2012" ref-type="bibr">2012</xref>).</p>
    </sec>
    <sec id="sec-principle">
      <title>Principle</title>
      <p>The idea is simple: for a given set of items that have distinct
      and very pictorial visual properties, we would ask a wide range of
      aphantasics, phantasics or hyperphantasics to imagine, mentally
      compare and make similarity judgements between the items. To
      compare these representations with actual perceptual
      representations, the subjects would also perform the same task
      afterwards, this time with actual pictures to compare. Subjects
      would also fill our usual psychometric imagery questionnaires.</p>
      <fig id="fig-spam-mur">
        <caption><p>Figure 2.2: Arena layout of the MA protocol used by
        Mur et al.
        (<xref alt="2013" rid="ref-murHumanObjectSimilarityJudgments2013" ref-type="bibr">2013</xref>)
        to acquire perceptual similarity judgements on natural images.
        <italic>Click to expand.</italic></p></caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="images/mur-spam-2.png" />
      </fig>
      <p>To “compare imagined items”, we could use a “word” version of
      the MA paradigm. An example from Majewska et al.
      (<xref alt="2020" rid="ref-majewskaSpatialMultiarrangementClustering2020" ref-type="bibr">2020</xref>)
      - <italic>who used the method to build large-scale semantic
      similarity resources for Natural Language Processing
      systems</italic> - is represented in
      <xref alt="Figure 2.3" rid="fig-majewska">Figure 2.3</xref>.</p>
      <fig id="fig-majewska">
        <caption><p>Figure 2.3: Arena layout of the MA protocol used by
        Majewska et al.
        (<xref alt="2020" rid="ref-majewskaSpatialMultiarrangementClustering2020" ref-type="bibr">2020</xref>)
        to acquire similarity judgements on word pairs. <italic>Click to
        expand.</italic></p></caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="images/majewska-spam-2.png" />
      </fig>
      <p>We could have the stimuli rated by another set of participants
      on several features.</p>
      <disp-quote>
        <p>« <italic>We deliberately did not specify which object
        properties to focus on, to avoid biasing participants’
        spontaneous mental representation of the similarities between
        objects. Our aim was to obtain similarity judgments that reflect
        the natural representation of objects without forcing
        participants to rely on one given dimension. However,
        participants were asked after having performed the task, what
        dimension(s) they used in judging object similarity.</italic> »
        (<xref alt="Jozwik et al., 2016" rid="ref-jozwik2016" ref-type="bibr">Jozwik
        et al., 2016</xref>)</p>
      </disp-quote>
      <disp-quote>
        <p>« <bold><italic>All but one of the 16 participants reported
        arranging the images according to a categorical
        structure.</italic></bold> »
        (<xref alt="Jozwik et al., 2017" rid="ref-jozwik2017" ref-type="bibr">Jozwik
        et al., 2017</xref>)</p>
      </disp-quote>
      <p>This result of Jozwik et al.
      (<xref alt="2017" rid="ref-jozwik2017" ref-type="bibr">2017</xref>)
      suggests that we should give an explicit instruction about the
      features to focus on, otherwise everyone might bypass visual
      features and mental images in favour of concepts and categories,
      regardless of their mental imagery profile.</p>
      <p>In contrast, if we ask to focus specifically on the visual
      features, then ask subjects about the strategy they used to
      evaluate the similarities, then on the subjectively felt mental
      format of these strategies, we might grasp better insight on the
      sensory representations of subjects.</p>
      <p>We could even go for several comparisons - even though this
      would increase quadratically the number of trials - e.g. :</p>
      <list list-type="bullet">
        <list-item>
          <p>Evaluate to what extent the <bold>shape</bold> <italic>of
          these animals are</italic>
          <bold><italic>similar</italic></bold> <bold>at rest, ignoring
          size differences.</bold></p>
        </list-item>
        <list-item>
          <p>Evaluate to what extent these animals <bold>sound like each
          other.</bold></p>
        </list-item>
        <list-item>
          <p>Etc.</p>
        </list-item>
      </list>
      <disp-quote>
        <p><italic>Note to be added: if you do not know the animal, just
        guess its placement, as this situation is quite unlikely to
        happen (animals chosen are fairly common
        knowledge).</italic></p>
      </disp-quote>
      <p>Kawakita et al.
      (<xref alt="2023" rid="ref-kawakita2023" ref-type="bibr">2023</xref>):
      To assess whether the color dissimilarity structures from
      different participants can be aligned in an unsupervised manner,
      we divided color pair similarity data from a large pool of 426
      participants into five participant groups (85 or 86 participants
      per group) to obtain five independent and complete sets of
      pairwise dissimilarity ratings for 93 color stimuli (Fig. 3a).
      Each participant provided a pairwise dissimilarity judgment for a
      randomly allocated subset of the 4371 possible color pairs. We
      computed the mean of all judgments for each color pair in each
      group, generating five full dissimilarity matrices referred to as
      Group 1 to Group 5.</p>
    </sec>
    <sec id="stimuli">
      <title>Stimuli</title>
      <p>We would have a list of animal items, that would have several
      characteristics:</p>
      <fig id="fig-marr">
        <caption><p>Figure 2.4: Representing the characteristics of
        shapes with cylinders. Figure from Marr et al.
        (<xref alt="1997" rid="ref-marr1997" ref-type="bibr">1997</xref>).
        <italic>Click to expand.</italic></p></caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="images/shapes-marr.png" />
      </fig>
      <list list-type="bullet">
        <list-item>
          <p>A name</p>
        </list-item>
        <list-item>
          <p>A category</p>
        </list-item>
        <list-item>
          <p>A shape</p>
        </list-item>
      </list>
      <p>We need orthogonal data:</p>
      <list list-type="bullet">
        <list-item>
          <p>Each class of animal should include each shape
          (roughly)</p>
        </list-item>
        <list-item>
          <p>Each shape should have an animal</p>
        </list-item>
      </list>
      <p>This would imply that category cannot be derived from shape,
      and vice-versa. Thus, a <bold>sorting by shape would reveal to be
      innately visual</bold> (or maybe spatial, if shape concerns this
      type of imagery), and a <bold>sorting by category would reveal an
      abstraction</bold> from these shapes. We expect that the two will
      be mixed to some degree in every subject, but that low-imagery
      would rather tend towards category sorting, while high-imagery
      would tend towards shape sorting.</p>
      <p>Shapes could be very tricky stimuli to discuss. Gardenfors
      (<xref alt="2004b" rid="ref-gardenfors2004" ref-type="bibr">2004b</xref>)
      noted that we only have a very sketchy understanding of how we
      perceive and conceptualize things according to their shapes. The
      works of Marr et al.
      (<xref alt="1997" rid="ref-marr1997" ref-type="bibr">1997</xref>)
      highlight this difficulty when analysing the complexity of the
      hierarchical judgements of shapes and volumes, as shown in
      <xref alt="Figure 2.4" rid="fig-marr">Figure 2.4</xref>.</p>
    </sec>
  </sec>
  <sec id="data-analysis-plan">
    <title>2.2 Data analysis plan</title>
    <sec id="unsupervised-alignment-rationale">
      <title>Unsupervised alignment rationale</title>
      <p>Visual images can be represented as points in a
      multidimensional psychological space. Embedding algorithms can be
      used to infer latent representations from human similarity
      judgments. While there are an infinite number of potential visual
      features, an embedding algorithm can be used to identify the
      subset of salient features that accurately model human-perceived
      similarity. (<italic>From Roads’ CV</italic>)</p>
      <p>Using an optimization algorithm, the free parameters of a
      psychological space are found by maximizing goodness of fit (i.e.,
      the loss function) to the observed data. Historically, when
      referring specifically to the free parameters that correspond to
      the representation of stimuli (e.g., coordinates in geometric
      space), inference algorithms were commonly called multidimensional
      scaling (MDS), or simply scaling, algorithms.</p>
      <p>In the machine learning literature, analogous inference
      algorithms are often called embedding algorithms. The term
      “embedding” denotes a higher-dimensional representation that is
      embedded in a lower-dimensional space. For that reason, the
      inferred mental representations of a psychological space could
      also be called a psychological embedding.</p>
      <p>Numerous techniques exist, and each has limitations. Popular
      techniques for comparing representations include RSA Kriegeskorte
      et al.
      (<xref alt="2008" rid="ref-kriegeskorte2008" ref-type="bibr">2008</xref>)
      and canonical correlation analysis (CCA) (Hotelling 1936).
      Briefly, RSA is a method for comparing two representations that
      assesses the correlation between the implied pairwise similarity
      matrices. CCA is a method that compares two representations by
      finding a pair of latent variables (one for each domain) that are
      maximally correlated.</p>
      <p>One might be tempted to compare two dissimilarity matrices
      assuming stimulus-level “external” correspondence: my “red”
      corresponds to your “red”(Fig. 1d). This type of supervised
      comparison between dissimilarity matrices, known as
      Representational Similarity Analysis (RSA), has been widely used
      in neuroscience to compare various similarity matrices obtained
      from behavioural and neural data. However, there is no guarantee
      that the same stimulus will necessarily evoke the same subjective
      experience across different participants. Accordingly, when
      considering which stimuli evoke which qualia for different
      individuals, we need to consider all possibilities of
      correspondence: my “red” might correspond to your “red”, “green”,
      “purple”, or might lie somewhere between your “orange” and
      “pink”(Fig. 1e). Thus, we compare qualia structures in a purely
      unsupervised manner, without assuming any correspondence between
      individual qualia across participants.</p>
    </sec>
    <sec id="gromov-wasserstein-optimal-transport">
      <title>Gromov-Wasserstein optimal transport</title>
      <p>To account for all possible correspondences, we use an
      unsupervised alignment method for quantifying the degree of
      similarity between qualia structures. As shown in Fig. 2a, in
      unsupervised alignment, we do not attach any external (stimuli)
      labels to the qualia embeddings. Instead, we try to find the best
      matching between qualia structures based only on their internal
      relationships (see Methods). After finding the optimal alignment,
      we can use external labels, such as the identity of a color
      stimulus (Fig. 2b), to evaluate how the embeddings of different
      individuals relate to each other. This allows us to determine
      which color embeddings correspond to the same color embeddings
      across individuals or which do not. Checking the assumption that
      these external labels are consistent across individuals allows us
      to assess the plausibility of determining accurate
      inter-individual correspondences between qualia structures of
      different participants.</p>
      <p>To this end, we used the Gromov-Wasserstein optimal transport
      (GWOT) method, which has been applied with great success in
      various fields. GWOT aims to find the optimal mapping between two
      point clouds in different domains based on the distance between
      points within each domain. Importantly, the distances (or
      correspondences) between points “across” different domains are not
      given while those “within” the same domain are given. GWOT aligns
      the point clouds according to the principle that a point in one
      domain should correspond to another point in the other domain that
      has a similar relationship to other points. The principle of the
      method is illustrated in
      <xref alt="Figure 2.5" rid="fig-gwot-kawa">Figure 2.5</xref></p>
      <fig id="fig-gwot-kawa">
        <caption><p>Figure 2.5: Gromov-Wassertein optimal transport
        principle. Figure from Kawakita et al.
        (<xref alt="2023" rid="ref-kawakita2023" ref-type="bibr">2023</xref>).
        <italic>Click to expand.</italic></p></caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="images/kawa-gwot-2.PNG" />
      </fig>
      <p>We first computed the GWD for all pairs of the dissimilarity
      matrices of the 5 groups (Group 1-5) using the optimized
      <inline-formula><alternatives>
      <tex-math><![CDATA[\epsilon]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ϵ</mml:mi></mml:math></alternatives></inline-formula>.
      In Fig. 3b, we show the optimized mapping
      <inline-formula><alternatives>
      <tex-math><![CDATA[\Gamma*]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>Γ</mml:mi><mml:mo>*</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
      between Group 1 and Groups 2-5 (see Supplementary Figure S1 for
      the other pairs). As shown in Fig. 3b, most of the diagonal
      elements in <inline-formula><alternatives>
      <tex-math><![CDATA[\Gamma*]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>Γ</mml:mi><mml:mo>*</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
      show high values, indicating that most colors in one group
      correspond to the same colors in the other groups with high
      probability. We next performed unsupervised alignment of the
      vector embeddings of qualia structures. Although
      <inline-formula><alternatives>
      <tex-math><![CDATA[\Gamma*]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>Γ</mml:mi><mml:mo>*</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
      provides the rough correspondence between the embeddings of qualia
      structures, we should find a more precise mathematical mapping
      between qualia structures in terms of their vector embeddings to
      more accurately assess the similarity between the qualia
      structures. Here, we consider aligning the embeddings of all the
      groups in a common space.</p>
      <p>By applying MDS, we obtained the 3-dimensional embeddings of
      Group 1 and Groups 2-5, referred to as X and Yi, where i = 2, …, 5
      (Fig. 3c). We then aligned Yi to X with the orthogonal rotation
      matrix Qi, which was obtained by solving a Procrustes-type problem
      using the optimized transportation plan
      <inline-formula><alternatives>
      <tex-math><![CDATA[\Gamma*]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>Γ</mml:mi><mml:mo>*</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
      obtained through GWOT (see Methods). Fig. 3d shows the aligned
      embed- dings of Group 2-5 (QiYi) and the embedding of Group 1 (X)
      plotted in the embedded space of X. Each color represents the
      label of a corresponding external color stimulus. Note that even
      though the color labels are shown in Fig. 3d, this is only for the
      visualization purpose and the whole alignment procedure is
      performed in a purely unsupervised manner without relying on the
      color labels. As can be seen in Fig. 3d, the embeddings of similar
      colors from the five groups are located close to each other,
      indicating that similar colors are ‘correctly’ aligned by the
      unsupervised alignment method.</p>
      <p>To evaluate the performance of the unsupervised alignment, we
      computed the k-nearest color matching rate in the aligned space.
      If the same colors from two groups are within the k-nearest colors
      in the aligned space, we consider that the colors are correctly
      matched. We evaluated the matching rates between all the pairs of
      Groups 1-5. The averaged matching rates are 51% when k = 1, 83%
      when k = 3, and 92% when k = 5, respectively. This demonstrates
      the effectiveness of the GW alignment for correctly aligning the
      qualia structures of different participants in an unsupervised
      manner.</p>
      <p>However, as can be seen in Fig. 4b, the optimized mapping
      <inline-formula><alternatives>
      <tex-math><![CDATA[\Gamma*]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>Γ</mml:mi><mml:mo>*</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
      is not lined up diagonally unlike the optimized map- pings between
      color-neurotypical participants groups shown in Fig. 3b (see
      Supplementary Figure S1 for the other pairs). Accordingly, top k
      matching rate between Group 1-5 and Group 6 is 3.0% when k = 1
      (Fig. 4c), which is only slightly above chance
      (<inline-formula><alternatives>
      <tex-math><![CDATA[\approx]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mo>≈</mml:mo></mml:math></alternatives></inline-formula>
      1%). The matching rate did not improve even when we relaxed the
      criterion (6.9% and 11% for k = 3 and k = 5, respectively).
      Moreover, all of the GWD values between Group 1-5 and Group 6 are
      larger than any of the GWD values between color-neurotypical
      participant groups (Fig. 4d).</p>
      <p>These results indicate that the difference between the qualia
      structures of neuro-typical and atypical participants is
      significantly larger than the difference between the qualia
      structures of neuro-typical participants.</p>
    </sec>
  </sec>
  <sec id="paradigm-summary">
    <title>2.3 Paradigm summary</title>
    <p>The experimental design and data analysis plans are succinctly
    summarised in
    <xref alt="Figure 2.6" rid="fig-expe-conditions">Figure 2.6</xref>.</p>
    <p></p>
    <fig id="fig-expe-conditions">
      <caption><p>Figure 2.6: Summary schematics of the proposed
      experimental protocol and data analysis plan. <italic>Click on the
      sub-figures to expand them.</italic>
      <xref alt="Figure 2.6 (a)" rid="fig-expe-subject">Figure 2.6
      (a)</xref> represents the two conditions to be completed by each
      subject. These two conditions will allow to compute comparisons
      (alignments) within a subject’s own perceptual and imaginal
      representational structures, but also between subjects (or groups)
      for each modality (see the next figure’s description).
      <bold>(A)</bold> The subject performs two simililarity judgement
      tasks using the MA paradigm presented earlier. <bold>(B)</bold>
      The low-dimensional similarity judgements are converted to a
      high-dimensional Representational Dissimilarity Matrix (RDM)
      through inverse-MDS as a follow-up to extract the results of the
      MA. <bold>(C)</bold> The RDMs are then reduced in dimensionality
      once again to extract relevant dimensions reflecting inferred
      features of the items through MDS, yielding embeddings.
      Three-dimensional projections of these embeddings have been chosen
      here for visualization purposes. <bold>(D)</bold> These embeddings
      are compared through unsupervised alignment using GWOT, which
      results in an estimate of the degree of alignment of the two
      representational structures and in coordinates of aligned
      embeddings. These coordinates allow us to examine the 3D
      visualization shown here and judge by ourselves the “look” of the
      alignment. Here the perception representation aligns with the
      imagination one, from which we could infer that imagined
      representations are made of sensory (rather than abstract
      properties). We expect inter-individual variability in these
      perception-imagination alignments, as shown in the next figure.
      <xref alt="Figure 2.6 (b)" rid="fig-expe-group">Figure 2.6
      (b)</xref> represents the comparison between the representational
      structure of different cognitive profiles. In practice, all pairs
      of subjects will be compared to assess their representational
      structure alignments, independently of arbitrary groups. This is
      computationally heavy, but analytically very powerful. This figure
      also tacitly shows an idea supporting the use of unsupervised
      alignment: it is possible that RDMs seem to be very correlated and
      similar, as shown in step <bold>(B)</bold>, but do not align when
      compared without supervision. This contrasts with several
      supervised alignment methods (such as RSA, see
      <xref alt="Kriegeskorte et al., 2008" rid="ref-kriegeskorte2008" ref-type="bibr">Kriegeskorte
      et al., 2008</xref>) which usually use the RDM as-is. This
      difference is due to the involvement of labels for items that are
      already known by the researcher to correlate the RDMs, whereas
      unsupervised algorithms such as GWOT are only concerned with the
      structures. This principle is eloquently illustrated by
      <xref alt="Figure 2.5" rid="fig-gwot-kawa">Figure 2.5</xref> from
      Kawakita et al.
      (<xref alt="2023" rid="ref-kawakita2023" ref-type="bibr">2023</xref>).</p></caption>
      <table-wrap>
        <table>
          <colgroup>
            <col width="50%" />
            <col width="50%" />
          </colgroup>
          <tbody>
            <tr>
              <td align="left"><p specific-use="wrapper">
                <boxed-text>
                  <fig id="fig-expe-subject">
                    <caption><p>(a)</p></caption>
                    <graphic id="fig-expe-subject" mimetype="image" mime-subtype="png" xlink:href="images/my-protocol-1.png" />
                  </fig>
                </boxed-text>
              </p></td>
              <td align="left"><p specific-use="wrapper">
                <boxed-text>
                  <fig id="fig-expe-group">
                    <caption><p>(b)</p></caption>
                    <graphic id="fig-expe-group" mimetype="image" mime-subtype="png" xlink:href="images/my-protocol-2.png" />
                  </fig>
                </boxed-text>
              </p></td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </fig>
  </sec>
  <sec id="hypotheses">
    <title>2.4 Hypotheses</title>
    <sec id="aphantasic-and-phantasic-psychological-spaces">
      <title>Aphantasic and phantasic psychological spaces</title>
      <p>The most representative members of a category are called
      prototypical members.</p>
      <p>Prototype theory builds on the observation that among the
      instances of a property, some are more representative than others.
      The most representative one is the prototype of the property.</p>
      <p>Thus, following the concepts illustrated by Gardenfors
      (<xref alt="2004b" rid="ref-gardenfors2004" ref-type="bibr">2004b</xref>),
      we would expect that aphantasics, when doing shape similarity
      judgements, would be more inclined to group items close to the
      prototypical items due to a lower definition of the mental image.
      In comparison, phantasics would have a much more distributed
      conceptual space of item shapes due to their higher-resolution
      mental images of said items.</p>
    </sec>
    <sec id="subjective-imagery-and-psychological-spaces">
      <title>Subjective imagery and psychological spaces</title>
      <p>In the proposed view of visual imagery as the subjective
      expression of a given type of psychological space, we mentioned
      earlier that <italic>spatial</italic> imagery could also
      constitute a subjective expression of other dimensions of
      psychological spaces. Hence, the <italic>verbal</italic> dimension
      of the simplified model of imagery we outlined in my thesis
      project could also represent different dimensions.</p>
      <p>This conception leads to the following theoretical hypothesis:
      provided that our visual-spatial-verbal model correctly fits
      subjective imagery, the imagery profile of individuals should map
      on their psychological spaces.</p>
      <p>Operationally, this would be evaluated by the fact that
      <bold>individuals with similar imagery profiles</bold> (visual,
      spatial, verbal, or any combination of the three) <bold>should
      have similar representations</bold> in their given psychological
      space, <bold>quantifiable by the degree of alignment between their
      similarity structures.</bold></p>
    </sec>
  </sec>
</sec>
<sec id="study-simulation-results">
  <title>3. Study simulation results</title>
  <sec id="sec-osv-model-theory">
    <title>3.1 Visual-spatial-verbal model of cognitive profiles</title>
    <p>One of the objectives of the study would be to link the
    subjective cognitive profiles of individuals with their
    representational structures. To evaluate these profiles, we are
    going to use psychometric questionnaires evaluating the
    visual-object, spatial, and verbal dimensions of imagery which will
    yield three scores, one for each dimension.</p>
    <p>We are going to simulate 30 participants presenting four
    different cognitive profiles, that I defined as, respectively,
    <italic>verbal</italic> aphantasics, <italic>spatial</italic>
    aphantasics, <italic>spatial</italic> phantasics, and
    <italic>visual</italic> phantasics. Their imagery abilities are
    summarised in
    <xref alt="Table 3.1" rid="tbl-imageries">Table 3.1</xref>.</p>
    <p>To simulate these four sub-groups, we will generate multivariate
    normal distributions of scores on these three dimensions for each
    sub-group. For instance, verbal aphantasics have normally
    distributed visual imagery scores centred around a mean of 0
    (normalized, so negative scores are possible), 0.4 for spatial
    imagery, and 0.7 for verbal style; Spatial aphantasics have means of
    0 for visual, 0.75 spatial, and 0.3 for verbal; etc. The numbers are
    arbitrary, but have been chosen by trial-and-error to obtain a model
    that is both well-defined and not exaggerated. The 30 subjects’
    imagery profiles are represented in the three dimensional space of
    the visual-spatial-verbal dimensions in
    <xref alt="Figure 3.1" rid="fig-plot-osv-model">Figure 3.1</xref>.</p>
    <fig id="tbl-imageries">
      <caption><p>Table 3.1: Imagery abilities of the four hypothesized
      cognitive profiles.</p></caption>
      <table-wrap>
        <table>
          <thead>
            <tr>
              <th>Cognitive profile</th>
              <th align="center">Visual imagery</th>
              <th align="center">Spatial imagery</th>
              <th align="center">Verbal style</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Verbal aphantasic</td>
              <td align="center">–</td>
              <td align="center">-</td>
              <td align="center">++</td>
            </tr>
            <tr>
              <td>Spatial aphantasic</td>
              <td align="center">–</td>
              <td align="center">++</td>
              <td align="center">-</td>
            </tr>
            <tr>
              <td>Spatial phantasic</td>
              <td align="center">+</td>
              <td align="center">++</td>
              <td align="center">-</td>
            </tr>
            <tr>
              <td>Visual phantasic</td>
              <td align="center">++</td>
              <td align="center">-</td>
              <td align="center">+</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </fig>
    <fig id="fig-plot-osv-model">
      <caption><p>Figure 3.1: Imagery profiles generated for 30 subjects
      on the three object, spatial, and verbal dimensions.</p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-plot-osv-model-1.png" />
    </fig>
  </sec>
  <sec id="data-simulation-creating-representational-structures">
    <title>3.2 Data simulation: Creating representational
    structures</title>
    <p>Gardenfors
    (<xref alt="2004b" rid="ref-gardenfors2004" ref-type="bibr">2004b</xref>)
    invokes two scientific concepts, to wit, prototypes and Voronoi
    tessellations. Prototype theory builds on the observation that among
    the instances of a property, some are more representative than
    others. The most representative one is the prototype of the
    property. <italic>We hypothesize that aphantasics will be more
    inclined to categorize items according to prototypes than
    phantasics.</italic></p>
    <p>A Voronoi tesselation of a given space divides that space into a
    number of cells such that each cell has a center and consists of all
    and only those points that lie no closer to the center of any other
    cell than to its own center; the centers of the various cells are
    called the generator points of the tesselation. This principle will
    underlie our data simulation, as we will build representations in a
    3D space based on distances to “centroids”, namely, prototypes.
    These representations will thus be located inside of the
    tessellations around these prototypes, more or less close to the
    centroid depending on the subject’s representational structures.</p>
    <sec id="generating-prototype-embeddings-from-a-sphere">
      <title>Generating “prototype” embeddings from a sphere</title>
      <p>A function will be used to generate embeddings. These spherical
      embeddings are displayed in
      <xref alt="Figure 3.2" rid="fig-perfect-embeddings">Figure 3.2</xref>
      We get 8 nicely distributed clusters. We’ll retrieve the centroids
      of each cluster, which would be the “perfect” categories of each
      species group (say, generated by a computational model on
      categorical criteria).</p>
      <p></p>
      <fig id="fig-perfect-embeddings">
        <caption><p>Figure 3.2: Initial random generations of 1000
        points grouped in 8 clusters to represent the theoretical
        embeddings of 8 groups (i.e. groups of species here).
        <bold><italic>Interact with the figures to see the
        details.</italic></bold></p></caption>
        <table-wrap>
          <table>
            <colgroup>
              <col width="50%" />
              <col width="50%" />
            </colgroup>
            <tbody>
              <tr>
                <td align="left"><p specific-use="wrapper">
                  <boxed-text>
                    <fig id="fig-perfect-embeddings-1">
                      <caption><p>(a) Generated spherical distribution
                      of 1000 observations grouped in 8 equal clusers
                      with Gaussian Mixture Clustering.</p></caption>
                      <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-perfect-embeddings-1.png" />
                    </fig>
                  </boxed-text>
                </p></td>
                <td align="left"><p specific-use="wrapper">
                  <boxed-text>
                    <fig id="fig-perfect-embeddings-2">
                      <caption><p>(b) Centroids of the 8 clusters
                      created on the sphere.</p></caption>
                      <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-perfect-embeddings-2.png" />
                    </fig>
                  </boxed-text>
                </p></td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </fig>
      <p>Now we want two sets of embeddings: one where the observations
      are very concentrated around the centroids, which would be the
      <bold>categorical model</bold>, and one where the observations are
      more spread out, which would be the <bold>visual model</bold>.</p>
      <p>We need to select 8 observations per cluster, which would be
      our animals per group. These observations will be subsets of the
      1000 observations we generated.</p>
    </sec>
    <sec id="categorical-model-embeddings">
      <title>Categorical model embeddings</title>
      <p>The selection procedure for the <bold>categorical model</bold>
      will consist of selecting points that are rather <italic>close to
      the centroids</italic>. Thus, we will filter the observations of
      the large sets to keep only points for which the distance to the
      centroid is inferior to a given value. That is, points for which
      the Euclidean norm of the vector from the observation to the
      centroid:</p>
      <p><disp-formula><alternatives>
      <tex-math><![CDATA[d(centroid, observation) = \sqrt{(x_{c} - x_{o})^{2} + (y_{c} - y_{o})^{2} + (z_{c} - z_{o})^{2}}]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>d</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:math></alternatives></disp-formula></p>
      <fig id="fig-categorical-embeddings">
        <caption><p>Figure 3.3: Selection of 64 points to represent
        prototypical categorical embeddings, based on the distances to
        each groups’ centroid. These will be the bases of the verbal
        aphantasics’ embeddings. <bold><italic>Interact with the figure
        to see the details.</italic></bold></p></caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-categorical-embeddings-1.png" />
      </fig>
    </sec>
    <sec id="visual-model-embeddings">
      <title>Visual model embeddings</title>
      <p>In the case of the <bold>visual model</bold>, we would like
      approximately evenly distributed embeddings, that could also dive
      <italic>inside</italic> the sphere, i.e. representing species that
      are visually close although diametrically opposed when it comes to
      taxonomy. To do this we can simulate multivariate normal
      distributions around the centroids.</p>
      <fig id="fig-visual-embeddings">
        <caption><p>Figure 3.4: Selection of 64 points to represent
        prototypical visual embeddings, chosen randomly in multivariate
        distributions centered around each categorical embedding. The
        visual embeddings are overlaid as diamonds along with
        categorical ones as dots. The two distributions keep the group
        structure, but are pretty far apart at times.
        <bold><italic>Interact with the figure to see the
        details.</italic></bold></p></caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-visual-embeddings-1.png" />
      </fig>
    </sec>
    <sec id="intermediate-embeddings">
      <title>Intermediate embeddings</title>
      <fig id="fig-diagram-intermediate">
        <caption><p>Figure 3.5: Model of the distances between
        participants’ representations. Note that here d is a
        one-dimensional distance between the representations, but it
        will be computed as a three-dimensional distance in our
        toy-model. The verbal aphantasic profile is hypothesized to be
        very categorical, thus diametrically opposed to the visual
        phantasic profile, by a given distance d. Spatial profiles are
        in-between: they are close to each other (10% x d), but the
        spatial aphantasic profile is a bit closer to the verbal
        aphantasic one (45% x d), and the spatial phantasic is a bit
        closer to the visual phantasic one (45% x d).</p></caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/mermaid-figure-2.png" />
      </fig>
      <fig id="fig-intermediate-embeddings">
        <caption><p>Figure 3.6: Space of embeddings with 128 additional
        points based on the euclidean distances between the visual and
        categorical embeddings. The empty dots are the
        <italic>aphantasics-spatial</italic> ones, and the empty
        diamonds are the <italic>phantasic-spatial</italic> ones. Some
        can be very close together, and sometimes further apart due to
        the various pairs of visual and categorical points used to
        create them. A network-like structure seems to appear, with
        empty points seemingly ‘connecting’ the dots and diamonds.
        <bold><italic>Interact with the figure to see the
        details.</italic></bold></p></caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-intermediate-embeddings-1.png" />
      </fig>
      <p>The distributions created are still gathered around the
      centroids of each group, but they are much more widespread, each
      group getting close to each other and even reaching inside the
      sphere.</p>
      <p>Perfect! Now we have two 3D embeddings per animal, in a
      categorical or a visual description of their features. Thus, we
      have four sets of coherent coordinates, around which we will
      simulate the embeddings of the 30 participants, depending on their
      groups.</p>
    </sec>
    <sec id="generating-the-subject-embeddings">
      <title>Generating the subject embeddings</title>
      <p>We have four “reference” sets of embeddings which represent
      animals either judged according to their similarity in categorical
      terms (namely, species), or in visual terms (namely shape or color
      similarities, assuming that these similarities are more evenly
      distributed, e.g. the crab looks like a spider, but is also pretty
      close to a scorpion, etc.).</p>
      <p>To generate the embeddings of each subject in each condition,
      we will start from these reference embeddings and generate random
      noise around <italic>each item</italic>, i.e. for all 64 animals.
      For 100 subjects, we would thus generate 100 noisy points around
      each animal, each point corresponding to a given subject.</p>
      <p>The visual and verbal groups will be generated with slightly
      more intra-group variance, so as to try to make the spatial groups
      as coherent as possible (and avoid blurring everything and making
      the groups disappear in noise).</p>
      <p>Although the groups and species in
      <xref alt="Figure 3.7 (a)" rid="fig-subject-embeddings-1">Figure 3.7
      (a)</xref> look fairly obvious when we colour the embeddings using
      the knowledge about how we built them, the algorithm will only be
      fed with the data for each subject, without any labelling or
      additional information. Thus,
      <xref alt="Figure 3.7 (b)" rid="fig-subject-embeddings-2">Figure 3.7
      (b)</xref> is what the algorithm will actually “see” (and what it
      will try to decrypt). Said otherwise, its objective will be to
      find all the correct colours and shapes in
      <xref alt="Figure 3.7 (a)" rid="fig-subject-embeddings-1">Figure 3.7
      (a)</xref> using only 30 sub-datasets (one for each subject) that
      are illustrated in
      <xref alt="Figure 3.7 (b)" rid="fig-subject-embeddings-2">Figure 3.7
      (b)</xref>. Admittedly, that looks a lot more complicated.</p>
      <p></p>
      <fig id="fig-subject-embeddings">
        <caption><p>Figure 3.7: Final distribution of the 64 embeddings
        of all the 30 subjects, amounting to 1920 points total.
        <bold><italic>Interact with the figures to see the
        details.</italic></bold></p></caption>
        <table-wrap>
          <table>
            <colgroup>
              <col width="50%" />
              <col width="50%" />
            </colgroup>
            <tbody>
              <tr>
                <td align="left"><p specific-use="wrapper">
                  <boxed-text>
                    <fig id="fig-subject-embeddings-1">
                      <caption><p>(a) Distribution of the embeddings of
                      the 30 subjects,<bold><italic>colored by the
                      species groups</italic></bold> they represent. The
                      symbols represent the four imagery groups (Aph.
                      verbal, spatial, etc.)</p></caption>
                      <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-subject-embeddings-1.png" />
                    </fig>
                  </boxed-text>
                </p></td>
                <td align="left"><p specific-use="wrapper">
                  <boxed-text>
                    <fig id="fig-subject-embeddings-2">
                      <caption><p>(b) Distribution of the embeddings of
                      the 30 subjects, <bold><italic>colored by
                      subject</italic></bold>.This is the only
                      information the unsupervised algorithm will have
                      to work with.</p></caption>
                      <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-subject-embeddings-2.png" />
                    </fig>
                  </boxed-text>
                </p></td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </fig>
    </sec>
  </sec>
  <sec id="data-analysis-aligning-representational-structures">
    <title>3.3 Data analysis: Aligning representational
    structures</title>
    <p>For all this section, we need to adapt a simple version of the
    explanations from Kawakita et al.
    (<xref alt="2023" rid="ref-kawakita2023" ref-type="bibr">2023</xref>)
    and Sasaki et al.
    (<xref alt="2023" rid="ref-sasakiToolboxGromovWassersteinOptimal2023" ref-type="bibr">2023</xref>)
    and avoid any technical aspects in the main manuscript.</p>
    <p>From there on, most of the code follows the instructions from the
    open-source scientific toolbox by Sasaki et al.
    (<xref alt="2023" rid="ref-sasakiToolboxGromovWassersteinOptimal2023" ref-type="bibr">2023</xref>).
    I added a few explanations on the purpose of each step, without
    diving into unnecessary details.</p>
    <sec id="step-1-importing-the-embeddings-in-the-python-instances">
      <title>Step 1: Importing the embeddings in the Python
      instances</title>
    </sec>
    <sec id="step-2-setting-the-parameters-for-the-optimization-of-gwot">
      <title>Step 2: Setting the parameters for the optimization of
      GWOT</title>
    </sec>
    <sec id="step-3-gromov-wasserstein-optimal-transport-gwot-between-representations">
      <title>Step 3: Gromov-Wasserstein Optimal Transport (GWOT) between
      Representations</title>
    </sec>
    <sec id="step-4-evaluation-and-visualization">
      <title>Step 4: Evaluation and Visualization</title>
      <sec id="clustering-the-subjects-by-alignment-accuracy">
        <title>Clustering the subjects by alignment accuracy</title>
        <p>First, we evaluate the accuracy per subject and group the
        subjects based on the alignment accuracy via hierarchical
        clustering. This procedure is represented in
        <xref alt="Figure 3.8" rid="fig-hclust">Figure 3.8</xref>.
        Second, we evaluate the accuracy of the alignment between these
        clusters.</p>
        <fig id="fig-hclust">
          <caption><p>Figure 3.8: Hierachical clustering of the 30
          subjects based on their representational
          alignment.</p></caption>
          <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-hclust-1.png" />
        </fig>
        <fig id="fig-clusters-accuracy">
          <caption><p>Figure 3.9: Accuracy of the alignments between the
          subject’s embeddings in each cluster. An alignment of a
          cluster with itself (e.g. Cluster 7 - Cluster 7) is the
          evaluation of the alignment of the subjects
          <bold><italic>inside</italic></bold> the
          cluster.</p></caption>
          <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-clusters-accuracy-1.png" />
        </fig>
      </sec>
      <sec id="evaluating-the-clusters-in-light-of-our-theoretical-osv-model">
        <title>Evaluating the clusters in light of our theoretical OSV
        model</title>
        <p>Let’s see the composition of the clusters in light of our
        initial O-S-V model. The cognitive profiles of the subjects are
        represented in
        <xref alt="Figure 3.10" rid="fig-plot-osv-clusters">Figure 3.10</xref>,
        and the distribution of the cognitive profiles in the clusters
        is represented in
        <xref alt="Figure 3.11" rid="fig-clusters-distribution">Figure 3.11</xref>.</p>
        <fig id="fig-plot-osv-clusters">
          <caption><p>Figure 3.10: Imagery profiles of the nine
          identified clusters on the three object, spatial, and verbal
          dimensions. <bold><italic>Interact with the figure to see the
          details.</italic></bold></p></caption>
          <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-plot-osv-clusters-1.png" />
        </fig>
        <fig id="fig-clusters-distribution">
          <caption><p>Figure 3.11: Repartion of our intial O-S-V groups
          in the clusters created by the unsupervised
          alignment.</p></caption>
          <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-clusters-distribution-1.png" />
        </fig>
        <p>Now let’s visualize the embeddings of the subjects in each
        cluster to get a visual idea of their representational
        structures and the intra-cluster alignment between the
        subjects.</p>
        <p></p>
        <fig id="fig-clusters_embeddings_final">
          <caption><p>Figure 3.12: Psychological spaces (embeddings) of
          the 30 subjects, aligned and clustered with other subjects
          having the most similar representations. The eight colors
          represent the initial eight groups of species that each
          subject had to ‘represent’ with imagery (legends for these
          colors have been taken out for display clarity purposes).
          <bold><italic>Interact with the figures to see the
          details.</italic></bold></p></caption>
          <table-wrap>
            <table>
              <colgroup>
                <col width="33%" />
                <col width="33%" />
                <col width="33%" />
              </colgroup>
              <tbody>
                <tr>
                  <td align="left"><p specific-use="wrapper">
                    <boxed-text>
                      <fig id="fig-clusters_embeddings_final-1">
                        <caption><p>(a) <bold>Cluster 1</bold>
                        embeddings (Aphantasic cluster).Within-cluster
                        aligment accuracy = 95.83%.</p></caption>
                        <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-clusters_embeddings_final-1.png" />
                      </fig>
                    </boxed-text>
                  </p></td>
                  <td align="left"><p specific-use="wrapper">
                    <boxed-text>
                      <fig id="fig-clusters_embeddings_final-2">
                        <caption><p>(b) <bold>Cluster 2</bold>
                        embeddings (Spatial aphantasic
                        cluster).Within-cluster aligment accuracy =
                        97.66%.</p></caption>
                        <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-clusters_embeddings_final-2.png" />
                      </fig>
                    </boxed-text>
                  </p></td>
                  <td align="left"><p specific-use="wrapper">
                    <boxed-text>
                      <fig id="fig-clusters_embeddings_final-3">
                        <caption><p>(c) <bold>Cluster 3</bold>
                        embeddings (Spatial aphantasic
                        cluster).Within-cluster aligment accuracy =
                        94.01%.</p></caption>
                        <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-clusters_embeddings_final-3.png" />
                      </fig>
                    </boxed-text>
                  </p></td>
                </tr>
              </tbody>
            </table>
          </table-wrap>
          <table-wrap>
            <table>
              <colgroup>
                <col width="33%" />
                <col width="33%" />
                <col width="33%" />
              </colgroup>
              <tbody>
                <tr>
                  <td align="left"><p specific-use="wrapper">
                    <boxed-text>
                      <fig id="fig-clusters_embeddings_final-4">
                        <caption><p>(d) <bold>Cluster 4</bold>
                        embeddings (Aphantasic cluster).Within-cluster
                        aligment accuracy = 85.03%.</p></caption>
                        <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-clusters_embeddings_final-4.png" />
                      </fig>
                    </boxed-text>
                  </p></td>
                  <td align="left"><p specific-use="wrapper">
                    <boxed-text>
                      <fig id="fig-clusters_embeddings_final-5">
                        <caption><p>(e) <bold>Cluster 5</bold>
                        embeddings (Verbal aphantasic
                        cluster).Within-cluster aligment accuracy =
                        96.88%.</p></caption>
                        <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-clusters_embeddings_final-5.png" />
                      </fig>
                    </boxed-text>
                  </p></td>
                  <td align="left"><p specific-use="wrapper">
                    <boxed-text>
                      <fig id="fig-clusters_embeddings_final-6">
                        <caption><p>(f) <bold>Cluster 6</bold>
                        embeddings (Verbal aphantasic
                        cluster).Within-cluster aligment accuracy =
                        81.80%.</p></caption>
                        <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-clusters_embeddings_final-6.png" />
                      </fig>
                    </boxed-text>
                  </p></td>
                </tr>
              </tbody>
            </table>
          </table-wrap>
          <table-wrap>
            <table>
              <colgroup>
                <col width="33%" />
                <col width="33%" />
                <col width="33%" />
              </colgroup>
              <tbody>
                <tr>
                  <td align="left"><p specific-use="wrapper">
                    <boxed-text>
                      <fig id="fig-clusters_embeddings_final-7">
                        <caption><p>(g) <bold>Cluster 7</bold>
                        embeddings (Phantasic cluster).Within-cluster
                        aligment accuracy = 98.23%.</p></caption>
                        <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-clusters_embeddings_final-7.png" />
                      </fig>
                    </boxed-text>
                  </p></td>
                  <td align="left"><p specific-use="wrapper">
                    <boxed-text>
                      <fig id="fig-clusters_embeddings_final-8">
                        <caption><p>(h) <bold>Cluster 8</bold>
                        embeddings (Phantasic cluster).Within-cluster
                        aligment accuracy = 74.64%.</p></caption>
                        <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-clusters_embeddings_final-8.png" />
                      </fig>
                    </boxed-text>
                  </p></td>
                  <td align="left"><p specific-use="wrapper">
                    <boxed-text>
                      <fig id="fig-clusters_embeddings_final-9">
                        <caption><p>(i) <bold>Cluster 9</bold>
                        embeddings (Phantasic cluster).Within-cluster
                        aligment accuracy = 42.45%.</p></caption>
                        <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-clusters_embeddings_final-9.png" />
                      </fig>
                    </boxed-text>
                  </p></td>
                </tr>
              </tbody>
            </table>
          </table-wrap>
        </fig>
      </sec>
    </sec>
  </sec>
  <sec id="summary-of-the-simulation-analysis">
    <title>3.4 Summary of the simulation analysis</title>
    <list list-type="bullet">
      <list-item>
        <p>We generated subject data on subjective imagery based on the
        Object-Spatial-Verbal model of cognitive styles and
        representations</p>
      </list-item>
      <list-item>
        <p>We translated this model into a model of the distances
        between participants’ mental representations</p>
      </list-item>
      <list-item>
        <p>We generated random data based on this theoretical model, on
        the representations of 64 items with categorical and visual
        properties by 30 participants</p>
      </list-item>
      <list-item>
        <p>We used an unsupervised alignment algorithm to judge the
        similarity between the representations of the subjects without
        any knowledge of their initial groups and relations</p>
      </list-item>
      <list-item>
        <p>The algorithm aligned with high precision 9 clusters of
        participants, which were coherent with the initial model we
        created, with several differences and unexpected alignments due
        to the randomization.</p>
      </list-item>
      <list-item>
        <p>The 9 clusters revealed a distinction between verbal
        aphantasics, spatial aphantasics, and phantasics in general.
        This interesting result shows that even though we tried to model
        spatial aphants and phantasics closer to each other, they all
        ended up separated based on visual imagery. This unexpected
        outcome, that went besides our initial intentions, shows that
        such an unsupervised method could reveal coherent patterns of
        representations that we did not expect, even with a relevant
        psychometric model.</p>
      </list-item>
    </list>
    <p>This simulation motivates the idea that, should the imagery of
    participants be accurately fitted by our OSV model (or any other
    model to be tested), this paradigm and analytic method would be able
    to align the representations of participants with the same
    subjective imagery abilities.</p>
    <p>I insist on a key finding : I did not use the data of the OSV
    model presented in
    <xref alt="Section 3.1" rid="sec-osv-model-theory">Section 3.1</xref>
    to generate the subject embeddings. The only hypothesis that guided
    how I simulated the subjects’ embeddings was the distance model I
    envisioned, which is represented in
    <xref alt="Figure 3.5" rid="fig-diagram-intermediate">Figure 3.5</xref>.
    Thus, <bold>the algorithm managed to reverse-engineer my
    logic</bold>, to find the subjects groups I simulated with this
    logic, and it so happens that these groups’ matched the cognitive
    profiles groups I built in the beginning.</p>
    <p>In other words, the algorithm managed to find the common pattern
    - which was the groups pattern - between two models built
    differently, <bold><italic>a common pattern that existed originally
    only in my head.</italic></bold></p>
    <p>I think this “mind-reading”<xref ref-type="fn" rid="fn2">2</xref>
    further argues for the potential of this method to reveal hidden
    patterns of inter-individual differences in subjective experience.
    These patterns could help build models of subjective mental imagery,
    one of the most challenging tasks in cognitive psychology to
    date.</p>
  </sec>
</sec>
<sec id="feasibility">
  <title>4. Feasibility</title>
  <sec id="stimuli-and-study-design">
    <title>4.1 Stimuli and study design</title>
    <p>The simulation study presented here focused on aligning the
    representations <bold><italic>between</italic></bold> various
    participants, but a real study should go further and also analyse
    the similarities of representations
    <bold><italic>within</italic></bold> participants, for instance with
    a perception and an imagery condition. This was the basis of the
    study of Shepard &amp; Chipman
    (<xref alt="1970" rid="ref-shepardSecondorderIsomorphismInternal1970" ref-type="bibr">1970</xref>),
    and remains a good starting point to design our own.</p>
  </sec>
  <sec id="sec-online-study-materials">
    <title>4.2 Online study materials</title>
  </sec>
  <sec id="analytic-methods-and-collaborations">
    <title>4.3 Analytic methods and collaborations</title>
    <p>I have proved (<italic>mostly to myself</italic>) that I was
    capable of implementing unsupervised GWOT alignment analysis in
    Python and R using the open-source toolbox provided by Sasaki et al.
    (<xref alt="2023" rid="ref-sasakiToolboxGromovWassersteinOptimal2023" ref-type="bibr">2023</xref>),
    firstly to demonstrate that this key feasibility aspect was not out
    of reach (and that I was prepared to handle it). This toolbox is
    very recent (the associated article was posted on
    bioR<inline-formula><alternatives>
    <tex-math><![CDATA[\chi]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>χ</mml:mi></mml:math></alternatives></inline-formula>iv
    last September) but is based on long-standing theory on similarity
    and cutting-edge topology analysis research. Consequently, I may not
    be confident enough on my expertise in these fields to state with
    confidence that my analyses of this project’s data would be solid,
    which is a very important aspect for me. I need to be convinced by
    analytical choices, which are often taken for granted, to believe in
    their results and implications. I’m convinced of the relevance of
    what I’ve done here, but in the context a real application, even
    more expertise (and other analytical points of view) would be most
    welcome.</p>
    <p>Therefore, I think that this project could be the opportunity to
    collaborate with several teams working in these fields that have
    great data analysis expertise.</p>
    <list list-type="bullet">
      <list-item>
        <p>Starting of course with
        <ext-link ext-link-type="uri" xlink:href="https://lnalborczyk.github.io/"><bold>Ladislas
        Nalborczyk</bold></ext-link> (who gave me this idea), who works
        on synesthesia and inner speech aphantasia at the <italic>Paris
        Brain Institute</italic> with <bold>Laurent Cohen</bold> and
        <bold>Stanislas Dehaene</bold>.</p>
      </list-item>
      <list-item>
        <p><ext-link ext-link-type="uri" xlink:href="https://zuckermaninstitute.columbia.edu/nikolaus-kriegeskorte-phd"><bold>Nikolaus
        Kriegeskorte</bold></ext-link>, one of the creators of the
        famous Representational Similarity Analysis (RSA, another
        <italic>supervised</italic> alignment method, see
        <xref alt="Kriegeskorte et al., 2008" rid="ref-kriegeskorte2008" ref-type="bibr">Kriegeskorte
        et al., 2008</xref>) and his colleagues could be precious
        collaborators for alignment analyses and study materials.</p>
      </list-item>
      <list-item>
        <p>The Japanese team of
        <ext-link ext-link-type="uri" xlink:href="https://sites.google.com/a/g.ecc.u-tokyo.ac.jp/oizumi-lab/home/member/masafumi_oizumi?pli=1"><bold>Masafumi
        Oizumi</bold></ext-link> behind the GWOT toolbox is of course
        also very knowledgeable on the subject, the method, and its
        technical implementation.</p>
      </list-item>
      <list-item>
        <p>They are collaborating with the Australia-based team of
        <ext-link ext-link-type="uri" xlink:href="https://research.monash.edu/en/persons/nao-tsuchiya"><bold>Naotsugu
        Tsuchiya</bold></ext-link>, with whom they recently published
        several very interesting articles on similarity as a concept and
        method for perception research
        (<xref alt="Kawakita et al., n.d." rid="ref-kawakitaComparingColorSimilarity2023" ref-type="bibr">Kawakita
        et al., n.d.</xref>,
        <xref alt="2023" rid="ref-kawakita2023" ref-type="bibr">2023</xref>;
        e.g.
        <xref alt="Tsuchiya et al., 2022" rid="ref-tsuchiyaEnrichedCategoryModel2022" ref-type="bibr">Tsuchiya
        et al., 2022</xref>;
        <xref alt="Zeleznikow-Johnston et al., 2023" rid="ref-zeleznikow-johnstonAreColorExperiences2023" ref-type="bibr">Zeleznikow-Johnston
        et al., 2023</xref>).</p>
      </list-item>
      <list-item>
        <p>Visiting
        <ext-link ext-link-type="uri" xlink:href="https://research.monash.edu/en/persons/nao-tsuchiya">Tsuchiya’s
        webpage</ext-link>, I also found an amazing chain of connections
        that lead us to his team. Tsuchiya also works on sleep and
        dreams and has collaborated several times with
        <ext-link ext-link-type="uri" xlink:href="https://www.movit.paris/team-pi/thomas-andrillon"><bold>Thomas
        Andrillon</bold></ext-link>, who works at the <italic>Paris
        Brain Institute</italic>, thus close to Dehaene, Cohen,
        Bartolomeo, and Nalborczyk, and is very technically
        knowledgeable. Even more interestingly, Andrillon is a co-author
        of
        <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41598-020-65705-7">one
        of Alexei Dawes’ most famous papers on aphantasia</ext-link>,
        probably because they surveyed aphantasics about dreams (looking
        at the author contributions, he apparently took part in the
        study concept, data analysis, and critical revisions). Further,
        Tsuchiya and Andrillon are co-directors of
        <ext-link ext-link-type="uri" xlink:href="https://www.movit.paris/team-members/decat-nicolas"><bold>Nicolas
        Decat</bold></ext-link>, whom I met at the <italic>Immersion and
        Synesthesia Conference</italic> where he gave another talk. So
        Tsuchiya and Andrillon might even have indirectly heard of our
        work! (Provided that my talk was noticeable enough for Nicolas -
        or anyone else - to tell them about it…)</p>
      </list-item>
    </list>
  </sec>
</sec>
<sec id="conclusion">
  <title>Conclusion</title>
  <p>Using the unsupervised alignment method that we exposed and tested
  in this report, Kawakita have shown that relational properties of
  color representations were universally shared by color-neurotypical
  individuals, but structurally different from color-atypical
  individuals. Yet intriguingly, their results also support the
  hypothesis that color-atypical individuals have a different structure
  of their color representations, rather than simply failing to
  experience certain colors. This observation on color-atypical
  individuals, which emerges primarily from the novel consideration of
  color representation in a psychological space, foreshadows of the
  potential of this technique to demystify aphantasia.Such perspectives
  open up unexpected avenues of research to address the impossibility of
  comparing subjective experiences using psychophysical science.</p>
  <p>I tried to show in this project report that this type of (very
  simple) paradigm focusing on similarities between participants’
  subjective representations, combined with a state-of-the-art
  unsupervised alignment method that I was able to implement using an
  open-source Python scientific toolbox, can be extremely promising for
  objectifying the difference (or lack of difference) between people’s
  representational formats. This objectification is intrinsically tied
  to the idea of a link between similarities and representations, but we
  have good evidence to support this hypothesis. So, provided we create
  a good study design, this project would enable us to make robust
  inferences about the contents of analog representations (visual,
  spatial, auditory-verbal) of aphantasics and phantasics, independent
  of any subjective assumptions or relationships.</p>
</sec>
</body>

<back>
<ref-list>
  <title></title>
  <ref id="ref-roads2024">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Roads</surname><given-names>Brett D.</given-names></name>
        <name><surname>Love</surname><given-names>Bradley C.</given-names></name>
      </person-group>
      <article-title>Modeling similarity and psychological space</article-title>
      <source>Annual Review of Psychology</source>
      <year iso-8601-date="2024-01">2024</year><month>01</month>
      <volume>75</volume>
      <issue>1</issue>
      <issn>0066-4308, 1545-2085</issn>
      <uri>https://www.annualreviews.org/doi/10.1146/annurev-psych-040323-115131</uri>
      <pub-id pub-id-type="doi">10.1146/annurev-psych-040323-115131</pub-id>
      <fpage>215</fpage>
      <lpage>240</lpage>
    </element-citation>
  </ref>
  <ref id="ref-shepardSecondorderIsomorphismInternal1970">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Shepard</surname><given-names>Roger N</given-names></name>
        <name><surname>Chipman</surname><given-names>Susan</given-names></name>
      </person-group>
      <article-title>Second-order isomorphism of internal representations: Shapes of states</article-title>
      <source>Cognitive Psychology</source>
      <year iso-8601-date="1970-01">1970</year><month>01</month>
      <volume>1</volume>
      <issue>1</issue>
      <issn>0010-0285</issn>
      <uri>https://www.sciencedirect.com/science/article/pii/0010028570900022</uri>
      <pub-id pub-id-type="doi">10.1016/0010-0285(70)90002-2</pub-id>
      <fpage>1</fpage>
      <lpage>17</lpage>
    </element-citation>
  </ref>
  <ref id="ref-sasakiToolboxGromovWassersteinOptimal2023">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Sasaki</surname><given-names>Masaru</given-names></name>
        <name><surname>Takeda</surname><given-names>Ken</given-names></name>
        <name><surname>Abe</surname><given-names>Kota</given-names></name>
        <name><surname>Oizumi</surname><given-names>Masafumi</given-names></name>
      </person-group>
      <article-title>Toolbox for gromov-wasserstein optimal transport: Application to unsupervised alignment in neuroscience</article-title>
      <year iso-8601-date="2023">2023</year>
      <uri>https://www.biorxiv.org/content/10.1101/2023.09.15.558038v1</uri>
      <pub-id pub-id-type="doi">10.1101/2023.09.15.558038</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-tsuchiyaEnrichedCategoryModel2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Tsuchiya</surname><given-names>Naotsugu</given-names></name>
        <name><surname>Phillips</surname><given-names>Steven</given-names></name>
        <name><surname>Saigo</surname><given-names>Hayato</given-names></name>
      </person-group>
      <article-title>Enriched category as a model of qualia structure based on similarity judgements</article-title>
      <source>Consciousness and Cognition</source>
      <year iso-8601-date="2022-05-01">2022</year><month>05</month><day>01</day>
      <volume>101</volume>
      <uri>https://www.sciencedirect.com/science/article/pii/S1053810022000514</uri>
      <pub-id pub-id-type="doi">10.1016/j.concog.2022.103319</pub-id>
      <fpage>103319</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-kawakitaComparingColorSimilarity2023">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kawakita</surname><given-names>Genji</given-names></name>
        <name><surname>Zeleznikow-Johnston</surname><given-names>Ariel</given-names></name>
        <name><surname>Tsuchiya</surname><given-names>Naotsugu</given-names></name>
        <name><surname>Oizumi</surname><given-names>Masafumi</given-names></name>
      </person-group>
      <article-title>Comparing color similarity structures between humans and LLMs via unsupervised alignment</article-title>
      <pub-id pub-id-type="doi">10.48550/arXiv.2308.04381</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-zeleznikow-johnstonAreColorExperiences2023">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Zeleznikow-Johnston</surname><given-names>Ariel</given-names></name>
        <name><surname>Aizawa</surname><given-names>Yasunori</given-names></name>
        <name><surname>Yamada</surname><given-names>Makiko</given-names></name>
        <name><surname>Tsuchiya</surname><given-names>Naotsugu</given-names></name>
      </person-group>
      <article-title>Are color experiences the same across the visual field?</article-title>
      <source>Journal of Cognitive Neuroscience</source>
      <year iso-8601-date="2023-04-01">2023</year><month>04</month><day>01</day>
      <volume>35</volume>
      <issue>4</issue>
      <uri>https://doi.org/10.1162/jocn_a_01962</uri>
      <pub-id pub-id-type="doi">10.1162/jocn_a_01962</pub-id>
      <fpage>509</fpage>
      <lpage>542</lpage>
    </element-citation>
  </ref>
  <ref id="ref-bainbridgeQuantifyingAphantasiaDrawing2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bainbridge</surname><given-names>Wilma A.</given-names></name>
        <name><surname>Pounder</surname><given-names>Zoë</given-names></name>
        <name><surname>Eardley</surname><given-names>Alison F.</given-names></name>
        <name><surname>Baker</surname><given-names>Chris I.</given-names></name>
      </person-group>
      <article-title>Quantifying aphantasia through drawing: Those without visual imagery show deficits in object but not spatial memory</article-title>
      <source>Cortex</source>
      <year iso-8601-date="2021-02-01">2021</year><month>02</month><day>01</day>
      <volume>135</volume>
      <uri>https://www.sciencedirect.com/science/article/pii/S0010945220304317</uri>
      <pub-id pub-id-type="doi">10.1016/j.cortex.2020.11.014</pub-id>
      <fpage>159</fpage>
      <lpage>172</lpage>
    </element-citation>
  </ref>
  <ref id="ref-kriegeskorteInverseMDSInferring2012">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name>
        <name><surname>Mur</surname><given-names>Marieke</given-names></name>
      </person-group>
      <article-title>Inverse MDS: Inferring dissimilarity structure from multiple item arrangements</article-title>
      <source>Frontiers in Psychology</source>
      <year iso-8601-date="2012">2012</year>
      <volume>3</volume>
      <uri>https://www.frontiersin.org/articles/10.3389/fpsyg.2012.00245</uri>
    </element-citation>
  </ref>
  <ref id="ref-decockSimilarityGoodman2011">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Decock</surname><given-names>Lieven</given-names></name>
        <name><surname>Douven</surname><given-names>Igor</given-names></name>
      </person-group>
      <article-title>Similarity After Goodman</article-title>
      <source>Review of Philosophy and Psychology</source>
      <year iso-8601-date="2011-03-01">2011</year><month>03</month><day>01</day>
      <volume>2</volume>
      <issue>1</issue>
      <uri>https://doi.org/10.1007/s13164-010-0035-y</uri>
      <pub-id pub-id-type="doi">10.1007/s13164-010-0035-y</pub-id>
      <fpage>61</fpage>
      <lpage>75</lpage>
    </element-citation>
  </ref>
  <ref id="ref-goodmanSevenStricturesSimilarity1972">
    <element-citation publication-type="chapter">
      <person-group person-group-type="author">
        <name><surname>Goodman</surname><given-names>Nelson</given-names></name>
      </person-group>
      <article-title>Seven strictures on similarity</article-title>
      <publisher-name>Bobs-Merril</publisher-name>
      <year iso-8601-date="1972">1972</year>
    </element-citation>
  </ref>
  <ref id="ref-gardenforsConceptualSpacesFramework2004">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Gardenfors</surname><given-names>Peter</given-names></name>
      </person-group>
      <article-title>Conceptual spaces as a framework for knowledge representation</article-title>
      <year iso-8601-date="2004">2004</year>
    </element-citation>
  </ref>
  <ref id="ref-murHumanObjectSimilarityJudgments2013">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Mur</surname><given-names>Marieke</given-names></name>
        <name><surname>Meys</surname><given-names>Mirjam</given-names></name>
        <name><surname>Bodurka</surname><given-names>Jerzy</given-names></name>
        <name><surname>Goebel</surname><given-names>Rainer</given-names></name>
        <name><surname>Bandettini</surname><given-names>Peter</given-names></name>
        <name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name>
      </person-group>
      <article-title>Human object-similarity judgments reflect and transcend the primate-IT object representation</article-title>
      <source>Frontiers in Psychology</source>
      <year iso-8601-date="2013">2013</year>
      <volume>4</volume>
      <uri>https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00128</uri>
    </element-citation>
  </ref>
  <ref id="ref-majewskaSpatialMultiarrangementClustering2020">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Majewska</surname><given-names>O.</given-names></name>
        <name><surname>McCarthy</surname><given-names>D.</given-names></name>
        <name><surname>Bosch</surname><given-names>J. van den</given-names></name>
        <name><surname>Kriegeskorte</surname><given-names>N.</given-names></name>
        <name><surname>Vulic</surname><given-names>I.</given-names></name>
        <name><surname>Korhonen</surname><given-names>A.</given-names></name>
      </person-group>
      <source>Spatial multi-arrangement for clustering and multi-way similarity dataset construction</source>
      <publisher-name>European Language Resources Association</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <uri>https://www.repository.cam.ac.uk/handle/1810/306834</uri>
    </element-citation>
  </ref>
  <ref id="ref-gardenfors2004">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Gardenfors</surname><given-names>Peter</given-names></name>
      </person-group>
      <article-title>Conceptual Spaces as a Framework for Knowledge Representation</article-title>
      <year iso-8601-date="2004">2004</year>
    </element-citation>
  </ref>
  <ref id="ref-jozwik2016">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Jozwik</surname><given-names>Kamila M.</given-names></name>
        <name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name>
        <name><surname>Mur</surname><given-names>Marieke</given-names></name>
      </person-group>
      <article-title>Visual features as stepping stones toward semantics: Explaining object similarity in IT and perception with non-negative least squares</article-title>
      <source>Neuropsychologia</source>
      <year iso-8601-date="2016-03-01">2016</year><month>03</month><day>01</day>
      <volume>83</volume>
      <uri>https://www.sciencedirect.com/science/article/pii/S0028393215301998</uri>
      <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2015.10.023</pub-id>
      <fpage>201</fpage>
      <lpage>226</lpage>
    </element-citation>
  </ref>
  <ref id="ref-jozwik2017">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Jozwik</surname><given-names>Kamila M.</given-names></name>
        <name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name>
        <name><surname>Storrs</surname><given-names>Katherine R.</given-names></name>
        <name><surname>Mur</surname><given-names>Marieke</given-names></name>
      </person-group>
      <article-title>Deep convolutional neural networks outperform feature-based but not categorical models in explaining object similarity judgments</article-title>
      <source>Frontiers in Psychology</source>
      <year iso-8601-date="2017">2017</year>
      <volume>8</volume>
      <uri>https://www.frontiersin.org/articles/10.3389/fpsyg.2017.01726</uri>
    </element-citation>
  </ref>
  <ref id="ref-marr1997">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Marr</surname><given-names>D.</given-names></name>
        <name><surname>Nishihara</surname><given-names>H. K.</given-names></name>
        <name><surname>Brenner</surname><given-names>Sydney</given-names></name>
      </person-group>
      <article-title>Representation and recognition of the spatial organization of three-dimensional shapes</article-title>
      <source>Proceedings of the Royal Society of London. Series B. Biological Sciences</source>
      <year iso-8601-date="1997-01">1997</year><month>01</month>
      <volume>200</volume>
      <issue>1140</issue>
      <uri>https://royalsocietypublishing.org/doi/abs/10.1098/rspb.1978.0020</uri>
      <pub-id pub-id-type="doi">10.1098/rspb.1978.0020</pub-id>
      <fpage>269</fpage>
      <lpage>294</lpage>
    </element-citation>
  </ref>
  <ref id="ref-kriegeskorte2008">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name>
        <name><surname>Mur</surname><given-names>Marieke</given-names></name>
        <name><surname>Bandettini</surname><given-names>Peter</given-names></name>
      </person-group>
      <article-title>Representational similarity analysis - connecting the branches of systems neuroscience</article-title>
      <source>Frontiers in Systems Neuroscience</source>
      <year iso-8601-date="2008">2008</year>
      <volume>2</volume>
      <uri>https://www.frontiersin.org/articles/10.3389/neuro.06.004.2008</uri>
    </element-citation>
  </ref>
  <ref id="ref-kawakita2023">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kawakita</surname><given-names>Genji</given-names></name>
        <name><surname>Zeleznikow-Johnston</surname><given-names>Ariel</given-names></name>
        <name><surname>Takeda</surname><given-names>Ken</given-names></name>
        <name><surname>Tsuchiya</surname><given-names>Naotsugu</given-names></name>
        <name><surname>Oizumi</surname><given-names>Masafumi</given-names></name>
      </person-group>
      <article-title>Is my&quot; red&quot; your&quot; red&quot;?: Unsupervised alignment of qualia structures via optimal transport</article-title>
      <year iso-8601-date="2023">2023</year>
    </element-citation>
  </ref>
  <ref id="ref-fechner1860">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Fechner</surname><given-names>Gustav Theodor</given-names></name>
      </person-group>
      <source>Elemente der Psychophysik</source>
      <publisher-name>Breitkopf u. Härtel</publisher-name>
      <year iso-8601-date="1860">1860</year>
    </element-citation>
  </ref>
  <ref id="ref-mach1890a">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Mach</surname><given-names>Ernst</given-names></name>
      </person-group>
      <article-title>The analysis of the sensations. antimetaphysical</article-title>
      <source>The Monist</source>
      <year iso-8601-date="1890">1890</year>
      <volume>1</volume>
      <issue>1</issue>
      <uri>https://www.jstor.org/stable/27896829</uri>
      <fpage>48</fpage>
      <lpage>68</lpage>
    </element-citation>
  </ref>
</ref-list>
<fn-group>
  <fn id="fn1">
    <label>1</label><p>Most notably in the works of Fechner
    (<xref alt="1860" rid="ref-fechner1860" ref-type="bibr">1860</xref>)
    and Mach
    (<xref alt="1890" rid="ref-mach1890a" ref-type="bibr">1890</xref>);
    see also Roads &amp; Love
    (<xref alt="2024" rid="ref-roads2024" ref-type="bibr">2024</xref>)
    for an extended review.</p>
  </fn>
  <fn id="fn2">
    <label>2</label><p>Or, less prettily put, “this unsupervised
    extraction of hidden representational features”.</p>
  </fn>
</fn-group>
</back>

<sub-article article-type="notebook" id="nb-8-nb-article">
<front-stub>
<title-group>
<article-title>Unravelling mental representations in aphantasia through
unsupervised alignment</article-title>
<subtitle>Project design and study simulation</subtitle>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Delem</surname>
<given-names>Maël</given-names>
</name>
<string-name>Maël Delem</string-name>

</contrib>
</contrib-group>
<abstract>
<p>Research on aphantasia is confronted with a long-standing conundrum
of all research on consciousness and representations, namely the
theoretical inaccessibility of subjective representations. Drawing on
concepts from similarity and representation research, I endorse the view
that the study of an individual’s mental representations is made
possible by exploiting second-order isomorphism. The concept of
second-order isomorphism means that correspondence should not be sought
in the first-order relation between (a) an external object and (b) the
corresponding internal representation, but in the second-order relation
between (a) the perceived similarities between various external objects
and (b) the similarities between their corresponding internal
representations. Building on this idea, this study project report was
divided into four parts. <bold>First</bold>, I outline the central ideas
underlying similarity research and its applicability to aphantasia
research. <bold>Second</bold>, I present a complete paradigm with an
experimental design and a data analysis plan. The design will be based
on multi-arrangement and inverse multidimensional scaling, a protocol
that can be implemented online to conduct such large-scale research with
high efficiency. The analysis plan will present a state-of-the-art
method for similarity analysis, unsupervised alignment with
Gromov-Wasserstein optimal transport (GWOT). <bold>Third</bold>, I
report a data simulation I’ve done of a potential outcome of this study,
and the successful analysis of this synthetic data using GWOT alignment.
<bold>Fourth</bold>, I analyse the feasability of such a project given
the material constraints of my thesis. I conclude with the expected
utility and benefits of this project.</p>
</abstract>
</front-stub>

<body>
<p></p>
<sec id="work-in-progress-everywhere-so-look-away-shoo-nb-article">
  <title>Work-In-Progress everywhere, <italic>so look away!
  Shoo!</italic></title>
  <p>I just wanted to try
  <ext-link ext-link-type="uri" xlink:href="https://quarto.org/docs/manuscripts/">Quarto’s
  new manuscript format</ext-link> with this personal project.
  Conclusion: it’s <bold><italic>incredibly cool</italic></bold>.</p>
  <boxed-text>
    <disp-quote>
      <p><bold>Project inception</bold></p>
      <p>This project stems from several elements:</p>
      <list list-type="order">
        <list-item>
          <p>The long standing knowledge of the fact that internal
          representations seem impossible to reach due to their
          subjective nature.</p>
        </list-item>
        <list-item>
          <p>The discovery of the article of Shepard &amp; Chipman
          (<xref alt="1970" rid="ref-shepardSecondorderIsomorphismInternal1970-nb-article" ref-type="bibr">1970</xref>)
          that expose the idea of “second-order isomorphism”.</p>
        </list-item>
        <list-item>
          <p>The discovery of state-of-the-art and accessible
          unsupervised analytic methods to study this principle in an
          astonishing way. The last two discoveries (and many more) are
          the fruit of amazing discussions and recommendations from
          Ladislas when he came to the lab on Jan. 26. These motivated
          me to try to implement GWOT in R on data that I wanted to
          create myself to emulate a study we could do.</p>
        </list-item>
      </list>
      <p><bold>I promise that I did this mostly on my spare time, we
      have too many other things to do elsewhere.</bold></p>
      <p><italic>Note: This website may seem very fancy. I wanted to
      take advantage of this personal project to try
      <ext-link ext-link-type="uri" xlink:href="https://quarto.org/docs/manuscripts/">Quarto’s
      new manuscript format</ext-link> for scientific editing.
      Conclusion: it’s <bold>awesome</bold>. It is very likely that I’ll
      end up writing my thesis using
      <ext-link ext-link-type="uri" xlink:href="https://quarto.org/docs/books/">Quarto’s
      book format</ext-link> (through RStudio). This will allow me to
      render the raw text and computations as beautifully formatted PDF
      and Word documents with low effort, and eventually port it as a
      self-contained website when I’m authorized to share it openly… All
      with a single command, just like I did for this website.
      <bold>This also means that you can read the present report on a
      PDF or Word if you wish to do so, the links are in the
      header</bold>. You’ll freeze the nice interactive figures though.
      As a bonus for the curious (or the reviewer), the “MECA Bundle”
      contains absolutely everything tied to this manuscript, well
      sorted, from the code scripts and configuration files to the final
      documents in all formats. <bold>Awesome, I tell
      you</bold>.</italic></p>
    </disp-quote>
  </boxed-text>
  <p></p>
</sec>
<sec id="theoretical-context-nb-article">
  <title>1. Theoretical context</title>
  <p>When we try to compare our thoughts and representations with those
  of others, we quickly realize that the task will be really difficult,
  if not impossible, as we are of course incapable of “living in someone
  else’s head”. If we both try to imagine a dog, I can examine what goes
  on in my head, so can you, but apart from trying to describe our
  experiences verbally, we are up against a wall.</p>
  <p>Now, what if I asked you to tell me how similar you think a dog and
  a panther look like? Let’s say, in the context of the animal kingdom
  as a whole. Visualize them well. Well, I could tell you that,
  <italic>in my opinion, a dog and a panther look no more alike than a
  dog and a whale</italic>. You might tell me:
  <named-content content-type="column-margin"><italic>For this thought
  experiment, let’s imagine two things: (1) that someone could honestly
  say that, and (2) that people would be rating the animals purely on
  the basis of their mental images, and not on categorical features
  (number of legs, fur, etc.), which is unfortunately almost</italic>
  <bold><italic>never</italic></bold> <italic>the case in
  reality.</italic></named-content></p>
  <disp-quote>
    <p>“<bold><italic>What on earth do you imagine a dog and a panther
    look like? Do you also think that a dog looks nothing like a cat?
    What goes on in your head?</italic></bold>”</p>
  </disp-quote>
  <p>…And many people probably agree with you. They mentally “see” and
  compare certain items the same way you do… And just like that,
  <italic>we are back on track</italic>. We managed to better “compare
  our thoughts”! And we even felt we could dive a bit into the “weird”
  representations of someone else.</p>
  <p>The study of individual differences in the format of
  representations and the attempt at understanding those of others
  obviously has a very rich history. It has interested many fields, in
  philosophy, linguistics, sociology, biology, psychology, or
  neuroscience, to name but a few. A myriad of ideas, concepts, models,
  methods, and paradigms have tried to deepen our understanding of
  representations and find the “key” to objectifying them. The principle
  I tried to illustrate with the thought experiment above is at the
  heart of one of these methods trying to unravel representations that
  was born in psychophysics<xref ref-type="fn" rid="fn1-nb-article">1</xref>: the
  study of <bold><italic>similarity.</italic></bold></p>
  <sec id="from-similarity-to-second-order-isomorphism-nb-article">
    <title>1.1 From similarity to second-order isomorphism</title>
  </sec>
  <sec id="psychological-spaces-and-aphantasia-nb-article">
    <title>1.2 Psychological spaces and aphantasia</title>
    <p>While attempting to demonstrate the uselessness of the concept of
    similarity as a philosophical and scientific notion, Goodman
    (<xref alt="1972" rid="ref-goodmanSevenStricturesSimilarity1972-nb-article" ref-type="bibr">1972</xref>)
    has inadvertently expressed an aspect of similarity judgements of
    primary importance to us aphantasia researchers:</p>
    <disp-quote>
      <p>Comparative judgments of similarity often require not merely
      selection of relevant properties but a weighting of their relative
      importance, and variation in both relevance and importance can be
      rapid and enormous. Consider baggage at an airport checking
      station. The spectator may notice shape, size, color, material,
      and even make of luggage; the pilot is more concerned with weight,
      and the passenger with destination and ownership. Which pieces are
      more alike than others depends not only upon what properties they
      share, but upon who makes the comparison, and when. . . .
      Circumstances alter similarities.
      <named-content content-type="column-margin"><italic>Goodman’s
      claim was dismissed since then by propositions of robust
      mathematical models of similarity, e.g. Gardenfors
      (<xref alt="2004a" rid="ref-gardenforsConceptualSpacesFramework2004-nb-article" ref-type="bibr">2004a</xref>),
      Decock &amp; Douven
      (<xref alt="2011" rid="ref-decockSimilarityGoodman2011-nb-article" ref-type="bibr">2011</xref>).</italic></named-content></p>
    </disp-quote>
    <p>This can be easily reversed as an argument in favor of the
    <bold>potential of similarity analyses to highlight the
    inter-individual differences in sensory mental
    representations</bold>. For example, should we ask individuals to
    judge the similarities in shape or color between various objects,
    the <italic>differences between the similarity structures</italic>
    of individuals will be precisely the most important phenomenon for
    us, far less than the constancy between these structures. If we can
    account for the context dependence, as we will propose here with
    explicit instructions, clever task design, and hypothesis-neutral
    analysis, we could overcome the limitations of the inherently
    subjective nature of similarity judgements.</p>
    <p>This idea of a difference in similarity judgements in aphantasia
    seems to transpire in the results of Bainbridge et al.
    (<xref alt="2021" rid="ref-bainbridgeQuantifyingAphantasiaDrawing2021-nb-article" ref-type="bibr">2021</xref>)
    on their drawing study. They have shown that aphantasics had more
    schematic representations during recall, accurate in their spatial
    positioning, but with less sensory details. This difference can be
    seen from two perspectives: (1) a memory deficit for sensory
    properties; (2) a different representational structure of the items
    in their psychological spaces. In the latter case, aphantasics would
    have greater/faster abstraction of their representation of a
    perceived scene, reducing the amount of encoded sensory details
    unconsciously considered to be relevant. Both (1) and (2) can
    theoretically explain the same behavioural response, i.e. less
    sensory elements and correct spatial recall accuracy in aphantasic
    drawings, but <bold>the two have drastically different consequences
    on how we define, characterize, and judge aphantasia.</bold></p>
    <p>The dominant hypothesis seems to be that aphantasics simply have
    an episodic or general memory deficit. Conversely, I hypothesize
    that aphantasics have different representational structures than
    phantasics in certain dimensions of their psychological spaces
    (notably sensory, but potentially abstract too). More generally, I
    hypothesize that the concept of visual imagery evaluates in reality
    the continuous spectrum of representational structures in
    <italic>sensory</italic> dimensions of psychological spaces.
    Mirroring visual imagery, spatial imagery could also be a rough
    psychometric evaluation of the continuous spectrum of structural
    differences in <italic>conceptual/abstract</italic> dimensions of
    psychological spaces. In this view, the psychological space of
    aphantasics would constrain internal representations to particularly
    abstract forms from a very early stage, thus selectively limiting
    the item properties thereafter encoded in long-term memory. In other
    terms, <bold>I hypothesize that aphantasia would not be
    characterized by an episodic memory deficit, but by an episodic
    memory <italic>selectivity</italic> caused by the specific
    characteristics of their representational structures and
    psychological spaces.</bold> This selectivity would have, as we
    already hypothesized several times, benefits and drawbacks.</p>
    <p>Gardenfors
    (<xref alt="2004a" rid="ref-gardenforsConceptualSpacesFramework2004-nb-article" ref-type="bibr">2004a</xref>)
    proposed that differences in psychological (in his terms,
    conceptual) spaces could arise from various sources, whether innate,
    due to learning, or broader cultural or social differences. All
    these hypotheses could be coherent to explain the sources of
    aphantasia. Nevertheless, the study of these sources should be the
    subject of very large-scale or longitudinal studies, which are out
    of the scope of this project.</p>
    <p>Here, we shall rather attempt to <bold>develop a method to
    characterize the differences in aphantasics’ representational
    structures and psychological spaces.</bold></p>
  </sec>
</sec>
<sec id="methods-nb-article">
  <title>2. Methods</title>
  <p>Roads &amp; Love
  (<xref alt="2024" rid="ref-roads2024-nb-article" ref-type="bibr">2024</xref>), in
  a recent review on the state and perspectives of similarity research,
  highlighted two challenges that studies in this field had to face: (1)
  The high cost of collecting behavioral data on a large number of
  stimuli; (2) The lack of software packages being a high barrier to
  entry, making the task of coding models difficult for the
  uninitiated.</p>
  <p>To solve these problems, we present here two solutions,
  respectively for (1) experimental design and (2) data analysis:</p>
  <list list-type="order">
    <list-item>
      <p>A recent method to efficiently acquire similarity judgements,
      the “multiple arrangement of items” and “inverse multidimensional
      scaling” developed by Kriegeskorte &amp; Mur
      (<xref alt="2012" rid="ref-kriegeskorteInverseMDSInferring2012-nb-article" ref-type="bibr">2012</xref>).</p>
    </list-item>
    <list-item>
      <p>An accessible and robust Python toolbox provided by Sasaki et
      al.
      (<xref alt="2023" rid="ref-sasakiToolboxGromovWassersteinOptimal2023-nb-article" ref-type="bibr">2023</xref>)
      to conduct unsupervised alignment analysis using
      Gromov-Wasserstein optimal transport.</p>
    </list-item>
  </list>
  <sec id="experimental-design-nb-article">
    <title>2.1 Experimental design</title>
    <sec id="sec-ma-nb-article">
      <title>Multi-arrangement and inverse multidimensional
      scaling</title>
      <p>Assuming a geometric model of representational similarities,
      Kriegeskorte &amp; Mur
      (<xref alt="2012" rid="ref-kriegeskorteInverseMDSInferring2012-nb-article" ref-type="bibr">2012</xref>)
      developed a multi-arrangement (MA) method to efficiently acquire
      (dis)similarity judgments for large sets of objects. The subject
      has to perform multiple arrangements of item subsets adaptively
      designed for optimal measurement efficiency and for estimating the
      representational dissimilarity matrix (RDM) by combining the
      evidence from the subset arrangements.</p>
      <p>The procedure is illustrated in
      <xref alt="Figure 2.1" rid="fig-multi-arrangement-nb-article">Figure 2.1</xref>.</p>
      <fig id="fig-multi-arrangement-nb-article">
        <caption><p>Figure 2.1: <bold>Acquiring similarity judgements
        with the multi-arrangement method. (A)</bold> Subjects are asked
        to arrange items according to their similarity, using mouse
        drag-and-drop on a computer. The similarity measure is taken as
        the distances between the items: similar items are closer, while
        dissimilar items are further apart. The upper part of the figure
        shows screenshots at different moments of the acquisition for
        one subject. Columns are trials and rows show the object
        arrangements over time, running from the start (top row) to the
        end (last row). The first trial contains all items; subsequent
        trials contain subsets of items that are adaptively selected to
        optimally estimate judged similarity for each subject.
        <bold>(B)</bold> Once acquisition of the final judgements is
        completed, inter-item distances in the final trial arrangements
        are combined over trials by rescaling and averaging to yield a
        single dissimilarity estimate for each object pair. The process
        is illustrated in this figure for two example item pairs: a
        boy’s face and a hand (red), and carrots and a stop sign (blue).
        Their single-trial dissimilarity estimates (arrows) are combined
        into a single dissimilarity estimate, which is placed at the
        corresponding entry of the RDM (lower panel). Mirror-symmetric
        entries are indicated by lighter colors. Figure from Mur et al.
        (<xref alt="2013" rid="ref-murHumanObjectSimilarityJudgments2013-nb-article" ref-type="bibr">2013</xref>).</p></caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="images/multi-arrangement-method-mur-2013.png" />
      </fig>
      <p>A key strength of this method that sets it as particularly
      effective is the “adaptive” part. The goal of the process is to
      acquire similarity judgements as precisely as possible while
      minimizing the total amount of trials. To do so, starting from the
      second trial, selected subsets of the items to be compared are
      presented to the subject: these items are the ones that were very
      close on-screen in previous trials and thus had their distance
      evaluated with lower accuracy by the subject. As the subject has
      to fill the entire “arena” with the items, these subsequent trials
      will necessarily increase the level of precision in the similarity
      judgement between pairs of items. The second key benefit of this
      method is the time and effort gain compared to others. For
      example, to compare every pair of items among 64 different items
      would require <inline-formula><alternatives>
      <tex-math><![CDATA[\frac{64 \times (64-1)}{2} = 2016]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>64</mml:mn><mml:mo>×</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mn>64</mml:mn><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>=</mml:mo><mml:mn>2016</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
      comparisons (i.e. trials). This would be extremely time-consuming,
      while also losing the <italic>context-independence</italic>
      afforded by the MA method due to the presence of other items
      around every time the subject mentally performs a pairwise
      comparison.</p>
      <p>Historically, when referring to the projection of the
      representations of stimuli (e.g., coordinates in geometric space)
      from a high-dimensional space into a lower-dimensional space,
      inference algorithms were commonly called multidimensional scaling
      (<xref alt="Roads &amp; Love, 2024" rid="ref-roads2024-nb-article" ref-type="bibr">Roads
      &amp; Love, 2024</xref>). By analogy, the process of combining
      several lower-dimensional (2D) similarity judgements on-screen to
      form one higher dimensional similarity representation (in the RDM)
      can be conceptually seen as “inverse” multidimensional scaling,
      hence the name given to the method by Kriegeskorte &amp; Mur
      (<xref alt="2012" rid="ref-kriegeskorteInverseMDSInferring2012-nb-article" ref-type="bibr">2012</xref>).</p>
    </sec>
    <sec id="sec-principle-nb-article">
      <title>Principle</title>
      <p>The idea is simple: for a given set of items that have distinct
      and very pictorial visual properties, we would ask a wide range of
      aphantasics, phantasics or hyperphantasics to imagine, mentally
      compare and make similarity judgements between the items. To
      compare these representations with actual perceptual
      representations, the subjects would also perform the same task
      afterwards, this time with actual pictures to compare. Subjects
      would also fill our usual psychometric imagery questionnaires.</p>
      <fig id="fig-spam-mur-nb-article">
        <caption><p>Figure 2.2: Arena layout of the MA protocol used by
        Mur et al.
        (<xref alt="2013" rid="ref-murHumanObjectSimilarityJudgments2013-nb-article" ref-type="bibr">2013</xref>)
        to acquire perceptual similarity judgements on natural images.
        <italic>Click to expand.</italic></p></caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="images/mur-spam-2.png" />
      </fig>
      <p>To “compare imagined items”, we could use a “word” version of
      the MA paradigm. An example from Majewska et al.
      (<xref alt="2020" rid="ref-majewskaSpatialMultiarrangementClustering2020-nb-article" ref-type="bibr">2020</xref>)
      - <italic>who used the method to build large-scale semantic
      similarity resources for Natural Language Processing
      systems</italic> - is represented in
      <xref alt="Figure 2.3" rid="fig-majewska-nb-article">Figure 2.3</xref>.</p>
      <fig id="fig-majewska-nb-article">
        <caption><p>Figure 2.3: Arena layout of the MA protocol used by
        Majewska et al.
        (<xref alt="2020" rid="ref-majewskaSpatialMultiarrangementClustering2020-nb-article" ref-type="bibr">2020</xref>)
        to acquire similarity judgements on word pairs. <italic>Click to
        expand.</italic></p></caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="images/majewska-spam-2.png" />
      </fig>
      <p>We could have the stimuli rated by another set of participants
      on several features.</p>
      <disp-quote>
        <p>« <italic>We deliberately did not specify which object
        properties to focus on, to avoid biasing participants’
        spontaneous mental representation of the similarities between
        objects. Our aim was to obtain similarity judgments that reflect
        the natural representation of objects without forcing
        participants to rely on one given dimension. However,
        participants were asked after having performed the task, what
        dimension(s) they used in judging object similarity.</italic> »
        (<xref alt="Jozwik et al., 2016" rid="ref-jozwik2016-nb-article" ref-type="bibr">Jozwik
        et al., 2016</xref>)</p>
      </disp-quote>
      <disp-quote>
        <p>« <bold><italic>All but one of the 16 participants reported
        arranging the images according to a categorical
        structure.</italic></bold> »
        (<xref alt="Jozwik et al., 2017" rid="ref-jozwik2017-nb-article" ref-type="bibr">Jozwik
        et al., 2017</xref>)</p>
      </disp-quote>
      <p>This result of Jozwik et al.
      (<xref alt="2017" rid="ref-jozwik2017-nb-article" ref-type="bibr">2017</xref>)
      suggests that we should give an explicit instruction about the
      features to focus on, otherwise everyone might bypass visual
      features and mental images in favour of concepts and categories,
      regardless of their mental imagery profile.</p>
      <p>In contrast, if we ask to focus specifically on the visual
      features, then ask subjects about the strategy they used to
      evaluate the similarities, then on the subjectively felt mental
      format of these strategies, we might grasp better insight on the
      sensory representations of subjects.</p>
      <p>We could even go for several comparisons - even though this
      would increase quadratically the number of trials - e.g. :</p>
      <list list-type="bullet">
        <list-item>
          <p>Evaluate to what extent the <bold>shape</bold> <italic>of
          these animals are</italic>
          <bold><italic>similar</italic></bold> <bold>at rest, ignoring
          size differences.</bold></p>
        </list-item>
        <list-item>
          <p>Evaluate to what extent these animals <bold>sound like each
          other.</bold></p>
        </list-item>
        <list-item>
          <p>Etc.</p>
        </list-item>
      </list>
      <disp-quote>
        <p><italic>Note to be added: if you do not know the animal, just
        guess its placement, as this situation is quite unlikely to
        happen (animals chosen are fairly common
        knowledge).</italic></p>
      </disp-quote>
      <p>Kawakita et al.
      (<xref alt="2023" rid="ref-kawakita2023-nb-article" ref-type="bibr">2023</xref>):
      To assess whether the color dissimilarity structures from
      different participants can be aligned in an unsupervised manner,
      we divided color pair similarity data from a large pool of 426
      participants into five participant groups (85 or 86 participants
      per group) to obtain five independent and complete sets of
      pairwise dissimilarity ratings for 93 color stimuli (Fig. 3a).
      Each participant provided a pairwise dissimilarity judgment for a
      randomly allocated subset of the 4371 possible color pairs. We
      computed the mean of all judgments for each color pair in each
      group, generating five full dissimilarity matrices referred to as
      Group 1 to Group 5.</p>
    </sec>
    <sec id="stimuli-nb-article">
      <title>Stimuli</title>
      <p>We would have a list of animal items, that would have several
      characteristics:</p>
      <fig id="fig-marr-nb-article">
        <caption><p>Figure 2.4: Representing the characteristics of
        shapes with cylinders. Figure from Marr et al.
        (<xref alt="1997" rid="ref-marr1997-nb-article" ref-type="bibr">1997</xref>).
        <italic>Click to expand.</italic></p></caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="images/shapes-marr.png" />
      </fig>
      <list list-type="bullet">
        <list-item>
          <p>A name</p>
        </list-item>
        <list-item>
          <p>A category</p>
        </list-item>
        <list-item>
          <p>A shape</p>
        </list-item>
      </list>
      <p>We need orthogonal data:</p>
      <list list-type="bullet">
        <list-item>
          <p>Each class of animal should include each shape
          (roughly)</p>
        </list-item>
        <list-item>
          <p>Each shape should have an animal</p>
        </list-item>
      </list>
      <p>This would imply that category cannot be derived from shape,
      and vice-versa. Thus, a <bold>sorting by shape would reveal to be
      innately visual</bold> (or maybe spatial, if shape concerns this
      type of imagery), and a <bold>sorting by category would reveal an
      abstraction</bold> from these shapes. We expect that the two will
      be mixed to some degree in every subject, but that low-imagery
      would rather tend towards category sorting, while high-imagery
      would tend towards shape sorting.</p>
      <p>Shapes could be very tricky stimuli to discuss. Gardenfors
      (<xref alt="2004b" rid="ref-gardenfors2004-nb-article" ref-type="bibr">2004b</xref>)
      noted that we only have a very sketchy understanding of how we
      perceive and conceptualize things according to their shapes. The
      works of Marr et al.
      (<xref alt="1997" rid="ref-marr1997-nb-article" ref-type="bibr">1997</xref>)
      highlight this difficulty when analysing the complexity of the
      hierarchical judgements of shapes and volumes, as shown in
      <xref alt="Figure 2.4" rid="fig-marr-nb-article">Figure 2.4</xref>.</p>
    </sec>
  </sec>
  <sec id="data-analysis-plan-nb-article">
    <title>2.2 Data analysis plan</title>
    <sec id="unsupervised-alignment-rationale-nb-article">
      <title>Unsupervised alignment rationale</title>
      <p>Visual images can be represented as points in a
      multidimensional psychological space. Embedding algorithms can be
      used to infer latent representations from human similarity
      judgments. While there are an infinite number of potential visual
      features, an embedding algorithm can be used to identify the
      subset of salient features that accurately model human-perceived
      similarity. (<italic>From Roads’ CV</italic>)</p>
      <p>Using an optimization algorithm, the free parameters of a
      psychological space are found by maximizing goodness of fit (i.e.,
      the loss function) to the observed data. Historically, when
      referring specifically to the free parameters that correspond to
      the representation of stimuli (e.g., coordinates in geometric
      space), inference algorithms were commonly called multidimensional
      scaling (MDS), or simply scaling, algorithms.</p>
      <p>In the machine learning literature, analogous inference
      algorithms are often called embedding algorithms. The term
      “embedding” denotes a higher-dimensional representation that is
      embedded in a lower-dimensional space. For that reason, the
      inferred mental representations of a psychological space could
      also be called a psychological embedding.</p>
      <p>Numerous techniques exist, and each has limitations. Popular
      techniques for comparing representations include RSA Kriegeskorte
      et al.
      (<xref alt="2008" rid="ref-kriegeskorte2008-nb-article" ref-type="bibr">2008</xref>)
      and canonical correlation analysis (CCA) (Hotelling 1936).
      Briefly, RSA is a method for comparing two representations that
      assesses the correlation between the implied pairwise similarity
      matrices. CCA is a method that compares two representations by
      finding a pair of latent variables (one for each domain) that are
      maximally correlated.</p>
      <p>One might be tempted to compare two dissimilarity matrices
      assuming stimulus-level “external” correspondence: my “red”
      corresponds to your “red”(Fig. 1d). This type of supervised
      comparison between dissimilarity matrices, known as
      Representational Similarity Analysis (RSA), has been widely used
      in neuroscience to compare various similarity matrices obtained
      from behavioural and neural data. However, there is no guarantee
      that the same stimulus will necessarily evoke the same subjective
      experience across different participants. Accordingly, when
      considering which stimuli evoke which qualia for different
      individuals, we need to consider all possibilities of
      correspondence: my “red” might correspond to your “red”, “green”,
      “purple”, or might lie somewhere between your “orange” and
      “pink”(Fig. 1e). Thus, we compare qualia structures in a purely
      unsupervised manner, without assuming any correspondence between
      individual qualia across participants.</p>
    </sec>
    <sec id="gromov-wasserstein-optimal-transport-nb-article">
      <title>Gromov-Wasserstein optimal transport</title>
      <p>To account for all possible correspondences, we use an
      unsupervised alignment method for quantifying the degree of
      similarity between qualia structures. As shown in Fig. 2a, in
      unsupervised alignment, we do not attach any external (stimuli)
      labels to the qualia embeddings. Instead, we try to find the best
      matching between qualia structures based only on their internal
      relationships (see Methods). After finding the optimal alignment,
      we can use external labels, such as the identity of a color
      stimulus (Fig. 2b), to evaluate how the embeddings of different
      individuals relate to each other. This allows us to determine
      which color embeddings correspond to the same color embeddings
      across individuals or which do not. Checking the assumption that
      these external labels are consistent across individuals allows us
      to assess the plausibility of determining accurate
      inter-individual correspondences between qualia structures of
      different participants.</p>
      <p>To this end, we used the Gromov-Wasserstein optimal transport
      (GWOT) method, which has been applied with great success in
      various fields. GWOT aims to find the optimal mapping between two
      point clouds in different domains based on the distance between
      points within each domain. Importantly, the distances (or
      correspondences) between points “across” different domains are not
      given while those “within” the same domain are given. GWOT aligns
      the point clouds according to the principle that a point in one
      domain should correspond to another point in the other domain that
      has a similar relationship to other points. The principle of the
      method is illustrated in
      <xref alt="Figure 2.5" rid="fig-gwot-kawa-nb-article">Figure 2.5</xref></p>
      <fig id="fig-gwot-kawa-nb-article">
        <caption><p>Figure 2.5: Gromov-Wassertein optimal transport
        principle. Figure from Kawakita et al.
        (<xref alt="2023" rid="ref-kawakita2023-nb-article" ref-type="bibr">2023</xref>).
        <italic>Click to expand.</italic></p></caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="images/kawa-gwot-2.PNG" />
      </fig>
      <p>We first computed the GWD for all pairs of the dissimilarity
      matrices of the 5 groups (Group 1-5) using the optimized
      <inline-formula><alternatives>
      <tex-math><![CDATA[\epsilon]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ϵ</mml:mi></mml:math></alternatives></inline-formula>.
      In Fig. 3b, we show the optimized mapping
      <inline-formula><alternatives>
      <tex-math><![CDATA[\Gamma*]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>Γ</mml:mi><mml:mo>*</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
      between Group 1 and Groups 2-5 (see Supplementary Figure S1 for
      the other pairs). As shown in Fig. 3b, most of the diagonal
      elements in <inline-formula><alternatives>
      <tex-math><![CDATA[\Gamma*]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>Γ</mml:mi><mml:mo>*</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
      show high values, indicating that most colors in one group
      correspond to the same colors in the other groups with high
      probability. We next performed unsupervised alignment of the
      vector embeddings of qualia structures. Although
      <inline-formula><alternatives>
      <tex-math><![CDATA[\Gamma*]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>Γ</mml:mi><mml:mo>*</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
      provides the rough correspondence between the embeddings of qualia
      structures, we should find a more precise mathematical mapping
      between qualia structures in terms of their vector embeddings to
      more accurately assess the similarity between the qualia
      structures. Here, we consider aligning the embeddings of all the
      groups in a common space.</p>
      <p>By applying MDS, we obtained the 3-dimensional embeddings of
      Group 1 and Groups 2-5, referred to as X and Yi, where i = 2, …, 5
      (Fig. 3c). We then aligned Yi to X with the orthogonal rotation
      matrix Qi, which was obtained by solving a Procrustes-type problem
      using the optimized transportation plan
      <inline-formula><alternatives>
      <tex-math><![CDATA[\Gamma*]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>Γ</mml:mi><mml:mo>*</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
      obtained through GWOT (see Methods). Fig. 3d shows the aligned
      embed- dings of Group 2-5 (QiYi) and the embedding of Group 1 (X)
      plotted in the embedded space of X. Each color represents the
      label of a corresponding external color stimulus. Note that even
      though the color labels are shown in Fig. 3d, this is only for the
      visualization purpose and the whole alignment procedure is
      performed in a purely unsupervised manner without relying on the
      color labels. As can be seen in Fig. 3d, the embeddings of similar
      colors from the five groups are located close to each other,
      indicating that similar colors are ‘correctly’ aligned by the
      unsupervised alignment method.</p>
      <p>To evaluate the performance of the unsupervised alignment, we
      computed the k-nearest color matching rate in the aligned space.
      If the same colors from two groups are within the k-nearest colors
      in the aligned space, we consider that the colors are correctly
      matched. We evaluated the matching rates between all the pairs of
      Groups 1-5. The averaged matching rates are 51% when k = 1, 83%
      when k = 3, and 92% when k = 5, respectively. This demonstrates
      the effectiveness of the GW alignment for correctly aligning the
      qualia structures of different participants in an unsupervised
      manner.</p>
      <p>However, as can be seen in Fig. 4b, the optimized mapping
      <inline-formula><alternatives>
      <tex-math><![CDATA[\Gamma*]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>Γ</mml:mi><mml:mo>*</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
      is not lined up diagonally unlike the optimized map- pings between
      color-neurotypical participants groups shown in Fig. 3b (see
      Supplementary Figure S1 for the other pairs). Accordingly, top k
      matching rate between Group 1-5 and Group 6 is 3.0% when k = 1
      (Fig. 4c), which is only slightly above chance
      (<inline-formula><alternatives>
      <tex-math><![CDATA[\approx]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mo>≈</mml:mo></mml:math></alternatives></inline-formula>
      1%). The matching rate did not improve even when we relaxed the
      criterion (6.9% and 11% for k = 3 and k = 5, respectively).
      Moreover, all of the GWD values between Group 1-5 and Group 6 are
      larger than any of the GWD values between color-neurotypical
      participant groups (Fig. 4d).</p>
      <p>These results indicate that the difference between the qualia
      structures of neuro-typical and atypical participants is
      significantly larger than the difference between the qualia
      structures of neuro-typical participants.</p>
    </sec>
  </sec>
  <sec id="paradigm-summary-nb-article">
    <title>2.3 Paradigm summary</title>
    <p>The experimental design and data analysis plans are succinctly
    summarised in
    <xref alt="Figure 2.6" rid="fig-expe-conditions-nb-article">Figure 2.6</xref>.</p>
    <p></p>
    <fig id="fig-expe-conditions-nb-article">
      <caption><p>Figure 2.6: Summary schematics of the proposed
      experimental protocol and data analysis plan. <italic>Click on the
      sub-figures to expand them.</italic>
      <xref alt="Figure 2.6 (a)" rid="fig-expe-subject-nb-article">Figure 2.6
      (a)</xref> represents the two conditions to be completed by each
      subject. These two conditions will allow to compute comparisons
      (alignments) within a subject’s own perceptual and imaginal
      representational structures, but also between subjects (or groups)
      for each modality (see the next figure’s description).
      <bold>(A)</bold> The subject performs two simililarity judgement
      tasks using the MA paradigm presented earlier. <bold>(B)</bold>
      The low-dimensional similarity judgements are converted to a
      high-dimensional Representational Dissimilarity Matrix (RDM)
      through inverse-MDS as a follow-up to extract the results of the
      MA. <bold>(C)</bold> The RDMs are then reduced in dimensionality
      once again to extract relevant dimensions reflecting inferred
      features of the items through MDS, yielding embeddings.
      Three-dimensional projections of these embeddings have been chosen
      here for visualization purposes. <bold>(D)</bold> These embeddings
      are compared through unsupervised alignment using GWOT, which
      results in an estimate of the degree of alignment of the two
      representational structures and in coordinates of aligned
      embeddings. These coordinates allow us to examine the 3D
      visualization shown here and judge by ourselves the “look” of the
      alignment. Here the perception representation aligns with the
      imagination one, from which we could infer that imagined
      representations are made of sensory (rather than abstract
      properties). We expect inter-individual variability in these
      perception-imagination alignments, as shown in the next figure.
      <xref alt="Figure 2.6 (b)" rid="fig-expe-group-nb-article">Figure 2.6
      (b)</xref> represents the comparison between the representational
      structure of different cognitive profiles. In practice, all pairs
      of subjects will be compared to assess their representational
      structure alignments, independently of arbitrary groups. This is
      computationally heavy, but analytically very powerful. This figure
      also tacitly shows an idea supporting the use of unsupervised
      alignment: it is possible that RDMs seem to be very correlated and
      similar, as shown in step <bold>(B)</bold>, but do not align when
      compared without supervision. This contrasts with several
      supervised alignment methods (such as RSA, see
      <xref alt="Kriegeskorte et al., 2008" rid="ref-kriegeskorte2008-nb-article" ref-type="bibr">Kriegeskorte
      et al., 2008</xref>) which usually use the RDM as-is. This
      difference is due to the involvement of labels for items that are
      already known by the researcher to correlate the RDMs, whereas
      unsupervised algorithms such as GWOT are only concerned with the
      structures. This principle is eloquently illustrated by
      <xref alt="Figure 2.5" rid="fig-gwot-kawa-nb-article">Figure 2.5</xref> from
      Kawakita et al.
      (<xref alt="2023" rid="ref-kawakita2023-nb-article" ref-type="bibr">2023</xref>).</p></caption>
      <table-wrap>
        <table>
          <colgroup>
            <col width="50%" />
            <col width="50%" />
          </colgroup>
          <tbody>
            <tr>
              <td align="left"><p specific-use="wrapper">
                <boxed-text>
                  <fig id="fig-expe-subject-nb-article">
                    <caption><p>(a)</p></caption>
                    <graphic id="fig-expe-subject-nb-article" mimetype="image" mime-subtype="png" xlink:href="images/my-protocol-1.png" />
                  </fig>
                </boxed-text>
              </p></td>
              <td align="left"><p specific-use="wrapper">
                <boxed-text>
                  <fig id="fig-expe-group-nb-article">
                    <caption><p>(b)</p></caption>
                    <graphic id="fig-expe-group-nb-article" mimetype="image" mime-subtype="png" xlink:href="images/my-protocol-2.png" />
                  </fig>
                </boxed-text>
              </p></td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </fig>
  </sec>
  <sec id="hypotheses-nb-article">
    <title>2.4 Hypotheses</title>
    <sec id="aphantasic-and-phantasic-psychological-spaces-nb-article">
      <title>Aphantasic and phantasic psychological spaces</title>
      <p>The most representative members of a category are called
      prototypical members.</p>
      <p>Prototype theory builds on the observation that among the
      instances of a property, some are more representative than others.
      The most representative one is the prototype of the property.</p>
      <p>Thus, following the concepts illustrated by Gardenfors
      (<xref alt="2004b" rid="ref-gardenfors2004-nb-article" ref-type="bibr">2004b</xref>),
      we would expect that aphantasics, when doing shape similarity
      judgements, would be more inclined to group items close to the
      prototypical items due to a lower definition of the mental image.
      In comparison, phantasics would have a much more distributed
      conceptual space of item shapes due to their higher-resolution
      mental images of said items.</p>
    </sec>
    <sec id="subjective-imagery-and-psychological-spaces-nb-article">
      <title>Subjective imagery and psychological spaces</title>
      <p>In the proposed view of visual imagery as the subjective
      expression of a given type of psychological space, we mentioned
      earlier that <italic>spatial</italic> imagery could also
      constitute a subjective expression of other dimensions of
      psychological spaces. Hence, the <italic>verbal</italic> dimension
      of the simplified model of imagery we outlined in my thesis
      project could also represent different dimensions.</p>
      <p>This conception leads to the following theoretical hypothesis:
      provided that our visual-spatial-verbal model correctly fits
      subjective imagery, the imagery profile of individuals should map
      on their psychological spaces.</p>
      <p>Operationally, this would be evaluated by the fact that
      <bold>individuals with similar imagery profiles</bold> (visual,
      spatial, verbal, or any combination of the three) <bold>should
      have similar representations</bold> in their given psychological
      space, <bold>quantifiable by the degree of alignment between their
      similarity structures.</bold></p>
    </sec>
  </sec>
</sec>
<sec id="study-simulation-results-nb-article">
  <title>3. Study simulation results</title>
  <sec id="nb-code-cell-1-nb-article" specific-use="notebook-code">
  <code language="r script"># ═══ Packages ═════════════════════════════════════════════════════════════════

if (!require(librarian)) install.packages(librarian) </code>
  <code id="annotated-cell-2-nb-article" language="r script">library(librarian)                                     

# now putting packages on our library's shelves:
shelf(
  # ─── data management ─────────────────
  holodeck,       # simulating multivariate data
  cluster,        # dissimilarity matrices
  
  # ─── modelling ───────────────────────
  mclust,         # mixture clustering
  
  #  data visualization ──────────────
  # palettes
  viridis,        # colour-blind friendly palettes
  # interactive
  plotly,         # interactive plots
  ggdendro,       # dendrograms
  seriation,      # dissimilarity plots
  webshot2,       # HTML screenshots for Word render
  webshot,
  
  # ─── essential package collections ───
  doParallel,     # parallel execution
  easystats,      # data analysis ecosystem
  reticulate,     # R to Python                    
  tidyverse,      # modern R ecosystem
)

# ─── Global cosmetic theme ───
theme_set(theme_modern(base_size = 14))

pal_okabe_ito &lt;- c(
  &quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;,                            
  &quot;#F5C710&quot;, &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;, &quot;#6c0009&quot;)      

# We'll need 9 colors at some point
pal_okabe_ito_extended &lt;- c(                                 
  &quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;,                           
  &quot;#F5C710&quot;, &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;, &quot;#6c0009&quot;, &quot;#414487FF&quot;)

# We'll need 30 colors at another moment
cool_30_colors &lt;- c(                                                   
  &quot;#3d51b4&quot;, &quot;#414487FF&quot;, &quot;#003d73&quot;, &quot;#440154FF&quot;, &quot;#6c0009&quot;, &quot;#b64e4e&quot;,
  &quot;#D55E00&quot;, &quot;#E69F00&quot;, &quot;#F5C710&quot;, &quot;#FDE725FF&quot;, &quot;#f2bb7b&quot;, &quot;#f1afad&quot;, &quot;#CC79A7&quot;, 
  &quot;#e57774&quot;, &quot;#7AD151FF&quot;, &quot;#57b571&quot;, &quot;#318a4a&quot;, &quot;#009E73&quot;, &quot;#22A884FF&quot;, 
  &quot;#2A788EFF&quot;, &quot;#0072B2&quot;, &quot;#2da6b5&quot;, &quot;#56B4E9&quot;, &quot;#889be0&quot;, &quot;#6677e0&quot;,   
  &quot;#3d51b4&quot;, &quot;#414487FF&quot;, &quot;#003d73&quot;, &quot;#440154FF&quot;, &quot;#6c0009&quot;, &quot;#b64e4e&quot;  
  )

path = &quot;notebooks/data/&quot;

df &lt;- read_rds(paste0(path, &quot;df.RDS&quot;))
df_embeds &lt;- read_rds(paste0(path, &quot;df_embeds.RDS&quot;))
# Categorical and visual embeddings
df_embeds_categ  &lt;- read_rds(paste0(path, &quot;df_embeds_categ.RDS&quot;))
df_embeds_visual &lt;- read_rds(paste0(path, &quot;df_embeds_visual.RDS&quot;))
# Subject embeddings per sub-group
df_embed_c_sub  &lt;- read_rds(paste0(path, &quot;df_embed_c_sub.RDS&quot;))
df_embed_cs_sub &lt;- read_rds(paste0(path, &quot;df_embed_cs_sub.RDS&quot;))
df_embed_v_sub  &lt;- read_rds(paste0(path, &quot;df_embed_v_sub.RDS&quot;))
df_embed_vs_sub &lt;- read_rds(paste0(path, &quot;df_embed_vs_sub.RDS&quot;))
# Accuracy of the unsupervised alignment (bad = not tidy data)
df_accuracy_all_bad &lt;- read_rds(paste0(path, &quot;df_accuracy_all_bad.RDS&quot;))
df_accuracy_cat_bad &lt;- read_rds(paste0(path, &quot;df_accuracy_cat_bad.RDS&quot;))
# Coordinates of the aligned embeddings from the Python output
coordinates_aligned_embeddings &lt;- read_rds(paste0(path, &quot;coordinates_aligned_embeddings.RDS&quot;))</code>
  <sec id="nb-code-cell-1-output-0-nb-article" specific-use="notebook-output">
  <preformat>Le chargement a nécessité le package : librarian</preformat>
  </sec>
  </sec>
  <sec id="sec-osv-model-theory-nb-article">
    <title>3.1 Visual-spatial-verbal model of cognitive profiles</title>
    <p>One of the objectives of the study would be to link the
    subjective cognitive profiles of individuals with their
    representational structures. To evaluate these profiles, we are
    going to use psychometric questionnaires evaluating the
    visual-object, spatial, and verbal dimensions of imagery which will
    yield three scores, one for each dimension.</p>
    <p>We are going to simulate 30 participants presenting four
    different cognitive profiles, that I defined as, respectively,
    <italic>verbal</italic> aphantasics, <italic>spatial</italic>
    aphantasics, <italic>spatial</italic> phantasics, and
    <italic>visual</italic> phantasics. Their imagery abilities are
    summarised in
    <xref alt="Table 3.1" rid="tbl-imageries-nb-article">Table 3.1</xref>.</p>
    <p>To simulate these four sub-groups, we will generate multivariate
    normal distributions of scores on these three dimensions for each
    sub-group. For instance, verbal aphantasics have normally
    distributed visual imagery scores centred around a mean of 0
    (normalized, so negative scores are possible), 0.4 for spatial
    imagery, and 0.7 for verbal style; Spatial aphantasics have means of
    0 for visual, 0.75 spatial, and 0.3 for verbal; etc. The numbers are
    arbitrary, but have been chosen by trial-and-error to obtain a model
    that is both well-defined and not exaggerated. The 30 subjects’
    imagery profiles are represented in the three dimensional space of
    the visual-spatial-verbal dimensions in
    <xref alt="Figure 3.1" rid="fig-plot-osv-model-nb-article">Figure 3.1</xref>.</p>
    <fig id="tbl-imageries-nb-article">
      <caption><p>Table 3.1: Imagery abilities of the four hypothesized
      cognitive profiles.</p></caption>
      <table-wrap>
        <table>
          <thead>
            <tr>
              <th>Cognitive profile</th>
              <th align="center">Visual imagery</th>
              <th align="center">Spatial imagery</th>
              <th align="center">Verbal style</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Verbal aphantasic</td>
              <td align="center">–</td>
              <td align="center">-</td>
              <td align="center">++</td>
            </tr>
            <tr>
              <td>Spatial aphantasic</td>
              <td align="center">–</td>
              <td align="center">++</td>
              <td align="center">-</td>
            </tr>
            <tr>
              <td>Spatial phantasic</td>
              <td align="center">+</td>
              <td align="center">++</td>
              <td align="center">-</td>
            </tr>
            <tr>
              <td>Visual phantasic</td>
              <td align="center">++</td>
              <td align="center">-</td>
              <td align="center">+</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </fig>
    <sec id="cell-fig-plot-osv-model-nb-article" specific-use="notebook-code">
    <code language="r script">plotting_osv_model &lt;- function(df, grouping_variable, size){
  df |&gt; 
    plot_ly(
      x = ~visual_imagery,
      y = ~spatial_imagery,
      z = ~verbal_profile,
      color = ~df[[grouping_variable]],
      text  = ~df[[grouping_variable]],
      colors = c(&quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;, &quot;#F5C710&quot;),
      type = &quot;scatter3d&quot;,
      mode = &quot;markers+text&quot;,
      marker = list(size = size),
      textfont = list(size = size + 4)
    ) |&gt; 
    layout(
      scene = list(
        xaxis = list(
          title = list(text = &quot;Visual imagery&quot;, font = list(color = &quot;grey&quot;)),
          tickfont = list(color = &quot;grey&quot;)
          ),
        yaxis = list(
          title = list(text = &quot;Spatial imagery&quot;, font = list(color = &quot;grey&quot;)),
          tickfont = list(color = &quot;grey&quot;)
          ),
        zaxis = list(
          title = list(text = &quot;Verbal profile&quot;, font = list(color = &quot;grey&quot;)),
          tickfont = list(color = &quot;grey&quot;)
          )
      ),
      legend = list(title = list(text = &quot;Group&quot;)),
      paper_bgcolor = &quot;transparent&quot;
      )
}

df |&gt; 
  mutate(vis_spa_group = case_when(
    vis_spa_group == &quot;aph_spa_high&quot; ~ &quot;Aph. spatial&quot;,
    vis_spa_group == &quot;aph_spa_low&quot;  ~ &quot;Aph. verbal&quot;,
    vis_spa_group == &quot;phant_spa_high&quot; ~ &quot;Phant. spatial&quot;,
    vis_spa_group == &quot;phant_spa_low&quot;  ~ &quot;Phant. visual&quot;
  )) |&gt; 
  plotting_osv_model(grouping_variable = &quot;vis_spa_group&quot;, size = 4)</code>
    <fig id="fig-plot-osv-model-nb-article">
      <caption><p>Figure 3.1: Imagery profiles generated for 30 subjects
      on the three object, spatial, and verbal dimensions.</p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-plot-osv-model-1.png" />
    </fig>
    <sec id="cell-fig-plot-osv-model-output-0-nb-article" specific-use="notebook-output">
    <preformat>PhantomJS not found. You can install it with webshot::install_phantomjs(). If it is installed, please make sure the phantomjs executable can be found via the PATH variable.</preformat>
    </sec>
    </sec>
  </sec>
  <sec id="data-simulation-creating-representational-structures-nb-article">
    <title>3.2 Data simulation: Creating representational
    structures</title>
    <p>Gardenfors
    (<xref alt="2004b" rid="ref-gardenfors2004-nb-article" ref-type="bibr">2004b</xref>)
    invokes two scientific concepts, to wit, prototypes and Voronoi
    tessellations. Prototype theory builds on the observation that among
    the instances of a property, some are more representative than
    others. The most representative one is the prototype of the
    property. <italic>We hypothesize that aphantasics will be more
    inclined to categorize items according to prototypes than
    phantasics.</italic></p>
    <p>A Voronoi tesselation of a given space divides that space into a
    number of cells such that each cell has a center and consists of all
    and only those points that lie no closer to the center of any other
    cell than to its own center; the centers of the various cells are
    called the generator points of the tesselation. This principle will
    underlie our data simulation, as we will build representations in a
    3D space based on distances to “centroids”, namely, prototypes.
    These representations will thus be located inside of the
    tessellations around these prototypes, more or less close to the
    centroid depending on the subject’s representational structures.</p>
    <sec id="generating-prototype-embeddings-from-a-sphere-nb-article">
      <title>Generating “prototype” embeddings from a sphere</title>
      <sec id="nb-code-cell-2-nb-article" specific-use="notebook-code">
      <code language="r script"># getting the centroids of each cluster
df_centroids &lt;- 
  df_embeds |&gt; 
  group_by(group) |&gt; 
  summarise(
    x_centroid = mean(x),
    y_centroid = mean(y),
    z_centroid = mean(z)
  )

# adding them to the data
df_embeds_2 &lt;- left_join(df_embeds, df_centroids, by = &quot;group&quot;)</code>
      </sec>
      <p>A function will be used to generate embeddings. These spherical
      embeddings are displayed in
      <xref alt="Figure 3.2" rid="fig-perfect-embeddings-nb-article">Figure 3.2</xref>
      We get 8 nicely distributed clusters. We’ll retrieve the centroids
      of each cluster, which would be the “perfect” categories of each
      species group (say, generated by a computational model on
      categorical criteria).</p>
      <code language="r script"># function for 3D plotting up to 8 groups (due to the palette)
plotting_3d &lt;- function(df, size, opacity){
  df |&gt; 
    plot_ly(
      type = &quot;scatter3d&quot;,
      mode = &quot;markers&quot;,
      x = ~x,
      y = ~y,
      z = ~z,
      color = ~paste0(&quot;Species group &quot;, group),
      colors = pal_okabe_ito,
      marker = list(size = size, opacity = opacity)
    ) |&gt; 
    layout(paper_bgcolor = &quot;transparent&quot;)
}

plotting_3d(df_embeds, 3, 1) |&gt; 
  layout(legend = list(
    yanchor = &quot;top&quot;,
    y = 1,
    xanchor = &quot;right&quot;,
    x = 0
    ))
df_centroids |&gt; 
  plot_ly(
    type = &quot;scatter3d&quot;,
    mode = &quot;markers+text&quot;,
    x = ~x_centroid,
    y = ~y_centroid,
    z = ~z_centroid,
    text = ~paste0(&quot;Species group &quot;, group),
    color = ~paste0(&quot;Species group &quot;, group),
    colors = pal_okabe_ito,
    marker = list(size = 12, opacity = 1)
    ) |&gt; 
  layout(
    scene = list(
      xaxis = list(title = &quot;x&quot;),
      yaxis = list(title = &quot;y&quot;),
      zaxis = list(title = &quot;z&quot;)
    ),
    paper_bgcolor = &quot;transparent&quot;,
    showlegend = FALSE
    )</code>
      <fig id="fig-perfect-embeddings-nb-article">
        <caption><p>Figure 3.2: Initial random generations of 1000
        points grouped in 8 clusters to represent the theoretical
        embeddings of 8 groups (i.e. groups of species here).
        <bold><italic>Interact with the figures to see the
        details.</italic></bold></p></caption>
        <table-wrap>
          <table>
            <colgroup>
              <col width="50%" />
              <col width="50%" />
            </colgroup>
            <tbody>
              <tr>
                <td align="left"><p specific-use="wrapper">
                  <boxed-text>
                    <fig id="fig-perfect-embeddings-1-nb-article">
                      <caption><p>(a) Generated spherical distribution
                      of 1000 observations grouped in 8 equal clusers
                      with Gaussian Mixture Clustering.</p></caption>
                      <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-perfect-embeddings-1.png" />
                    </fig>
                  </boxed-text>
                </p></td>
                <td align="left"><p specific-use="wrapper">
                  <boxed-text>
                    <fig id="fig-perfect-embeddings-2-nb-article">
                      <caption><p>(b) Centroids of the 8 clusters
                      created on the sphere.</p></caption>
                      <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-perfect-embeddings-2.png" />
                    </fig>
                  </boxed-text>
                </p></td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </fig>
      <p>Now we want two sets of embeddings: one where the observations
      are very concentrated around the centroids, which would be the
      <bold>categorical model</bold>, and one where the observations are
      more spread out, which would be the <bold>visual model</bold>.</p>
      <p>We need to select 8 observations per cluster, which would be
      our animals per group. These observations will be subsets of the
      1000 observations we generated.</p>
    </sec>
    <sec id="categorical-model-embeddings-nb-article">
      <title>Categorical model embeddings</title>
      <p>The selection procedure for the <bold>categorical model</bold>
      will consist of selecting points that are rather <italic>close to
      the centroids</italic>. Thus, we will filter the observations of
      the large sets to keep only points for which the distance to the
      centroid is inferior to a given value. That is, points for which
      the Euclidean norm of the vector from the observation to the
      centroid:</p>
      <p><disp-formula><alternatives>
      <tex-math><![CDATA[d(centroid, observation) = \sqrt{(x_{c} - x_{o})^{2} + (y_{c} - y_{o})^{2} + (z_{c} - z_{o})^{2}}]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>d</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:math></alternatives></disp-formula></p>
      <sec id="cell-fig-categorical-embeddings-nb-article" specific-use="notebook-code">
      <code language="r script"># Plotting these observations
plotting_3d(df_embeds_categ, 6, 1)</code>
      <fig id="fig-categorical-embeddings-nb-article">
        <caption><p>Figure 3.3: Selection of 64 points to represent
        prototypical categorical embeddings, based on the distances to
        each groups’ centroid. These will be the bases of the verbal
        aphantasics’ embeddings. <bold><italic>Interact with the figure
        to see the details.</italic></bold></p></caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-categorical-embeddings-1.png" />
      </fig>
      </sec>
    </sec>
    <sec id="visual-model-embeddings-nb-article">
      <title>Visual model embeddings</title>
      <p>In the case of the <bold>visual model</bold>, we would like
      approximately evenly distributed embeddings, that could also dive
      <italic>inside</italic> the sphere, i.e. representing species that
      are visually close although diametrically opposed when it comes to
      taxonomy. To do this we can simulate multivariate normal
      distributions around the centroids.</p>
      <sec id="cell-fig-visual-embeddings-nb-article" specific-use="notebook-code">
      <code language="r script">plotting_3d(df_embeds_categ, 4, 1) |&gt; 
  add_trace(
      data = df_embeds_visual,
      type = &quot;scatter3d&quot;,
      mode = &quot;markers&quot;,
      x = ~x,
      y = ~y,
      z = ~z,
      color = ~paste0(&quot;Species group &quot;, group),
      colors = pal_okabe_ito,
      marker = list(size = 4, opacity = 1, symbol = &quot;diamond&quot;)
  )</code>
      <fig id="fig-visual-embeddings-nb-article">
        <caption><p>Figure 3.4: Selection of 64 points to represent
        prototypical visual embeddings, chosen randomly in multivariate
        distributions centered around each categorical embedding. The
        visual embeddings are overlaid as diamonds along with
        categorical ones as dots. The two distributions keep the group
        structure, but are pretty far apart at times.
        <bold><italic>Interact with the figure to see the
        details.</italic></bold></p></caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-visual-embeddings-1.png" />
      </fig>
      </sec>
    </sec>
    <sec id="intermediate-embeddings-nb-article">
      <title>Intermediate embeddings</title>
      <sec id="nb-code-cell-3-nb-article" specific-use="notebook-code">
      <fig id="fig-diagram-intermediate-nb-article">
        <caption><p>Figure 3.5: Model of the distances between
        participants’ representations. Note that here d is a
        one-dimensional distance between the representations, but it
        will be computed as a three-dimensional distance in our
        toy-model. The verbal aphantasic profile is hypothesized to be
        very categorical, thus diametrically opposed to the visual
        phantasic profile, by a given distance d. Spatial profiles are
        in-between: they are close to each other (10% x d), but the
        spatial aphantasic profile is a bit closer to the verbal
        aphantasic one (45% x d), and the spatial phantasic is a bit
        closer to the visual phantasic one (45% x d).</p></caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/mermaid-figure-2.png" />
      </fig>
      </sec>
      <sec id="nb-code-cell-4-nb-article" specific-use="notebook-code">
      <code language="r script"># ─── Generating visual embeddings based on multivariate distributions around 
# the categorical embeddings ───

dist_c = 0.45
dist_v = 0.55

df_embeddings &lt;-
  df_embeds_categ |&gt;
  rename(
    group_c = group,
    x_c = x,
    y_c = y,
    z_c = z
  ) |&gt;
  bind_cols(df_embeds_visual) |&gt;
  rename(
    group_v = group,
    x_v = x,
    y_v = y,
    z_v = z
  ) |&gt; 
  select(!group_v) |&gt; 
  rename(group = group_c) |&gt; 
  mutate(
    x_cs = x_c + dist_c*(x_v - x_c),
    y_cs = y_c + dist_c*(y_v - y_c),
    z_cs = z_c + dist_c*(z_v - z_c),
    x_vs = x_c + dist_v*(x_v - x_c),
    y_vs = y_c + dist_v*(y_v - y_c),
    z_vs = z_c + dist_v*(z_v - z_c)
  )</code>
      <sec id="nb-code-cell-4-output-0-nb-article" specific-use="notebook-output">
      <preformat>New names:
• `species` -&gt; `species...2`
• `species` -&gt; `species...7`</preformat>
      </sec>
      </sec>
      <sec id="cell-fig-intermediate-embeddings-nb-article" specific-use="notebook-code">
      <code language="r script">size = 3

df_embeddings |&gt; 
  plot_ly(
    type = &quot;scatter3d&quot;, 
    mode = &quot;marker&quot;,
    color  = ~paste0(&quot;Species group &quot;, group),
    colors = ~pal_okabe_ito
    ) |&gt; 
  add_markers(
    x = ~x_c, y = ~y_c, z = ~z_c, 
    mode = &quot;marker&quot;,
    marker = list(size = size, symbol = &quot;circle&quot;)
  ) |&gt; 
  add_markers(
    x = ~x_v, y = ~y_v, z = ~z_v, 
    mode = &quot;marker&quot;,
    marker = list(size = size, symbol = &quot;diamond&quot;)
  ) |&gt; 
  add_markers(
    x = ~x_cs, y = ~y_cs, z = ~z_cs, 
    mode = &quot;marker&quot;,
    marker = list(size = size, symbol = &quot;circle-open&quot;)
  ) |&gt; 
  add_markers(
    x = ~x_vs, y = ~y_vs, z = ~z_vs, 
    mode = &quot;marker&quot;,
    marker = list(size = size, symbol = &quot;diamond-open&quot;)
  ) |&gt; 
  layout(
    scene = list(
      xaxis = list(title = &quot;x&quot;),
      yaxis = list(title = &quot;y&quot;),
      zaxis = list(title = &quot;z&quot;)
    ),
    paper_bgcolor = &quot;transparent&quot;)</code>
      <fig id="fig-intermediate-embeddings-nb-article">
        <caption><p>Figure 3.6: Space of embeddings with 128 additional
        points based on the euclidean distances between the visual and
        categorical embeddings. The empty dots are the
        <italic>aphantasics-spatial</italic> ones, and the empty
        diamonds are the <italic>phantasic-spatial</italic> ones. Some
        can be very close together, and sometimes further apart due to
        the various pairs of visual and categorical points used to
        create them. A network-like structure seems to appear, with
        empty points seemingly ‘connecting’ the dots and diamonds.
        <bold><italic>Interact with the figure to see the
        details.</italic></bold></p></caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-intermediate-embeddings-1.png" />
      </fig>
      </sec>
      <p>The distributions created are still gathered around the
      centroids of each group, but they are much more widespread, each
      group getting close to each other and even reaching inside the
      sphere.</p>
      <p>Perfect! Now we have two 3D embeddings per animal, in a
      categorical or a visual description of their features. Thus, we
      have four sets of coherent coordinates, around which we will
      simulate the embeddings of the 30 participants, depending on their
      groups.</p>
      <sec id="nb-code-cell-5-nb-article" specific-use="notebook-code">
      <code language="r script"># ─── Labeling each species in each group ───

df_embeddings &lt;- 
  df_embeddings |&gt; 
  mutate(
    group = case_when(
    group == 1 ~ &quot;a&quot;, 
    group == 2 ~ &quot;b&quot;,
    group == 3 ~ &quot;c&quot;,
    group == 4 ~ &quot;d&quot;,
    group == 5 ~ &quot;e&quot;,
    group == 6 ~ &quot;f&quot;,
    group == 7 ~ &quot;g&quot;,
    group == 8 ~ &quot;h&quot;,
    TRUE ~ group
    )
  ) |&gt; 
  group_by(group) |&gt; 
  mutate(
    species = paste0(&quot;species_&quot;, group, 1:8),
    species = as.factor(species),
    group   = as.factor(group)
    ) |&gt; 
  select(group, species, everything())</code>
      </sec>
    </sec>
    <sec id="generating-the-subject-embeddings-nb-article">
      <title>Generating the subject embeddings</title>
      <p>We have four “reference” sets of embeddings which represent
      animals either judged according to their similarity in categorical
      terms (namely, species), or in visual terms (namely shape or color
      similarities, assuming that these similarities are more evenly
      distributed, e.g. the crab looks like a spider, but is also pretty
      close to a scorpion, etc.).</p>
      <p>To generate the embeddings of each subject in each condition,
      we will start from these reference embeddings and generate random
      noise around <italic>each item</italic>, i.e. for all 64 animals.
      For 100 subjects, we would thus generate 100 noisy points around
      each animal, each point corresponding to a given subject.</p>
      <p>The visual and verbal groups will be generated with slightly
      more intra-group variance, so as to try to make the spatial groups
      as coherent as possible (and avoid blurring everything and making
      the groups disappear in noise).</p>
      <p>Although the groups and species in
      <xref alt="Figure 3.7 (a)" rid="fig-subject-embeddings-1-nb-article">Figure 3.7
      (a)</xref> look fairly obvious when we colour the embeddings using
      the knowledge about how we built them, the algorithm will only be
      fed with the data for each subject, without any labelling or
      additional information. Thus,
      <xref alt="Figure 3.7 (b)" rid="fig-subject-embeddings-2-nb-article">Figure 3.7
      (b)</xref> is what the algorithm will actually “see” (and what it
      will try to decrypt). Said otherwise, its objective will be to
      find all the correct colours and shapes in
      <xref alt="Figure 3.7 (a)" rid="fig-subject-embeddings-1-nb-article">Figure 3.7
      (a)</xref> using only 30 sub-datasets (one for each subject) that
      are illustrated in
      <xref alt="Figure 3.7 (b)" rid="fig-subject-embeddings-2-nb-article">Figure 3.7
      (b)</xref>. Admittedly, that looks a lot more complicated.</p>
      <code language="r script">size = 2

plot_ly(
    type = &quot;scatter3d&quot;, 
    mode = &quot;marker&quot;,
    colors = ~pal_okabe_ito
    ) |&gt; 
  add_markers(
    data = df_embed_c_sub, 
    x = ~x, y = ~y, z = ~z, color  = ~paste0(&quot;Species group &quot;, group),
    mode = &quot;marker&quot;,
    marker = list(size = size, symbol = &quot;circle&quot;)
  ) |&gt; 
  add_markers(
    data = df_embed_v_sub, 
    x = ~x, y = ~y, z = ~z, color  = ~paste0(&quot;Species group &quot;, group),
    mode = &quot;marker&quot;,
    marker = list(size = size, symbol = &quot;diamond&quot;)
  ) |&gt; 
  add_markers(
    data = df_embed_cs_sub, 
    x = ~x, y = ~y, z = ~z, color  = ~paste0(&quot;Species group &quot;, group), 
    mode = &quot;marker&quot;,
    marker = list(size = size, symbol = &quot;circle-open&quot;)
  ) |&gt; 
  add_markers(
    data = df_embed_vs_sub, 
    x = ~x, y = ~y, z = ~z, color  = ~paste0(&quot;Species group &quot;, group), 
    mode = &quot;marker&quot;,
    marker = list(size = size, symbol = &quot;diamond-open&quot;)
  ) |&gt; 
  layout(legend = list(
    yanchor = &quot;top&quot;,
    y = 1,
    xanchor = &quot;right&quot;,
    x = 0
    ),
    paper_bgcolor = &quot;transparent&quot;,
    showlegend = FALSE
    )
bind_rows(
  # aph_spatial
  df_embed_cs_sub |&gt; 
    separate_wider_delim(
      subject,
      delim = &quot;_&quot;,
      names = c(&quot;subject&quot;, &quot;number&quot;)
    ) |&gt; 
    mutate(subject = paste0(subject,&quot;_a_aph_s&quot;, number), .keep = &quot;unused&quot;),
  
  # aph_verbal
  df_embed_c_sub  |&gt;
    separate_wider_delim(
      subject,
      delim = &quot;_&quot;,
      names = c(&quot;subject&quot;, &quot;number&quot;)
    ) |&gt; 
    mutate(subject = paste0(subject,&quot;_b_aph_v&quot;, number), .keep = &quot;unused&quot;),
  
  # phant spatial
  df_embed_vs_sub  |&gt;
    separate_wider_delim(
      subject,
      delim = &quot;_&quot;,
      names = c(&quot;subject&quot;, &quot;number&quot;)
    ) |&gt; 
    mutate(subject = paste0(subject,&quot;_c_phant_s&quot;, number), .keep = &quot;unused&quot;),
  
  # phant visual
  df_embed_v_sub  |&gt;
    separate_wider_delim(
      subject,
      delim = &quot;_&quot;,
      names = c(&quot;subject&quot;, &quot;number&quot;)
    ) |&gt; 
    mutate(subject = paste0(subject,&quot;_d_phant_v&quot;, number), .keep = &quot;unused&quot;)
  ) |&gt;
  plot_ly() |&gt; 
  add_markers(
      x = ~x, y = ~y, z = ~z,
      color = ~subject,
      colors = cool_30_colors,
      marker = list(size = 3)
    ) |&gt; 
  layout(
    paper_bgcolor = &quot;transparent&quot;,
    showlegend = FALSE
    )</code>
      <fig id="fig-subject-embeddings-nb-article">
        <caption><p>Figure 3.7: Final distribution of the 64 embeddings
        of all the 30 subjects, amounting to 1920 points total.
        <bold><italic>Interact with the figures to see the
        details.</italic></bold></p></caption>
        <table-wrap>
          <table>
            <colgroup>
              <col width="50%" />
              <col width="50%" />
            </colgroup>
            <tbody>
              <tr>
                <td align="left"><p specific-use="wrapper">
                  <boxed-text>
                    <fig id="fig-subject-embeddings-1-nb-article">
                      <caption><p>(a) Distribution of the embeddings of
                      the 30 subjects,<bold><italic>colored by the
                      species groups</italic></bold> they represent. The
                      symbols represent the four imagery groups (Aph.
                      verbal, spatial, etc.)</p></caption>
                      <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-subject-embeddings-1.png" />
                    </fig>
                  </boxed-text>
                </p></td>
                <td align="left"><p specific-use="wrapper">
                  <boxed-text>
                    <fig id="fig-subject-embeddings-2-nb-article">
                      <caption><p>(b) Distribution of the embeddings of
                      the 30 subjects, <bold><italic>colored by
                      subject</italic></bold>.This is the only
                      information the unsupervised algorithm will have
                      to work with.</p></caption>
                      <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-subject-embeddings-2.png" />
                    </fig>
                  </boxed-text>
                </p></td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </fig>
      <sec id="nb-code-cell-6-nb-article" specific-use="notebook-code">
      <code language="r script"># ─── Grouping the 64 embeddings of each subject in their own dataframe ───

df_embeddings_sub &lt;-
  bind_rows(
    # aphantasic spatial
    df_embed_cs_sub |&gt; 
      separate_wider_delim(
        subject,
        delim = &quot;_&quot;,
        names = c(&quot;subject&quot;, &quot;number&quot;)
      ) |&gt; 
      mutate(subject = paste0(subject,&quot;_a_aph_s&quot;, number), .keep = &quot;unused&quot;),
    
    # aphantasic verbal
    df_embed_c_sub  |&gt;
      separate_wider_delim(
        subject,
        delim = &quot;_&quot;,
        names = c(&quot;subject&quot;, &quot;number&quot;)
      ) |&gt; 
      mutate(subject = paste0(subject,&quot;_b_aph_v&quot;, number), .keep = &quot;unused&quot;),
    
    # phantasic spatial
    df_embed_vs_sub  |&gt;
      separate_wider_delim(
        subject,
        delim = &quot;_&quot;,
        names = c(&quot;subject&quot;, &quot;number&quot;)
      ) |&gt; 
      mutate(subject = paste0(subject,&quot;_c_phant_s&quot;, number), .keep = &quot;unused&quot;),
    
    # phantasic visual
    df_embed_v_sub  |&gt;
      separate_wider_delim(
        subject,
        delim = &quot;_&quot;,
        names = c(&quot;subject&quot;, &quot;number&quot;)
      ) |&gt; 
      mutate(subject = paste0(subject,&quot;_d_phant_v&quot;, number), .keep = &quot;unused&quot;)
  ) |&gt; 
  mutate(subject = as.factor(subject)) |&gt; 
  select(!c(group, species)) |&gt; 
  group_by(subject) |&gt; 
  nest() |&gt; 
  rename(embedding = data) |&gt; 
  rowwise() |&gt; 
  mutate(embedding = list(as.matrix(embedding)))</code>
      </sec>
      <sec id="nb-code-cell-7-nb-article" specific-use="notebook-code">
      <code language="r script"># number of available cores
n_cores &lt;- as.integer(parallel::detectCores() - 1)

# all the categories of animals
animal_categories &lt;- paste0(&quot;genre_&quot;, letters[1:8])

# the ids of each animal
animal_names &lt;- 
  paste0(&quot;species_&quot;,  letters[1:8]) |&gt; 
  paste(rep(seq(1, 8, 1), each = 8), sep = &quot;&quot;)

# the indexes of each animal, linking them to the categories
animal_indexes &lt;- list(
  rep(0:7), 
  rep(8:15),
  rep(16:23), 
  rep(24:31), 
  rep(32:39), 
  rep(40:47), 
  rep(48:55), 
  rep(56:63) 
  ) |&gt; 
  lapply(as.integer) |&gt; 
  lapply(as.array)

# number of animals per category
animal_n_per_cat &lt;- rep(8, each = 8) |&gt; as.integer()

# matrix of the category per animal
animal_matrix &lt;-
  tibble(
  genre     = rep(animal_categories, each = 8), 
  species   = sort(animal_names),
  belonging = rep(1, 64)
  ) |&gt; 
  pivot_wider(
    names_from  = genre,
    values_from = belonging
  ) |&gt; 
  mutate(across(everything(), ~replace_na(.x, 0))) |&gt; 
  as.data.frame() |&gt; 
  select(!species)

row.names(animal_matrix) &lt;- animal_names |&gt; sort()

# These structures will be loaded right away when we'll transition to Python.</code>
      </sec>
    </sec>
  </sec>
  <sec id="data-analysis-aligning-representational-structures-nb-article">
    <title>3.3 Data analysis: Aligning representational
    structures</title>
    <p>For all this section, we need to adapt a simple version of the
    explanations from Kawakita et al.
    (<xref alt="2023" rid="ref-kawakita2023-nb-article" ref-type="bibr">2023</xref>)
    and Sasaki et al.
    (<xref alt="2023" rid="ref-sasakiToolboxGromovWassersteinOptimal2023-nb-article" ref-type="bibr">2023</xref>)
    and avoid any technical aspects in the main manuscript.</p>
    <p>From there on, most of the code follows the instructions from the
    open-source scientific toolbox by Sasaki et al.
    (<xref alt="2023" rid="ref-sasakiToolboxGromovWassersteinOptimal2023-nb-article" ref-type="bibr">2023</xref>).
    I added a few explanations on the purpose of each step, without
    diving into unnecessary details.</p>
    <sec id="step-1-importing-the-embeddings-in-the-python-instances-nb-article">
      <title>Step 1: Importing the embeddings in the Python
      instances</title>
    </sec>
    <sec id="step-2-setting-the-parameters-for-the-optimization-of-gwot-nb-article">
      <title>Step 2: Setting the parameters for the optimization of
      GWOT</title>
    </sec>
    <sec id="step-3-gromov-wasserstein-optimal-transport-gwot-between-representations-nb-article">
      <title>Step 3: Gromov-Wasserstein Optimal Transport (GWOT) between
      Representations</title>
    </sec>
    <sec id="step-4-evaluation-and-visualization-nb-article">
      <title>Step 4: Evaluation and Visualization</title>
      <sec id="clustering-the-subjects-by-alignment-accuracy-nb-article">
        <title>Clustering the subjects by alignment accuracy</title>
        <p>First, we evaluate the accuracy per subject and group the
        subjects based on the alignment accuracy via hierarchical
        clustering. This procedure is represented in
        <xref alt="Figure 3.8" rid="fig-hclust-nb-article">Figure 3.8</xref>.
        Second, we evaluate the accuracy of the alignment between these
        clusters.</p>
        <sec id="nb-code-cell-8-nb-article" specific-use="notebook-code">
        <code language="r script">group_aph_s  &lt;- paste0(&quot;Subject_&quot;, seq(1, 7, 1))
group_aph_v   &lt;- paste0(&quot;Subject_&quot;, seq(8, 15, 1))
groups_aph    &lt;- paste0(&quot;Subject_&quot;, seq(1, 15, 1))

group_phant_s  &lt;- paste0(&quot;Subject_&quot;, seq(16, 22, 1))
group_phant_v   &lt;- paste0(&quot;Subject_&quot;, seq(23, 30, 1))
groups_phant    &lt;- paste0(&quot;Subject_&quot;, seq(16, 30, 1))

summarise_accuracy &lt;- function(df, name){
  df &lt;- 
    df |&gt; 
    mutate(k = c(1, 5, 10)) |&gt; 
    pivot_longer(
      !k,
      names_to  = &quot;comparisons&quot;,
      values_to = &quot;accuracy&quot; 
    ) |&gt; 
    pivot_wider(
      names_from = &quot;k&quot;,
      values_from = &quot;accuracy&quot;
    ) |&gt; 
    rename_with(.fn = ~paste0(&quot;k_&quot;, name, &quot;_&quot;, .x), .cols = c(2, 3, 4)
    ) |&gt; 
    separate_wider_delim(
      comparisons, 
      delim = &quot;_vs_&quot;,
      names = c(&quot;group_a&quot;, &quot;group_b&quot;)
      )
  
  return(df)
}

df_accuracy_all &lt;- 
  summarise_accuracy(df_accuracy_all_bad, &quot;all&quot;) |&gt;
  mutate(mean_acc_all = (k_all_1 + k_all_5 + k_all_10)/3)

df_accuracy_cat &lt;- 
  summarise_accuracy(df_accuracy_cat_bad, &quot;cat&quot;) |&gt;
  mutate(mean_acc_cat = (k_cat_1 + k_cat_5 + k_cat_10)/3) 

df_accuracy &lt;-
  left_join(
    df_accuracy_all, 
    df_accuracy_cat, 
    by = c(&quot;group_a&quot;, &quot;group_b&quot;)
    ) |&gt; 
  mutate(mean_acc = (mean_acc_all + mean_acc_cat) /2) |&gt; 
  select(group_a, group_b, mean_acc)

dist_matrix &lt;-
  structure(
    100 - df_accuracy$mean_acc, 
    Size = 30, 
    Labels = 1:30, 
    method = &quot;user&quot;, 
    class = &quot;dist&quot;
    )

clustering &lt;- hclust(dist_matrix)</code>
        </sec>
        <sec id="nb-code-cell-9-nb-article" specific-use="notebook-code">
        <code language="r script">clusters &lt;- clustering |&gt; cutree(k = 9)

# 9 Cluster solution
cluster_1 &lt;- paste0(&quot;Subject_&quot;, c(1,14))
cluster_2 &lt;- paste0(&quot;Subject_&quot;, c(3,6,7))
cluster_3 &lt;- paste0(&quot;Subject_&quot;, c(5,15))
cluster_4 &lt;- paste0(&quot;Subject_&quot;, c(8,9,10,11))
cluster_5 &lt;- paste0(&quot;Subject_&quot;, c(12,13))
cluster_6 &lt;- paste0(&quot;Subject_&quot;, c(16,17,23,25,30))
cluster_7 &lt;- paste0(&quot;Subject_&quot;, c(18,19,26,28))
cluster_8 &lt;- paste0(&quot;Subject_&quot;, c(20,21,22,24,27,29))
cluster_9 &lt;- paste0(&quot;Subject_&quot;, c(2,4))

df_9_cluster_accuracy &lt;-
  df_accuracy |&gt; 
  mutate(
    group_a = case_when(
      group_a %in% cluster_1 ~ &quot;Cluster 1&quot;,
      group_a %in% cluster_2 ~ &quot;Cluster 2&quot;,
      group_a %in% cluster_3 ~ &quot;Cluster 3&quot;,
      group_a %in% cluster_4 ~ &quot;Cluster 4&quot;,
      group_a %in% cluster_5 ~ &quot;Cluster 5&quot;,
      group_a %in% cluster_6 ~ &quot;Cluster 6&quot;,
      group_a %in% cluster_7 ~ &quot;Cluster 7&quot;,
      group_a %in% cluster_8 ~ &quot;Cluster 8&quot;,
      group_a %in% cluster_9 ~ &quot;Cluster 9&quot;
    ),
    group_b = case_when(
      group_b %in% cluster_1 ~ &quot;Cluster 1&quot;,
      group_b %in% cluster_2 ~ &quot;Cluster 2&quot;,
      group_b %in% cluster_3 ~ &quot;Cluster 3&quot;,
      group_b %in% cluster_4 ~ &quot;Cluster 4&quot;,
      group_b %in% cluster_5 ~ &quot;Cluster 5&quot;,
      group_b %in% cluster_6 ~ &quot;Cluster 6&quot;,
      group_b %in% cluster_7 ~ &quot;Cluster 7&quot;,
      group_b %in% cluster_8 ~ &quot;Cluster 8&quot;,
      group_b %in% cluster_9 ~ &quot;Cluster 9&quot;
    )
  ) |&gt; 
  rowwise() |&gt;
  mutate(alignment = paste0(sort(c(group_a,group_b))[1], &quot; - &quot;, sort(c(group_a,group_b))[2])) |&gt;
  group_by(alignment) |&gt;
  summarise(accuracy = mean(mean_acc)) |&gt; 
  separate_wider_delim(alignment, &quot; - &quot;, names = c(&quot;cluster_a&quot;, &quot;cluster_b&quot;))</code>
        </sec>
        <sec id="cell-fig-hclust-nb-article" specific-use="notebook-code">
        <code language="r script">dendro &lt;-
  clustering |&gt;
  as.dendrogram() |&gt; 
  dendro_data(type = &quot;rectangle&quot;)

df_plot_cluster &lt;- 
  data.frame(
    label = names(clusters), 
    cluster = factor(clusters)
    )

dendro[[&quot;labels&quot;]] &lt;- merge(dendro[[&quot;labels&quot;]], df_plot_cluster, by = &quot;label&quot;)

ggplot() +
  geom_segment(
    data = dendro$segments,
    aes(x = x, y = y, xend = xend, yend = yend),
    color = &quot;white&quot;
    ) +
  geom_hline(yintercept = 75, color = &quot;grey&quot;, linetype = 2, linewidth = .5) +
  geom_text(
    data = dendro$labels,
    aes(x = x, y = y, label = label, color = cluster),
    size = 5,
    nudge_y = -7
  ) +
  labs(y = &quot;Height&quot;, x = &quot;Subject&quot;) +
  scale_color_manual(
    values = c(&quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;, &quot;#F5C710&quot;, 
               &quot;#7AD151FF&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;, &quot;#6c0009&quot;, &quot;#414487FF&quot;), 
    name = &quot;Cluster&quot;
  ) +
  theme_blackboard() +
  theme(
    axis.line.y = element_blank(),
    axis.line.x = element_blank(),
    axis.text.x = element_blank()
  )</code>
        <fig id="fig-hclust-nb-article">
          <caption><p>Figure 3.8: Hierachical clustering of the 30
          subjects based on their representational
          alignment.</p></caption>
          <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-hclust-1.png" />
        </fig>
        </sec>
        <sec id="cell-fig-clusters-accuracy-nb-article" specific-use="notebook-code">
        <code language="r script">df_9_cluster_accuracy |&gt; 
  ggplot(aes(x = cluster_a, y = cluster_b, fill = accuracy)) +
  geom_tile(color = &quot;white&quot;, linewidth = .5) +
  geom_text(aes(label = round(accuracy/100, digits  =2)), color = &quot;black&quot;) +
  scale_fill_viridis(guide = &quot;none&quot;) +
  theme(
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    axis.line  = element_blank()
  )</code>
        <fig id="fig-clusters-accuracy-nb-article">
          <caption><p>Figure 3.9: Accuracy of the alignments between the
          subject’s embeddings in each cluster. An alignment of a
          cluster with itself (e.g. Cluster 7 - Cluster 7) is the
          evaluation of the alignment of the subjects
          <bold><italic>inside</italic></bold> the
          cluster.</p></caption>
          <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-clusters-accuracy-1.png" />
        </fig>
        </sec>
      </sec>
      <sec id="evaluating-the-clusters-in-light-of-our-theoretical-osv-model-nb-article">
        <title>Evaluating the clusters in light of our theoretical OSV
        model</title>
        <p>Let’s see the composition of the clusters in light of our
        initial O-S-V model. The cognitive profiles of the subjects are
        represented in
        <xref alt="Figure 3.10" rid="fig-plot-osv-clusters-nb-article">Figure 3.10</xref>,
        and the distribution of the cognitive profiles in the clusters
        is represented in
        <xref alt="Figure 3.11" rid="fig-clusters-distribution-nb-article">Figure 3.11</xref>.</p>
        <sec id="nb-code-cell-10-nb-article" specific-use="notebook-code">
        <code language="r script">df_coordinates &lt;- coordinates_aligned_embeddings |&gt; lapply(as.data.frame)

df_aligned_clustered &lt;-
  tibble(
    embedding = list(df_coordinates)
  ) |&gt; 
  unnest(embedding) |&gt; 
  unnest(embedding) |&gt;
  mutate(
    species = rep(animal_names, 30),
    species_group = paste0(&quot;species_group_&quot;, rep(rep(1:8, each = 8), 30)),
    subject = rep(pull(df_embeddings_sub, subject), each = 64),
    group   = rep(pull(df, group), each = 64),
    vis_spa_group = rep(pull(df, vis_spa_group), each = 64)
  ) |&gt; 
  rename(x = V1, y = V2, z = V3) |&gt; 
  select(
    subject,
    group,
    vis_spa_group,
    species,
    species_group,
    x, y, z) |&gt; 
  mutate(clusters = as.factor(rep(clusters, each = 64))) |&gt; 
  select(subject, group, clusters, everything())

# analyzing subject assignations
df_aligned_subjects &lt;-
  df_aligned_clustered |&gt; 
  select(subject:vis_spa_group) |&gt; 
  group_by(subject) |&gt; 
  unique()

df_clustered &lt;- bind_cols(df_aligned_subjects, df[,c(6:8)])</code>
        </sec>
        <sec id="cell-fig-plot-osv-clusters-nb-article" specific-use="notebook-code">
        <code language="r script">df_clustered |&gt; 
  plot_ly(
    x = ~visual_imagery,
    y = ~spatial_imagery,
    z = ~verbal_profile,
    color = ~paste0(&quot;Cluster &quot;, clusters),
    text = ~paste0(&quot;Cluster &quot;, clusters),
    colors = c(
      &quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;, &quot;#F5C710&quot;, 
      &quot;#7AD151FF&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;, &quot;#6c0009&quot;, &quot;#414487FF&quot;),
    type = &quot;scatter3d&quot;,
    mode = &quot;markers+text&quot;,
    marker = list(size = 6),
    textfont = list(size = 11)
  ) |&gt; 
  layout(
    scene = list(
      xaxis = list(
        title = list(text = &quot;Visual imagery&quot;, font = list(color = &quot;grey&quot;)),
        tickfont = list(color = &quot;grey&quot;)
        ),
      yaxis = list(
        title = list(text = &quot;Spatial imagery&quot;, font = list(color = &quot;grey&quot;)),
        tickfont = list(color = &quot;grey&quot;)
        ),
      zaxis = list(
        title = list(text = &quot;Verbal profile&quot;, font = list(color = &quot;grey&quot;)),
        tickfont = list(color = &quot;grey&quot;)
        )
    ),
    # legend = list(title = list(text = &quot;Group&quot;)),
    paper_bgcolor = &quot;transparent&quot;,
    showlegend = FALSE
    )</code>
        <fig id="fig-plot-osv-clusters-nb-article">
          <caption><p>Figure 3.10: Imagery profiles of the nine
          identified clusters on the three object, spatial, and verbal
          dimensions. <bold><italic>Interact with the figure to see the
          details.</italic></bold></p></caption>
          <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-plot-osv-clusters-1.png" />
        </fig>
        </sec>
        <sec id="cell-fig-clusters-distribution-nb-article" specific-use="notebook-code">
        <code language="r script">names = c(&quot;Aph. spatial&quot;, &quot;Aph. verbal&quot;, &quot;Phant. spatial&quot;, &quot;Phant. visual&quot;)

df_clustered |&gt; 
  ggplot(aes(x = clusters, fill = vis_spa_group, color = vis_spa_group)) +
  geom_bar(alpha = .5, position = position_dodge()) +
  labs(x = &quot;Cluster&quot;, y = &quot;Count&quot;) +
  scale_color_manual(values = pal_okabe_ito, labels = names, name = &quot;Group&quot;) +
  scale_fill_manual(values = pal_okabe_ito, labels = names, name = &quot;Group&quot;) +
  theme_blackboard()</code>
        <fig id="fig-clusters-distribution-nb-article">
          <caption><p>Figure 3.11: Repartion of our intial O-S-V groups
          in the clusters created by the unsupervised
          alignment.</p></caption>
          <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-clusters-distribution-1.png" />
        </fig>
        </sec>
        <p>Now let’s visualize the embeddings of the subjects in each
        cluster to get a visual idea of their representational
        structures and the intra-cluster alignment between the
        subjects.</p>
        <code language="r script">size = 4

# Cluster 1
plot_ly(
  type = &quot;scatter3d&quot;, 
  mode = &quot;marker+text&quot;, 
  colors = c(&quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;, &quot;#F5C710&quot;, 
             &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;, &quot;#6c0009&quot;)) |&gt; 
  add_markers(
    data = df_aligned_clustered |&gt; filter(clusters == &quot;1&quot;),
    x = ~x, y = ~y, z = ~z,
    color = ~paste0(&quot;Species group &quot;, species_group),
    marker = list(size = size, symbol = &quot;circle&quot;)
  ) |&gt; 
  layout(
    paper_bgcolor = &quot;transparent&quot;,
    showlegend = FALSE
    )
# Cluster 2
plot_ly(
  type = &quot;scatter3d&quot;, 
  mode = &quot;marker+text&quot;, 
  colors = c(&quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;, &quot;#F5C710&quot;, 
             &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;, &quot;#6c0009&quot;)) |&gt; 
  add_markers(
    data = df_aligned_clustered |&gt; filter(clusters == &quot;2&quot;),
    x = ~x, y = ~y, z = ~z,
    color = ~paste0(&quot;Species group &quot;, species_group),
    marker = list(size = size, symbol = &quot;circle&quot;)
  ) |&gt; 
  layout(
    paper_bgcolor = &quot;transparent&quot;,
    showlegend = FALSE
    )
# Cluster 3
plot_ly(
  type = &quot;scatter3d&quot;, 
  mode = &quot;marker+text&quot;, 
  colors = c(&quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;, &quot;#F5C710&quot;, 
             &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;, &quot;#6c0009&quot;)) |&gt; 
  add_markers(
    data = df_aligned_clustered |&gt; filter(clusters == &quot;3&quot;),
    x = ~x, y = ~y, z = ~z,
    color = ~paste0(&quot;Species group &quot;, species_group),
    marker = list(size = size, symbol = &quot;circle&quot;)
  ) |&gt; 
  layout(
    paper_bgcolor = &quot;transparent&quot;,
    showlegend = FALSE
    )
# Cluster 4
plot_ly(
  type = &quot;scatter3d&quot;, 
  mode = &quot;marker+text&quot;, 
  colors = c(&quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;, &quot;#F5C710&quot;, 
             &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;, &quot;#6c0009&quot;)) |&gt; 
  add_markers(
    data = df_aligned_clustered |&gt; filter(clusters == &quot;4&quot;),
    x = ~x, y = ~y, z = ~z,
    color = ~species_group,
    marker = list(size = size, symbol = &quot;circle&quot;)
  ) |&gt; 
  layout(
    paper_bgcolor = &quot;transparent&quot;,
    showlegend = FALSE
    )
# Cluster 5
plot_ly(
  type = &quot;scatter3d&quot;, 
  mode = &quot;marker+text&quot;, 
  colors = c(&quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;, &quot;#F5C710&quot;, 
             &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;, &quot;#6c0009&quot;)) |&gt; 
  add_markers(
    data = df_aligned_clustered |&gt; filter(clusters == &quot;5&quot;),
    x = ~x, y = ~y, z = ~z,
    color = ~species_group,
    marker = list(size = size, symbol = &quot;circle&quot;)
  ) |&gt; 
  layout(
    paper_bgcolor = &quot;transparent&quot;,
    showlegend = FALSE
    )
# Cluster 6
plot_ly(
  type = &quot;scatter3d&quot;, 
  mode = &quot;marker+text&quot;, 
  colors = c(&quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;, &quot;#F5C710&quot;, 
             &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;, &quot;#6c0009&quot;)) |&gt; 
  add_markers(
    data = df_aligned_clustered |&gt; filter(clusters == &quot;6&quot;),
    x = ~x, y = ~y, z = ~z,
    color = ~species_group,
    marker = list(size = size, symbol = &quot;circle&quot;)
  ) |&gt; 
  layout(
    paper_bgcolor = &quot;transparent&quot;,
    showlegend = FALSE
    )
# Cluster 7
plot_ly(
  type = &quot;scatter3d&quot;, 
  mode = &quot;marker+text&quot;, 
  colors = c(&quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;, &quot;#F5C710&quot;, 
             &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;, &quot;#6c0009&quot;)) |&gt; 
  add_markers(
    data = df_aligned_clustered |&gt; filter(clusters == &quot;7&quot;),
    x = ~x, y = ~y, z = ~z,
    color = ~species_group,
    marker = list(size = size, symbol = &quot;circle&quot;)
  ) |&gt; 
  layout(
    paper_bgcolor = &quot;transparent&quot;,
    showlegend = FALSE
    )
# Cluster 8
plot_ly(
  type = &quot;scatter3d&quot;, 
  mode = &quot;marker+text&quot;, 
  colors = c(&quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;, &quot;#F5C710&quot;, 
             &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;, &quot;#6c0009&quot;)) |&gt; 
  add_markers(
    data = df_aligned_clustered |&gt; filter(clusters == &quot;8&quot;),
    x = ~x, y = ~y, z = ~z,
    color = ~species_group,
    marker = list(size = size, symbol = &quot;circle&quot;)
  ) |&gt; 
  layout(
    paper_bgcolor = &quot;transparent&quot;,
    showlegend = FALSE
    )
# Cluster 9
plot_ly(
  type = &quot;scatter3d&quot;, 
  mode = &quot;marker+text&quot;, 
  colors = c(&quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;, &quot;#F5C710&quot;, 
             &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;, &quot;#6c0009&quot;)) |&gt; 
  add_markers(
    data = df_aligned_clustered |&gt; filter(clusters == &quot;9&quot;),
    x = ~x, y = ~y, z = ~z,
    color = ~species_group,
    marker = list(size = size, symbol = &quot;circle&quot;)
  ) |&gt; 
  layout(
    paper_bgcolor = &quot;transparent&quot;,
    showlegend = FALSE
    )</code>
        <fig id="fig-clusters_embeddings_final-nb-article">
          <caption><p>Figure 3.12: Psychological spaces (embeddings) of
          the 30 subjects, aligned and clustered with other subjects
          having the most similar representations. The eight colors
          represent the initial eight groups of species that each
          subject had to ‘represent’ with imagery (legends for these
          colors have been taken out for display clarity purposes).
          <bold><italic>Interact with the figures to see the
          details.</italic></bold></p></caption>
          <table-wrap>
            <table>
              <colgroup>
                <col width="33%" />
                <col width="33%" />
                <col width="33%" />
              </colgroup>
              <tbody>
                <tr>
                  <td align="left"><p specific-use="wrapper">
                    <boxed-text>
                      <fig id="fig-clusters_embeddings_final-1-nb-article">
                        <caption><p>(a) <bold>Cluster 1</bold>
                        embeddings (Aphantasic cluster).Within-cluster
                        aligment accuracy = 95.83%.</p></caption>
                        <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-clusters_embeddings_final-1.png" />
                      </fig>
                    </boxed-text>
                  </p></td>
                  <td align="left"><p specific-use="wrapper">
                    <boxed-text>
                      <fig id="fig-clusters_embeddings_final-2-nb-article">
                        <caption><p>(b) <bold>Cluster 2</bold>
                        embeddings (Spatial aphantasic
                        cluster).Within-cluster aligment accuracy =
                        97.66%.</p></caption>
                        <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-clusters_embeddings_final-2.png" />
                      </fig>
                    </boxed-text>
                  </p></td>
                  <td align="left"><p specific-use="wrapper">
                    <boxed-text>
                      <fig id="fig-clusters_embeddings_final-3-nb-article">
                        <caption><p>(c) <bold>Cluster 3</bold>
                        embeddings (Spatial aphantasic
                        cluster).Within-cluster aligment accuracy =
                        94.01%.</p></caption>
                        <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-clusters_embeddings_final-3.png" />
                      </fig>
                    </boxed-text>
                  </p></td>
                </tr>
              </tbody>
            </table>
          </table-wrap>
          <table-wrap>
            <table>
              <colgroup>
                <col width="33%" />
                <col width="33%" />
                <col width="33%" />
              </colgroup>
              <tbody>
                <tr>
                  <td align="left"><p specific-use="wrapper">
                    <boxed-text>
                      <fig id="fig-clusters_embeddings_final-4-nb-article">
                        <caption><p>(d) <bold>Cluster 4</bold>
                        embeddings (Aphantasic cluster).Within-cluster
                        aligment accuracy = 85.03%.</p></caption>
                        <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-clusters_embeddings_final-4.png" />
                      </fig>
                    </boxed-text>
                  </p></td>
                  <td align="left"><p specific-use="wrapper">
                    <boxed-text>
                      <fig id="fig-clusters_embeddings_final-5-nb-article">
                        <caption><p>(e) <bold>Cluster 5</bold>
                        embeddings (Verbal aphantasic
                        cluster).Within-cluster aligment accuracy =
                        96.88%.</p></caption>
                        <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-clusters_embeddings_final-5.png" />
                      </fig>
                    </boxed-text>
                  </p></td>
                  <td align="left"><p specific-use="wrapper">
                    <boxed-text>
                      <fig id="fig-clusters_embeddings_final-6-nb-article">
                        <caption><p>(f) <bold>Cluster 6</bold>
                        embeddings (Verbal aphantasic
                        cluster).Within-cluster aligment accuracy =
                        81.80%.</p></caption>
                        <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-clusters_embeddings_final-6.png" />
                      </fig>
                    </boxed-text>
                  </p></td>
                </tr>
              </tbody>
            </table>
          </table-wrap>
          <table-wrap>
            <table>
              <colgroup>
                <col width="33%" />
                <col width="33%" />
                <col width="33%" />
              </colgroup>
              <tbody>
                <tr>
                  <td align="left"><p specific-use="wrapper">
                    <boxed-text>
                      <fig id="fig-clusters_embeddings_final-7-nb-article">
                        <caption><p>(g) <bold>Cluster 7</bold>
                        embeddings (Phantasic cluster).Within-cluster
                        aligment accuracy = 98.23%.</p></caption>
                        <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-clusters_embeddings_final-7.png" />
                      </fig>
                    </boxed-text>
                  </p></td>
                  <td align="left"><p specific-use="wrapper">
                    <boxed-text>
                      <fig id="fig-clusters_embeddings_final-8-nb-article">
                        <caption><p>(h) <bold>Cluster 8</bold>
                        embeddings (Phantasic cluster).Within-cluster
                        aligment accuracy = 74.64%.</p></caption>
                        <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-clusters_embeddings_final-8.png" />
                      </fig>
                    </boxed-text>
                  </p></td>
                  <td align="left"><p specific-use="wrapper">
                    <boxed-text>
                      <fig id="fig-clusters_embeddings_final-9-nb-article">
                        <caption><p>(i) <bold>Cluster 9</bold>
                        embeddings (Phantasic cluster).Within-cluster
                        aligment accuracy = 42.45%.</p></caption>
                        <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-clusters_embeddings_final-9.png" />
                      </fig>
                    </boxed-text>
                  </p></td>
                </tr>
              </tbody>
            </table>
          </table-wrap>
        </fig>
      </sec>
    </sec>
  </sec>
  <sec id="summary-of-the-simulation-analysis-nb-article">
    <title>3.4 Summary of the simulation analysis</title>
    <list list-type="bullet">
      <list-item>
        <p>We generated subject data on subjective imagery based on the
        Object-Spatial-Verbal model of cognitive styles and
        representations</p>
      </list-item>
      <list-item>
        <p>We translated this model into a model of the distances
        between participants’ mental representations</p>
      </list-item>
      <list-item>
        <p>We generated random data based on this theoretical model, on
        the representations of 64 items with categorical and visual
        properties by 30 participants</p>
      </list-item>
      <list-item>
        <p>We used an unsupervised alignment algorithm to judge the
        similarity between the representations of the subjects without
        any knowledge of their initial groups and relations</p>
      </list-item>
      <list-item>
        <p>The algorithm aligned with high precision 9 clusters of
        participants, which were coherent with the initial model we
        created, with several differences and unexpected alignments due
        to the randomization.</p>
      </list-item>
      <list-item>
        <p>The 9 clusters revealed a distinction between verbal
        aphantasics, spatial aphantasics, and phantasics in general.
        This interesting result shows that even though we tried to model
        spatial aphants and phantasics closer to each other, they all
        ended up separated based on visual imagery. This unexpected
        outcome, that went besides our initial intentions, shows that
        such an unsupervised method could reveal coherent patterns of
        representations that we did not expect, even with a relevant
        psychometric model.</p>
      </list-item>
    </list>
    <p>This simulation motivates the idea that, should the imagery of
    participants be accurately fitted by our OSV model (or any other
    model to be tested), this paradigm and analytic method would be able
    to align the representations of participants with the same
    subjective imagery abilities.</p>
    <p>I insist on a key finding : I did not use the data of the OSV
    model presented in
    <xref alt="Section 3.1" rid="sec-osv-model-theory-nb-article">Section 3.1</xref>
    to generate the subject embeddings. The only hypothesis that guided
    how I simulated the subjects’ embeddings was the distance model I
    envisioned, which is represented in
    <xref alt="Figure 3.5" rid="fig-diagram-intermediate-nb-article">Figure 3.5</xref>.
    Thus, <bold>the algorithm managed to reverse-engineer my
    logic</bold>, to find the subjects groups I simulated with this
    logic, and it so happens that these groups’ matched the cognitive
    profiles groups I built in the beginning.</p>
    <p>In other words, the algorithm managed to find the common pattern
    - which was the groups pattern - between two models built
    differently, <bold><italic>a common pattern that existed originally
    only in my head.</italic></bold></p>
    <p>I think this “mind-reading”<xref ref-type="fn" rid="fn2-nb-article">2</xref>
    further argues for the potential of this method to reveal hidden
    patterns of inter-individual differences in subjective experience.
    These patterns could help build models of subjective mental imagery,
    one of the most challenging tasks in cognitive psychology to
    date.</p>
  </sec>
</sec>
<sec id="feasibility-nb-article">
  <title>4. Feasibility</title>
  <sec id="stimuli-and-study-design-nb-article">
    <title>4.1 Stimuli and study design</title>
    <p>The simulation study presented here focused on aligning the
    representations <bold><italic>between</italic></bold> various
    participants, but a real study should go further and also analyse
    the similarities of representations
    <bold><italic>within</italic></bold> participants, for instance with
    a perception and an imagery condition. This was the basis of the
    study of Shepard &amp; Chipman
    (<xref alt="1970" rid="ref-shepardSecondorderIsomorphismInternal1970-nb-article" ref-type="bibr">1970</xref>),
    and remains a good starting point to design our own.</p>
  </sec>
  <sec id="sec-online-study-materials-nb-article">
    <title>4.2 Online study materials</title>
  </sec>
  <sec id="analytic-methods-and-collaborations-nb-article">
    <title>4.3 Analytic methods and collaborations</title>
    <p>I have proved (<italic>mostly to myself</italic>) that I was
    capable of implementing unsupervised GWOT alignment analysis in
    Python and R using the open-source toolbox provided by Sasaki et al.
    (<xref alt="2023" rid="ref-sasakiToolboxGromovWassersteinOptimal2023-nb-article" ref-type="bibr">2023</xref>),
    firstly to demonstrate that this key feasibility aspect was not out
    of reach (and that I was prepared to handle it). This toolbox is
    very recent (the associated article was posted on
    bioR<inline-formula><alternatives>
    <tex-math><![CDATA[\chi]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>χ</mml:mi></mml:math></alternatives></inline-formula>iv
    last September) but is based on long-standing theory on similarity
    and cutting-edge topology analysis research. Consequently, I may not
    be confident enough on my expertise in these fields to state with
    confidence that my analyses of this project’s data would be solid,
    which is a very important aspect for me. I need to be convinced by
    analytical choices, which are often taken for granted, to believe in
    their results and implications. I’m convinced of the relevance of
    what I’ve done here, but in the context a real application, even
    more expertise (and other analytical points of view) would be most
    welcome.</p>
    <p>Therefore, I think that this project could be the opportunity to
    collaborate with several teams working in these fields that have
    great data analysis expertise.</p>
    <list list-type="bullet">
      <list-item>
        <p>Starting of course with
        <ext-link ext-link-type="uri" xlink:href="https://lnalborczyk.github.io/"><bold>Ladislas
        Nalborczyk</bold></ext-link> (who gave me this idea), who works
        on synesthesia and inner speech aphantasia at the <italic>Paris
        Brain Institute</italic> with <bold>Laurent Cohen</bold> and
        <bold>Stanislas Dehaene</bold>.</p>
      </list-item>
      <list-item>
        <p><ext-link ext-link-type="uri" xlink:href="https://zuckermaninstitute.columbia.edu/nikolaus-kriegeskorte-phd"><bold>Nikolaus
        Kriegeskorte</bold></ext-link>, one of the creators of the
        famous Representational Similarity Analysis (RSA, another
        <italic>supervised</italic> alignment method, see
        <xref alt="Kriegeskorte et al., 2008" rid="ref-kriegeskorte2008-nb-article" ref-type="bibr">Kriegeskorte
        et al., 2008</xref>) and his colleagues could be precious
        collaborators for alignment analyses and study materials.</p>
      </list-item>
      <list-item>
        <p>The Japanese team of
        <ext-link ext-link-type="uri" xlink:href="https://sites.google.com/a/g.ecc.u-tokyo.ac.jp/oizumi-lab/home/member/masafumi_oizumi?pli=1"><bold>Masafumi
        Oizumi</bold></ext-link> behind the GWOT toolbox is of course
        also very knowledgeable on the subject, the method, and its
        technical implementation.</p>
      </list-item>
      <list-item>
        <p>They are collaborating with the Australia-based team of
        <ext-link ext-link-type="uri" xlink:href="https://research.monash.edu/en/persons/nao-tsuchiya"><bold>Naotsugu
        Tsuchiya</bold></ext-link>, with whom they recently published
        several very interesting articles on similarity as a concept and
        method for perception research
        (<xref alt="Kawakita et al., n.d." rid="ref-kawakitaComparingColorSimilarity2023-nb-article" ref-type="bibr">Kawakita
        et al., n.d.</xref>,
        <xref alt="2023" rid="ref-kawakita2023-nb-article" ref-type="bibr">2023</xref>;
        e.g.
        <xref alt="Tsuchiya et al., 2022" rid="ref-tsuchiyaEnrichedCategoryModel2022-nb-article" ref-type="bibr">Tsuchiya
        et al., 2022</xref>;
        <xref alt="Zeleznikow-Johnston et al., 2023" rid="ref-zeleznikow-johnstonAreColorExperiences2023-nb-article" ref-type="bibr">Zeleznikow-Johnston
        et al., 2023</xref>).</p>
      </list-item>
      <list-item>
        <p>Visiting
        <ext-link ext-link-type="uri" xlink:href="https://research.monash.edu/en/persons/nao-tsuchiya">Tsuchiya’s
        webpage</ext-link>, I also found an amazing chain of connections
        that lead us to his team. Tsuchiya also works on sleep and
        dreams and has collaborated several times with
        <ext-link ext-link-type="uri" xlink:href="https://www.movit.paris/team-pi/thomas-andrillon"><bold>Thomas
        Andrillon</bold></ext-link>, who works at the <italic>Paris
        Brain Institute</italic>, thus close to Dehaene, Cohen,
        Bartolomeo, and Nalborczyk, and is very technically
        knowledgeable. Even more interestingly, Andrillon is a co-author
        of
        <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41598-020-65705-7">one
        of Alexei Dawes’ most famous papers on aphantasia</ext-link>,
        probably because they surveyed aphantasics about dreams (looking
        at the author contributions, he apparently took part in the
        study concept, data analysis, and critical revisions). Further,
        Tsuchiya and Andrillon are co-directors of
        <ext-link ext-link-type="uri" xlink:href="https://www.movit.paris/team-members/decat-nicolas"><bold>Nicolas
        Decat</bold></ext-link>, whom I met at the <italic>Immersion and
        Synesthesia Conference</italic> where he gave another talk. So
        Tsuchiya and Andrillon might even have indirectly heard of our
        work! (Provided that my talk was noticeable enough for Nicolas -
        or anyone else - to tell them about it…)</p>
      </list-item>
    </list>
  </sec>
</sec>
<sec id="conclusion-nb-article">
  <title>Conclusion</title>
  <p>Using the unsupervised alignment method that we exposed and tested
  in this report, Kawakita have shown that relational properties of
  color representations were universally shared by color-neurotypical
  individuals, but structurally different from color-atypical
  individuals. Yet intriguingly, their results also support the
  hypothesis that color-atypical individuals have a different structure
  of their color representations, rather than simply failing to
  experience certain colors. This observation on color-atypical
  individuals, which emerges primarily from the novel consideration of
  color representation in a psychological space, foreshadows of the
  potential of this technique to demystify aphantasia.Such perspectives
  open up unexpected avenues of research to address the impossibility of
  comparing subjective experiences using psychophysical science.</p>
  <p>I tried to show in this project report that this type of (very
  simple) paradigm focusing on similarities between participants’
  subjective representations, combined with a state-of-the-art
  unsupervised alignment method that I was able to implement using an
  open-source Python scientific toolbox, can be extremely promising for
  objectifying the difference (or lack of difference) between people’s
  representational formats. This objectification is intrinsically tied
  to the idea of a link between similarities and representations, but we
  have good evidence to support this hypothesis. So, provided we create
  a good study design, this project would enable us to make robust
  inferences about the contents of analog representations (visual,
  spatial, auditory-verbal) of aphantasics and phantasics, independent
  of any subjective assumptions or relationships.</p>
</sec>
</body>



<back>
<ref-list>
  <title></title>
  <ref id="ref-roads2024-nb-article">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Roads</surname><given-names>Brett D.</given-names></name>
        <name><surname>Love</surname><given-names>Bradley C.</given-names></name>
      </person-group>
      <article-title>Modeling similarity and psychological space</article-title>
      <source>Annual Review of Psychology</source>
      <year iso-8601-date="2024-01">2024</year><month>01</month>
      <volume>75</volume>
      <issue>1</issue>
      <issn>0066-4308, 1545-2085</issn>
      <uri>https://www.annualreviews.org/doi/10.1146/annurev-psych-040323-115131</uri>
      <pub-id pub-id-type="doi">10.1146/annurev-psych-040323-115131</pub-id>
      <fpage>215</fpage>
      <lpage>240</lpage>
    </element-citation>
  </ref>
  <ref id="ref-shepardSecondorderIsomorphismInternal1970-nb-article">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Shepard</surname><given-names>Roger N</given-names></name>
        <name><surname>Chipman</surname><given-names>Susan</given-names></name>
      </person-group>
      <article-title>Second-order isomorphism of internal representations: Shapes of states</article-title>
      <source>Cognitive Psychology</source>
      <year iso-8601-date="1970-01">1970</year><month>01</month>
      <volume>1</volume>
      <issue>1</issue>
      <issn>0010-0285</issn>
      <uri>https://www.sciencedirect.com/science/article/pii/0010028570900022</uri>
      <pub-id pub-id-type="doi">10.1016/0010-0285(70)90002-2</pub-id>
      <fpage>1</fpage>
      <lpage>17</lpage>
    </element-citation>
  </ref>
  <ref id="ref-sasakiToolboxGromovWassersteinOptimal2023-nb-article">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Sasaki</surname><given-names>Masaru</given-names></name>
        <name><surname>Takeda</surname><given-names>Ken</given-names></name>
        <name><surname>Abe</surname><given-names>Kota</given-names></name>
        <name><surname>Oizumi</surname><given-names>Masafumi</given-names></name>
      </person-group>
      <article-title>Toolbox for gromov-wasserstein optimal transport: Application to unsupervised alignment in neuroscience</article-title>
      <year iso-8601-date="2023">2023</year>
      <uri>https://www.biorxiv.org/content/10.1101/2023.09.15.558038v1</uri>
      <pub-id pub-id-type="doi">10.1101/2023.09.15.558038</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-tsuchiyaEnrichedCategoryModel2022-nb-article">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Tsuchiya</surname><given-names>Naotsugu</given-names></name>
        <name><surname>Phillips</surname><given-names>Steven</given-names></name>
        <name><surname>Saigo</surname><given-names>Hayato</given-names></name>
      </person-group>
      <article-title>Enriched category as a model of qualia structure based on similarity judgements</article-title>
      <source>Consciousness and Cognition</source>
      <year iso-8601-date="2022-05-01">2022</year><month>05</month><day>01</day>
      <volume>101</volume>
      <uri>https://www.sciencedirect.com/science/article/pii/S1053810022000514</uri>
      <pub-id pub-id-type="doi">10.1016/j.concog.2022.103319</pub-id>
      <fpage>103319</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-kawakitaComparingColorSimilarity2023-nb-article">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kawakita</surname><given-names>Genji</given-names></name>
        <name><surname>Zeleznikow-Johnston</surname><given-names>Ariel</given-names></name>
        <name><surname>Tsuchiya</surname><given-names>Naotsugu</given-names></name>
        <name><surname>Oizumi</surname><given-names>Masafumi</given-names></name>
      </person-group>
      <article-title>Comparing color similarity structures between humans and LLMs via unsupervised alignment</article-title>
      <pub-id pub-id-type="doi">10.48550/arXiv.2308.04381</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-zeleznikow-johnstonAreColorExperiences2023-nb-article">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Zeleznikow-Johnston</surname><given-names>Ariel</given-names></name>
        <name><surname>Aizawa</surname><given-names>Yasunori</given-names></name>
        <name><surname>Yamada</surname><given-names>Makiko</given-names></name>
        <name><surname>Tsuchiya</surname><given-names>Naotsugu</given-names></name>
      </person-group>
      <article-title>Are color experiences the same across the visual field?</article-title>
      <source>Journal of Cognitive Neuroscience</source>
      <year iso-8601-date="2023-04-01">2023</year><month>04</month><day>01</day>
      <volume>35</volume>
      <issue>4</issue>
      <uri>https://doi.org/10.1162/jocn_a_01962</uri>
      <pub-id pub-id-type="doi">10.1162/jocn_a_01962</pub-id>
      <fpage>509</fpage>
      <lpage>542</lpage>
    </element-citation>
  </ref>
  <ref id="ref-bainbridgeQuantifyingAphantasiaDrawing2021-nb-article">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bainbridge</surname><given-names>Wilma A.</given-names></name>
        <name><surname>Pounder</surname><given-names>Zoë</given-names></name>
        <name><surname>Eardley</surname><given-names>Alison F.</given-names></name>
        <name><surname>Baker</surname><given-names>Chris I.</given-names></name>
      </person-group>
      <article-title>Quantifying aphantasia through drawing: Those without visual imagery show deficits in object but not spatial memory</article-title>
      <source>Cortex</source>
      <year iso-8601-date="2021-02-01">2021</year><month>02</month><day>01</day>
      <volume>135</volume>
      <uri>https://www.sciencedirect.com/science/article/pii/S0010945220304317</uri>
      <pub-id pub-id-type="doi">10.1016/j.cortex.2020.11.014</pub-id>
      <fpage>159</fpage>
      <lpage>172</lpage>
    </element-citation>
  </ref>
  <ref id="ref-kriegeskorteInverseMDSInferring2012-nb-article">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name>
        <name><surname>Mur</surname><given-names>Marieke</given-names></name>
      </person-group>
      <article-title>Inverse MDS: Inferring dissimilarity structure from multiple item arrangements</article-title>
      <source>Frontiers in Psychology</source>
      <year iso-8601-date="2012">2012</year>
      <volume>3</volume>
      <uri>https://www.frontiersin.org/articles/10.3389/fpsyg.2012.00245</uri>
    </element-citation>
  </ref>
  <ref id="ref-decockSimilarityGoodman2011-nb-article">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Decock</surname><given-names>Lieven</given-names></name>
        <name><surname>Douven</surname><given-names>Igor</given-names></name>
      </person-group>
      <article-title>Similarity After Goodman</article-title>
      <source>Review of Philosophy and Psychology</source>
      <year iso-8601-date="2011-03-01">2011</year><month>03</month><day>01</day>
      <volume>2</volume>
      <issue>1</issue>
      <uri>https://doi.org/10.1007/s13164-010-0035-y</uri>
      <pub-id pub-id-type="doi">10.1007/s13164-010-0035-y</pub-id>
      <fpage>61</fpage>
      <lpage>75</lpage>
    </element-citation>
  </ref>
  <ref id="ref-goodmanSevenStricturesSimilarity1972-nb-article">
    <element-citation publication-type="chapter">
      <person-group person-group-type="author">
        <name><surname>Goodman</surname><given-names>Nelson</given-names></name>
      </person-group>
      <article-title>Seven strictures on similarity</article-title>
      <publisher-name>Bobs-Merril</publisher-name>
      <year iso-8601-date="1972">1972</year>
    </element-citation>
  </ref>
  <ref id="ref-gardenforsConceptualSpacesFramework2004-nb-article">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Gardenfors</surname><given-names>Peter</given-names></name>
      </person-group>
      <article-title>Conceptual spaces as a framework for knowledge representation</article-title>
      <year iso-8601-date="2004">2004</year>
    </element-citation>
  </ref>
  <ref id="ref-murHumanObjectSimilarityJudgments2013-nb-article">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Mur</surname><given-names>Marieke</given-names></name>
        <name><surname>Meys</surname><given-names>Mirjam</given-names></name>
        <name><surname>Bodurka</surname><given-names>Jerzy</given-names></name>
        <name><surname>Goebel</surname><given-names>Rainer</given-names></name>
        <name><surname>Bandettini</surname><given-names>Peter</given-names></name>
        <name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name>
      </person-group>
      <article-title>Human object-similarity judgments reflect and transcend the primate-IT object representation</article-title>
      <source>Frontiers in Psychology</source>
      <year iso-8601-date="2013">2013</year>
      <volume>4</volume>
      <uri>https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00128</uri>
    </element-citation>
  </ref>
  <ref id="ref-majewskaSpatialMultiarrangementClustering2020-nb-article">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Majewska</surname><given-names>O.</given-names></name>
        <name><surname>McCarthy</surname><given-names>D.</given-names></name>
        <name><surname>Bosch</surname><given-names>J. van den</given-names></name>
        <name><surname>Kriegeskorte</surname><given-names>N.</given-names></name>
        <name><surname>Vulic</surname><given-names>I.</given-names></name>
        <name><surname>Korhonen</surname><given-names>A.</given-names></name>
      </person-group>
      <source>Spatial multi-arrangement for clustering and multi-way similarity dataset construction</source>
      <publisher-name>European Language Resources Association</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <uri>https://www.repository.cam.ac.uk/handle/1810/306834</uri>
    </element-citation>
  </ref>
  <ref id="ref-gardenfors2004-nb-article">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Gardenfors</surname><given-names>Peter</given-names></name>
      </person-group>
      <article-title>Conceptual Spaces as a Framework for Knowledge Representation</article-title>
      <year iso-8601-date="2004">2004</year>
    </element-citation>
  </ref>
  <ref id="ref-jozwik2016-nb-article">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Jozwik</surname><given-names>Kamila M.</given-names></name>
        <name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name>
        <name><surname>Mur</surname><given-names>Marieke</given-names></name>
      </person-group>
      <article-title>Visual features as stepping stones toward semantics: Explaining object similarity in IT and perception with non-negative least squares</article-title>
      <source>Neuropsychologia</source>
      <year iso-8601-date="2016-03-01">2016</year><month>03</month><day>01</day>
      <volume>83</volume>
      <uri>https://www.sciencedirect.com/science/article/pii/S0028393215301998</uri>
      <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2015.10.023</pub-id>
      <fpage>201</fpage>
      <lpage>226</lpage>
    </element-citation>
  </ref>
  <ref id="ref-jozwik2017-nb-article">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Jozwik</surname><given-names>Kamila M.</given-names></name>
        <name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name>
        <name><surname>Storrs</surname><given-names>Katherine R.</given-names></name>
        <name><surname>Mur</surname><given-names>Marieke</given-names></name>
      </person-group>
      <article-title>Deep convolutional neural networks outperform feature-based but not categorical models in explaining object similarity judgments</article-title>
      <source>Frontiers in Psychology</source>
      <year iso-8601-date="2017">2017</year>
      <volume>8</volume>
      <uri>https://www.frontiersin.org/articles/10.3389/fpsyg.2017.01726</uri>
    </element-citation>
  </ref>
  <ref id="ref-marr1997-nb-article">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Marr</surname><given-names>D.</given-names></name>
        <name><surname>Nishihara</surname><given-names>H. K.</given-names></name>
        <name><surname>Brenner</surname><given-names>Sydney</given-names></name>
      </person-group>
      <article-title>Representation and recognition of the spatial organization of three-dimensional shapes</article-title>
      <source>Proceedings of the Royal Society of London. Series B. Biological Sciences</source>
      <year iso-8601-date="1997-01">1997</year><month>01</month>
      <volume>200</volume>
      <issue>1140</issue>
      <uri>https://royalsocietypublishing.org/doi/abs/10.1098/rspb.1978.0020</uri>
      <pub-id pub-id-type="doi">10.1098/rspb.1978.0020</pub-id>
      <fpage>269</fpage>
      <lpage>294</lpage>
    </element-citation>
  </ref>
  <ref id="ref-kriegeskorte2008-nb-article">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name>
        <name><surname>Mur</surname><given-names>Marieke</given-names></name>
        <name><surname>Bandettini</surname><given-names>Peter</given-names></name>
      </person-group>
      <article-title>Representational similarity analysis - connecting the branches of systems neuroscience</article-title>
      <source>Frontiers in Systems Neuroscience</source>
      <year iso-8601-date="2008">2008</year>
      <volume>2</volume>
      <uri>https://www.frontiersin.org/articles/10.3389/neuro.06.004.2008</uri>
    </element-citation>
  </ref>
  <ref id="ref-kawakita2023-nb-article">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kawakita</surname><given-names>Genji</given-names></name>
        <name><surname>Zeleznikow-Johnston</surname><given-names>Ariel</given-names></name>
        <name><surname>Takeda</surname><given-names>Ken</given-names></name>
        <name><surname>Tsuchiya</surname><given-names>Naotsugu</given-names></name>
        <name><surname>Oizumi</surname><given-names>Masafumi</given-names></name>
      </person-group>
      <article-title>Is my&quot; red&quot; your&quot; red&quot;?: Unsupervised alignment of qualia structures via optimal transport</article-title>
      <year iso-8601-date="2023">2023</year>
    </element-citation>
  </ref>
  <ref id="ref-fechner1860-nb-article">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Fechner</surname><given-names>Gustav Theodor</given-names></name>
      </person-group>
      <source>Elemente der Psychophysik</source>
      <publisher-name>Breitkopf u. Härtel</publisher-name>
      <year iso-8601-date="1860">1860</year>
    </element-citation>
  </ref>
  <ref id="ref-mach1890a-nb-article">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Mach</surname><given-names>Ernst</given-names></name>
      </person-group>
      <article-title>The analysis of the sensations. antimetaphysical</article-title>
      <source>The Monist</source>
      <year iso-8601-date="1890">1890</year>
      <volume>1</volume>
      <issue>1</issue>
      <uri>https://www.jstor.org/stable/27896829</uri>
      <fpage>48</fpage>
      <lpage>68</lpage>
    </element-citation>
  </ref>
</ref-list>
<fn-group>
  <fn id="fn1-nb-article">
    <label>1</label><p>Most notably in the works of Fechner
    (<xref alt="1860" rid="ref-fechner1860-nb-article" ref-type="bibr">1860</xref>)
    and Mach
    (<xref alt="1890" rid="ref-mach1890a-nb-article" ref-type="bibr">1890</xref>);
    see also Roads &amp; Love
    (<xref alt="2024" rid="ref-roads2024-nb-article" ref-type="bibr">2024</xref>)
    for an extended review.</p>
  </fn>
  <fn id="fn2-nb-article">
    <label>2</label><p>Or, less prettily put, “this unsupervised
    extraction of hidden representational features”.</p>
  </fn>
</fn-group>
</back>


</sub-article>
<sub-article article-type="notebook" id="nb-4-nb-1">
<front-stub>
<title-group>
<article-title>Study simulation</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Delem</surname>
<given-names>Maël</given-names>
</name>
<string-name>Maël Delem</string-name>

</contrib>
</contrib-group>
</front-stub>

<body>
<boxed-text>
  <disp-quote>
    <p><bold>Packages and setup</bold></p>
    <p>Down below is the code to load necessary packages used for the
    simulation and analysis, along with some setups for the whole
    document (<italic>hover over the numbers on the far right for
    additional explanation of code and mechanics</italic>).</p>
    <sec id="nb-code-cell-1-nb-1" specific-use="notebook-code">
    <code language="r script">
# ═══ Packages ═════════════════════════════════════════════════════════════════

if (!require(librarian)) install.packages(librarian)
    </code>
    <def-list>
      <def-item>
        <term>Line 3</term>
        <def>
          <p>The package <monospace>librairian</monospace> eases package
          management with the “shelf” function, which automatically: (1)
          checks if a package is installed; (2) installs it if need be;
          (3) loads the package like the “library()” function would.</p>
        </def>
      </def-item>
      <def-item>
        <term>Line 27</term>
        <def>
          <p><monospace>reticulate</monospace> allows to translate and
          transfer objects and functions from R to Python and
          vice-versa, and was thus of primary importance for the
          successful use of the Python toolbox on our simulated
          data.</p>
        </def>
      </def-item>
      <def-item>
        <term>Lines 35,51</term>
        <def>
          <p>These are personal custom color palettes meant to extend my
          favourite palette, the color-atypical friendly Okabe-Ito color
          palette. The palette originally has only eight colors, but I
          will need nine, then up to 30 for later graphs, so I extended
          it with a hand-picked selection of mine.</p>
        </def>
      </def-item>
      <def-item>
        <term>Lines 56,70</term>
        <def>
          <p>These are R objects that were the results of a previous run
          of the simulation.</p>
        </def>
      </def-item>
    </def-list>
    <code id="annotated-cell-7-nb-1" language="r script">
# loading
library(librarian)     

# now putting packages on our library's shelves:
shelf(
  # ─── data management ─────────────────
  holodeck,       # simulating multivariate data
  cluster,        # dissimilarity matrices
  
  # ─── modelling ───────────────────────
  mclust,         # mixture clustering
  
  #  data visualization ──────────────
  # palettes
  viridis,        # colour-blind friendly palettes
  # interactive
  plotly,         # interactive plots
  ggdendro,       # dendrograms
  seriation,      # dissimilarity plots
  webshot2,       # HTML screenshots for Word render
  webshot,
  
  # ─── essential package collections ───
  doParallel,     # parallel execution
  easystats,      # data analysis ecosystem
  reticulate,     # R to Python                       
  tidyverse,      # modern R ecosystem
)

# ─── Global cosmetic theme ───
theme_set(theme_modern(base_size = 14))  

# ─── My palettes ───
pal_okabe_ito &lt;- c(                                                  
  &quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;,                            
  &quot;#F5C710&quot;, &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;, &quot;#6c0009&quot;)      

# We'll need 9 colors at some point
pal_okabe_ito_extended &lt;- c(                                 
  &quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;,                           
  &quot;#F5C710&quot;, &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;, &quot;#6c0009&quot;, &quot;#414487FF&quot;)

# We'll need 30 colors at another moment
cool_30_colors &lt;- c(                                                   
  &quot;#3d51b4&quot;, &quot;#414487FF&quot;, &quot;#003d73&quot;, &quot;#440154FF&quot;, &quot;#6c0009&quot;, &quot;#b64e4e&quot;,
  &quot;#D55E00&quot;, &quot;#E69F00&quot;, &quot;#F5C710&quot;, &quot;#FDE725FF&quot;, &quot;#f2bb7b&quot;, &quot;#f1afad&quot;, &quot;#CC79A7&quot;, 
  &quot;#e57774&quot;, &quot;#7AD151FF&quot;, &quot;#57b571&quot;, &quot;#318a4a&quot;, &quot;#009E73&quot;, &quot;#22A884FF&quot;, 
  &quot;#2A788EFF&quot;, &quot;#0072B2&quot;, &quot;#2da6b5&quot;, &quot;#56B4E9&quot;, &quot;#889be0&quot;, &quot;#6677e0&quot;,   
  &quot;#3d51b4&quot;, &quot;#414487FF&quot;, &quot;#003d73&quot;, &quot;#440154FF&quot;, &quot;#6c0009&quot;, &quot;#b64e4e&quot;
  )                                                                 

path = &quot;data/&quot;

# loading result data
df &lt;- read_rds(paste0(path, &quot;df.RDS&quot;))
df_embeds &lt;- read_rds(paste0(path, &quot;df_embeds.RDS&quot;))
# Categorical and visual embeddings
df_embeds_categ  &lt;- read_rds(paste0(path, &quot;df_embeds_categ.RDS&quot;))
df_embeds_visual &lt;- read_rds(paste0(path, &quot;df_embeds_visual.RDS&quot;))
# Subject embeddings per sub-group
df_embed_c_sub  &lt;- read_rds(paste0(path, &quot;df_embed_c_sub.RDS&quot;))
df_embed_cs_sub &lt;- read_rds(paste0(path, &quot;df_embed_cs_sub.RDS&quot;))
df_embed_v_sub  &lt;- read_rds(paste0(path, &quot;df_embed_v_sub.RDS&quot;))
df_embed_vs_sub &lt;- read_rds(paste0(path, &quot;df_embed_vs_sub.RDS&quot;))
# Accuracy of the unsupervised alignment (bad = not tidy data)
df_accuracy_all_bad &lt;- read_rds(paste0(path, &quot;df_accuracy_all_bad.RDS&quot;))
df_accuracy_cat_bad &lt;- read_rds(paste0(path, &quot;df_accuracy_cat_bad.RDS&quot;))
# Coordinates of the aligned embeddings from the Python output
coordinates_aligned_embeddings &lt;- read_rds(paste0(path, &quot;coordinates_aligned_embeddings.RDS&quot;)) 
    </code>
    <sec id="nb-code-cell-1-output-0-nb-1" specific-use="notebook-output">
    <preformat>Le chargement a nécessité le package : librarian</preformat>
    </sec>
    </sec>
  </disp-quote>
</boxed-text>
<sec id="visual-spatial-verbal-model-of-cognitive-profiles-nb-1">
  <title>Visual-spatial-verbal model of cognitive profiles</title>
  <p>We are going to simulate 30 participants presenting four different
  cognitive profiles, that I defined as, respectively,
  <italic>verbal</italic> aphantasics, <italic>spatial</italic>
  aphantasics, <italic>spatial</italic> phantasics, and
  <italic>visual</italic> phantasics.</p>
  <p>To simulate these four sub-groups, we use the
  <monospace>holodeck</monospace> R package to generate multivariate
  normal distributions of scores on these three dimensions for each
  sub-group. For instance, verbal aphantasics have normally distributed
  visual imagery scores centered around a mean of 0 (normalized, so
  negative scores are possible), 0.4 for spatial imagery, and 0.7 for
  verbal style; Spatial aphantasics have means of 0 for visual, 0.75
  spatial, and 0.3 for verbal; etc. The numbers are arbitrary, but have
  been chosen by trial-and-error to obtain a model that is both
  well-defined and not exaggerated.</p>
  <sec id="nb-code-cell-2-nb-1" specific-use="notebook-code">
  <code language="r script">
# ─── Generating hypothetical imagery values for 30 subjects ─── 

# The function takes the variance and covariance of the imagery distributions
# as arguments
generate_osv_model &lt;- function(var, cov){
  df &lt;- 
    tibble(group = rep(c(&quot;aph&quot;, &quot;phant&quot;), each = 8)) |&gt; 
    group_by(group) |&gt; 
    mutate(
      spatial_group = c(rep(&quot;spa_low&quot;, 4), rep(&quot;spa_high&quot;, 4)),
      vis_spa_group = paste0(group, &quot;_&quot;, spatial_group),
      verbal_group = &quot;verbal_low&quot;,
      verbal_group  = case_when(
        vis_spa_group == &quot;aph_spa_low&quot; ~ &quot;verbal_high&quot;, 
        vis_spa_group == &quot;phant_spa_low&quot; ~ &quot;verbal_mid&quot;,
        TRUE ~ verbal_group)
    ) |&gt; 
    group_by(vis_spa_group) |&gt; 
    # ─── visual ───
    sim_discr(
      n_vars = 1, 
      var = var, 
      cov = cov, 
      # aph_s, aph_v, phant_s, phant_v
      group_means = c(0, 0, 0.6, 0.87), 
      name = &quot;v&quot;) |&gt; 
    # ─── spatial ───
    sim_discr(
      n_vars = 1,  
      var = var, 
      cov = cov, 
      # aph_s, aph_v, phant_s, phant_v
      group_means = c(0.75, 0.4, 0.7, 0.3), 
      name = &quot;s&quot;) |&gt;
    # ─── verbal ───
    sim_discr(
      n_vars = 1,  
      var = var, 
      cov = cov, 
      # aph_s, aph_v, phant_s, phant_v
      group_means = c(0.3, 0.7, 0.3, 0.5), 
      name = &quot;i&quot;) |&gt;
    rename(
      visual_imagery  = v_1,
      spatial_imagery = s_1,
      verbal_profile  = i_1
      )
}

df &lt;- generate_osv_model(0.03, 0)
  </code>
  </sec>
  <sec id="generating-prototype-embeddings-from-a-sphere-nb-1">
    <title>Generating “prototype” embeddings from a sphere</title>
    <p>Proposal from
    <ext-link ext-link-type="uri" xlink:href="https://stats.stackexchange.com/questions/7977/how-to-generate-uniformly-distributed-points-on-the-surface-of-the-3-d-unit-sphe">StackExchange</ext-link>
    to generate points on a sphere:</p>
    <p>Let’s use a function to generate embeddings. We get 8 nicely
    distributed clusters. We’ll retrieve the centroids of each cluster,
    which would be the “perfect” categories of each species group (say,
    generated by a computational model on categorical criteria).</p>
    <sec id="nb-code-cell-3-nb-1" specific-use="notebook-code">
    <code language="r script">
# ─── Generating a sphere of 1000 points and 8 equally distributed clusters ─── 

generate_sphere &lt;- function(n){
  z     &lt;- 2*runif(n) - 1          # uniform on [-1, 1]
  theta &lt;- 2*pi*runif(n) - pi      # uniform on [-pi, pi]
  x     &lt;- sin(theta)*sqrt(1-z^2)  # based on angle
  y     &lt;- cos(theta)*sqrt(1-z^2) 
  
  df &lt;- tibble(x = x, y = y, z = z)
  
  return(df)
}

# 1000 random observations with embeddings uniformly distributed on a sphere
df_embeds &lt;- generate_sphere(1000)

# Clustering the observations in 8 groups based on their coordinates
clusters &lt;- Mclust(df_embeds, G = 8)

# adding the classification to the data
df_embeds &lt;- df_embeds |&gt; mutate(group = as.factor(clusters$classification))

# getting the centroids of each cluster
df_centroids &lt;- 
  df_embeds |&gt; 
  group_by(group) |&gt; 
  summarise(
    x_centroid = mean(x),
    y_centroid = mean(y),
    z_centroid = mean(z)
  )

# adding them to the data
df_embeds_2 &lt;- left_join(df_embeds, df_centroids, by = &quot;group&quot;)
    </code>
    </sec>
  </sec>
  <sec id="categorical-model-embeddings-nb-1">
    <title>Categorical model embeddings</title>
    <p>The selection procedure for the <bold>categorical model</bold>
    will consist of selecting points that are rather <italic>close to
    the centroids</italic>. Thus, we will filter the observations of the
    large sets to keep only points for which the distance to the
    centroid is inferior to a given value. That is, points for which the
    Euclidean norm of the vector from the observation to the
    centroid:</p>
    <p><disp-formula><alternatives>
    <tex-math><![CDATA[d(centroid, observation) = \sqrt{(x_{c} - x_{o})^{2} + (y_{c} - y_{o})^{2} + (z_{c} - z_{o})^{2}}]]></tex-math>
    <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>d</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:math></alternatives></disp-formula></p>
    <p>This can be done using the function
    <monospace>norm(coordinates, type = &quot;2&quot;)</monospace> in
    R.</p>
    <sec id="nb-code-cell-4-nb-1" specific-use="notebook-code">
    <code language="r script">
# ─── Selecting categorical embeddings based on distances to the centroids ───

# Function to filter points of the sphere based on the distance to the centroids
generate_embeddings &lt;- function(df, n_embeddings, distance_quantile){
  df &lt;- 
    df |&gt; 
    # computing the euclidean distance to the centroids for each observation
    rowwise() |&gt; 
    mutate(
      distance = norm(
        c((x_centroid - x), (y_centroid - y), (z_centroid - z)),
        type = &quot;2&quot;)
      ) |&gt; 
    # filtering by distance to the centroid by group
    group_by(group) |&gt; 
    # selecting the X% closest (specified with &quot;distance_quantile&quot;)
    filter(distance &lt; quantile(distance, probs = distance_quantile)) |&gt; 
    # selecting X random observations per cluster in these 
    # (specified with &quot;n_embeddings&quot;)
    slice(1:n_embeddings) |&gt; 
    select(group, x, y, z) |&gt;
    ungroup()
}

df_embeds_categ &lt;- generate_embeddings(df_embeds_2, 8, 0.5)
    </code>
    </sec>
  </sec>
  <sec id="visual-model-embeddings-nb-1">
    <title>Visual model embeddings</title>
    <p>In the case of the <bold>visual model</bold>, we would like
    approximately evenly distributed embeddings, that could also dive
    <italic>inside</italic> the sphere, i.e. representing species that
    are visually close although diametrically opposed when it comes to
    taxonomy. To do this we could try to simulate multivariate normal
    distributions around the
    centroids<xref ref-type="fn" rid="fn1-nb-1">1</xref>. This can be done
    with the <monospace>holodeck</monospace> package.</p>
    <sec id="nb-code-cell-5-nb-1" specific-use="notebook-code">
    <code language="r script">
# ─── Generating visual embeddings based on multivariate distributions 
# around the categorical embeddings ───

# defining the variance and covariance of the distributions
var2 &lt;- 0.05
cov2 &lt;- 0

# generating multivariate distributions around the categorical 3D means
df_embeds_visual &lt;-
  tibble(
    id = as.factor(seq(1,6400)),
    category = as.factor(rep(seq(1:64), each = 100))
  )|&gt; 
  group_by(category) |&gt; 
  sim_discr(
    n_vars = 1, 
    var = var2, 
    cov = cov2, 
    group_means = df_embeds_categ$x, 
    name = &quot;x&quot;) |&gt; 
  sim_discr(
    n_vars = 1, 
    var = var2, 
    cov = cov2, 
    group_means = df_embeds_categ$y, 
    name = &quot;y&quot;) |&gt; 
  sim_discr(
    n_vars = 1, 
    var = var2, 
    cov = cov2, 
    group_means = df_embeds_categ$z, 
    name = &quot;z&quot;) |&gt; 
  # keeping only 8 points per distribution
  slice(1) |&gt; 
  ungroup() |&gt; 
  mutate(group = as.factor(rep(seq(1, 8), each = 8))) |&gt; 
  rename(x = x_1, y = y_1, z = z_1) |&gt; 
  select(group, x, y, z)
    </code>
    </sec>
  </sec>
  <sec id="generating-the-subject-embeddings-nb-1">
    <title>Generating the subject embeddings</title>
    <p>We have four “reference” sets of embeddings which represent
    animals either judged according to their similarity in categorical
    terms (namely, species), or in visual terms (namely shape or color
    similarities, assuming that these similarities are more evenly
    distributed, e.g. the crab looks like a spider, but is also pretty
    close to a scorpion, etc.).</p>
    <p>To generate the embeddings of each subject in each condition, we
    will start from these reference embeddings and generate random noise
    around <italic>each item</italic>, i.e. for all 64 animals. For 100
    subjects, we would thus generate 100 noisy points around each
    animal, each point corresponding to a given subject.</p>
    <p>The visual and verbal groups will be generated with slightly more
    intra-group variance, so as to try to make the spatial groups as
    coherent as possible (and avoid blurring everything and making the
    groups disappear in noise).</p>
    <sec id="nb-code-cell-6-nb-1" specific-use="notebook-code">
    <code language="r script">
# ─── Generating unique embeddings for each subject based on the models ───

# creating dfs with participants
df_subjects_7 &lt;- 
  tibble(subject = seq(1, 7, 1)) |&gt; 
  mutate(subject = paste0(&quot;subject_&quot;, subject))

df_subjects_8 &lt;- 
  tibble(subject = seq(1, 8, 1)) |&gt; 
  mutate(subject = paste0(&quot;subject_&quot;, subject))

# splitting df_embeddings
df_embed_c  &lt;- df_embeddings |&gt; select(group, species,  x_c:z_c)
df_embed_cs &lt;- df_embeddings |&gt; select(group, species, x_cs:z_cs)
df_embed_vs &lt;- df_embeddings |&gt; select(group, species, x_vs:z_vs)
df_embed_v  &lt;- df_embeddings |&gt; select(group, species,  x_v:z_v)

# function to create embeddings per subject with normal random noise
generate_subject_embeddings &lt;- function(df, df_subjects, var){
  df &lt;-
    df |&gt; 
    mutate(subject = list(df_subjects)) |&gt; 
    unnest(subject) |&gt; 
    group_by(species) |&gt; 
    # simulating x coordinates
    sim_discr(
      n_vars = 1, 
      var = var, 
      cov = 0, 
      group_means = pull(df, 3), 
      name = &quot;x&quot;) |&gt; 
    # simulating y coordinates
    sim_discr(
      n_vars = 1, 
      var = var, 
      cov = 0, 
      group_means = pull(df, 4), 
      name = &quot;y&quot;) |&gt; 
    # simulating z coordinates
    sim_discr(
      n_vars = 1, 
      var = var, 
      cov = 0, 
      group_means = pull(df, 5), 
      name = &quot;z&quot;) |&gt;
  select(group, species, subject, 7:9) |&gt; 
  rename(x = 4, y = 5, z = 6) |&gt; 
  ungroup()
  
  return(df)
}

var_s1 = 0.001
var_s2 = 0.0005

df_embed_c_sub  &lt;- generate_subject_embeddings(df_embed_c,  df_subjects_4, var_s1)
df_embed_cs_sub &lt;- generate_subject_embeddings(df_embed_cs, df_subjects_4, var_s2)
df_embed_vs_sub &lt;- generate_subject_embeddings(df_embed_vs, df_subjects_4, var_s2)
df_embed_v_sub  &lt;- generate_subject_embeddings(df_embed_v,  df_subjects_4, var_s1)
    </code>
    </sec>
  </sec>
</sec>
</body>



<back>
<fn-group>
  <fn id="fn1-nb-1">
    <label>1</label><p>A simpler alternative would be generating the
    visual embeddings with the same code as the categorical ones,
    selecting 8 points per cluster but much more spread out
    (e.g. selecting 8 among the 90% closest to the centroids, which
    would create more variability than the categorical one set to 60%).
    I chose otherwise because this wouldn’t have had points reaching
    <italic>inside</italic> the sphere.</p>
  </fn>
</fn-group>
</back>


</sub-article>

</article>