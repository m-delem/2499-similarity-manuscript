<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving
and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">

<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">

<front>


<article-meta>


<title-group>
<article-title>Unravelling mental representations in aphantasia through
unsupervised alignment</article-title>
<subtitle>Project design and data analysis simulation</subtitle>
</title-group>

<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Delem</surname>
<given-names>Maël</given-names>
</name>
<string-name>Maël Delem</string-name>

</contrib>
</contrib-group>









<history></history>


<abstract>
<p>Research on aphantasia is confronted with a long-standing conundrum
of all research on consciousness and representations, namely the
theoretical inaccessibility of subjective representations. Drawing on
concepts from similarity and representation research, I endorse the view
that the study of an individual’s mental representations is made
possible by exploiting second-order isomorphism. The concept of
second-order isomorphism means that correspondence should not be sought
in the first-order relation between (a) an external object and (b) the
corresponding internal representation, but in the second-order relation
between (a) the perceived similarities between various external objects
and (b) the similarities between their corresponding internal
representations. Building on this idea, this study project report is
divided into five parts. <bold>First</bold>, I outline the central ideas
underlying similarity research and its applicability to aphantasia
research. <bold>Second</bold>, I present a methodological rationale and
protocol based on inverse multidimensional scaling that can be
implemented online to conduct such large-scale research with high
efficiency. <bold>Third</bold>, I present a data analysis plan using a
state-of-the-art method for similarity analysis, unsupervised alignment
with Gromov-Wasserstein optimal transport (GWOT). <bold>Fourth</bold>, I
report a data simulation of a potential outcome of this project and the
successful analysis of this synthetic data using GWOT alignment.
<bold>Fifth</bold>, I analyse the feasability of such a project given
the material constraints of my thesis. I conclude with the expected
utility and benefits of this project.</p>
</abstract>




</article-meta>

</front>

<body>
<boxed-text>
<p><bold>Project inception</bold></p>
<p>This project stems from several elements:</p>
<list list-type="order">
  <list-item>
    <p>The long standing knowledge of the fact that internal
    representations seem impossible to reach due to their subjective
    nature.</p>
  </list-item>
  <list-item>
    <p>The discovery of the article of Shepard and Chipman
    (<xref alt="1970" rid="ref-shepardSecondorderIsomorphismInternal1970" ref-type="bibr">1970</xref>)
    that expose the idea of “second-order isomorphism”.</p>
  </list-item>
  <list-item>
    <p>The discovery of state-of-the-art and accessible unsupervised
    analytic methods to study this principle in an astonishing way. The
    last two discoveries (and many more) are the fruit of amazing
    discussions and recommendations from Ladislas when he came here.
    These motivated me to try to implement GWOT in R on data that I
    wanted to create myself to emulate a study we could do.</p>
  </list-item>
</list>
<p><italic>I promise that I did this mostly on my spare time, we have
too many other things to do elsewhere.</italic></p>
</boxed-text>
<sec id="theoretical-context">
  <title>1. Theoretical context</title>
  <sec id="psychological-spaces-and-aphantasia">
    <title>1.1 Psychological spaces and aphantasia</title>
    <p>While attempting to demonstrate the uselessness of the concept of
    similarity as a philosophical and scientific
    notion<xref ref-type="fn" rid="fn1">1</xref>, Goodman
    (<xref alt="1972" rid="ref-goodmanSevenStricturesSimilarity1972" ref-type="bibr">1972</xref>)
    has inadvertently expressed an aspect of similarity judgements of
    primary importance to us aphantasia researchers:</p>
    <disp-quote>
      <p>Comparative judgments of similarity often require not merely
      selection of relevant properties but a weighting of their relative
      importance, and variation in both relevance and importance can be
      rapid and enormous. Consider baggage at an airport checking
      station. The spectator may notice shape, size, color, material,
      and even make of luggage; the pilot is more concerned with weight,
      and the passenger with destination and ownership. Which pieces are
      more alike than others depends not only upon what properties they
      share, but upon who makes the comparison, and when. . . .
      Circumstances alter similarities.</p>
    </disp-quote>
    <p>This can be easily reversed as an argument in favor of the
    <bold>potential of similarity analyses to highlight the
    inter-individual differences in sensory mental
    representations</bold>. For example, should we ask individuals to
    judge the similarities in shape or color between various objects,
    the <italic>differences between the similarity structures</italic>
    of individuals will be precisely the most important phenomenon for
    us, far less than the constancy between these structures. If we can
    account for the context dependence, as we will propose here with
    explicit instructions, clever task design, and hypothesis-neutral
    analysis, we could overcome the limitations of the inherently
    subjective nature of similarity judgements.</p>
    <p>This idea of a difference in similarity judgements in aphantasia
    seems to transpire in the results of Bainbridge et al.
    (<xref alt="2021" rid="ref-bainbridgeQuantifyingAphantasiaDrawing2021" ref-type="bibr">2021</xref>)
    on their drawing study. They have shown that aphantasics had more
    schematic representations during recall, accurate in their spatial
    positioning, but with less sensory details. This difference can be
    seen from two perspectives: (1) a memory deficit for sensory
    properties; (2) a different representational structure of the items
    in their psychological spaces. In the latter case, aphantasics would
    have greater/faster abstraction of their representation of a
    perceived scene, reducing the amount of encoded sensory details
    unconsciously considered to be relevant. Both (1) and (2) can
    theoretically explain the same behavioural response, i.e. less
    sensory elements and correct spatial recall accuracy in aphantasic
    drawings, but <bold>the two have drastically different consequences
    on how we define, characterize, and judge aphantasia.</bold></p>
    <p>The dominant hypothesis seems to be that aphantasics simply have
    an episodic or general memory deficit. Conversely, I hypothesize
    that aphantasics have different representational structures than
    phantasics in certain dimensions of their psychological spaces
    (notably sensory, but potentially abstract too). More generally, I
    hypothesize that the concept of visual imagery evaluates in reality
    the continuous spectrum of representational structures in
    <italic>sensory</italic> dimensions of psychological spaces.
    Mirroring visual imagery, spatial imagery could also be a rough
    psychometric evaluation of the continuous spectrum of structural
    differences in <italic>conceptual/abstract</italic> dimensions of
    psychological spaces. In this view, the psychological space of
    aphantasics would constrain internal representations to particularly
    abstract forms from a very early stage, thus selectively limiting
    the item properties thereafter encoded in long-term memory. In other
    terms, <bold>I hypothesize that aphantasia would not be
    characterized by an episodic memory deficit, but by an episodic
    memory <italic>selectivity</italic> caused by the specific
    characteristics of their representational structures and
    psychological spaces.</bold> This selectivity would have, as we
    already hypothesized several times, benefits and drawbacks.</p>
    <p>Gardenfors
    (<xref alt="2004a" rid="ref-gardenforsConceptualSpacesFramework2004" ref-type="bibr">2004a</xref>)
    proposed that differences in psychological (in his terms,
    conceptual) spaces could arise from various sources, whether innate,
    due to learning, or broader cultural or social differences. All
    these hypotheses could be coherent to explain the sources of
    aphantasia. Nevertheless, the study of these sources should be the
    subject of very large-scale or longitudinal studies, which are out
    of the scope of this project.</p>
    <p>Here, we shall rather attempt to <bold>develop a method to
    characterize the differences in aphantasics’ representational
    structures and psychological spaces.</bold></p>
  </sec>
</sec>
<sec id="methodology">
  <title>2. Methodology</title>
  <p>Roads and Love
  (<xref alt="2024" rid="ref-roads2024" ref-type="bibr">2024</xref>), in
  a recent review on the state and perspectives of similarity research,
  highlighted two challenges that studies in this field had to face: (1)
  The high cost of collecting behavioral data on a large number of
  stimuli; (2) The lack of software packages being a high barrier to
  entry, making the task of coding models difficult for the
  uninitiated.</p>
  <p>To solve these problems, we present here two solutions,
  respectively for (1) experimental design and (2) data analysis:</p>
  <list list-type="order">
    <list-item>
      <p>A recent method to efficiently acquire similarity judgements,
      the “multiple arrangement of items” and “inverse multidimensional
      scaling” developed by Kriegeskorte and Mur
      (<xref alt="2012" rid="ref-kriegeskorteInverseMDSInferring2012" ref-type="bibr">2012</xref>).</p>
    </list-item>
    <list-item>
      <p>An accessible and robust Python toolbox provided by Sasaki et
      al.
      (<xref alt="2023" rid="ref-sasakiToolboxGromovWassersteinOptimal2023" ref-type="bibr">2023</xref>)
      to conduct unsupervised alignment analysis using
      Gromov-Wasserstein optimal transport.</p>
    </list-item>
  </list>
  <sec id="experimental-design">
    <title>2.1 Experimental design</title>
    <sec id="multi-arrangement-and-inverse-multidimensional-scaling">
      <title>Multi-arrangement and inverse multidimensional
      scaling</title>
      <p>Assuming a geometric model of representational similarities,
      Kriegeskorte and Mur
      (<xref alt="2012" rid="ref-kriegeskorteInverseMDSInferring2012" ref-type="bibr">2012</xref>)
      developed a multi-arrangement (MA) method to efficiently acquire
      (dis)similarity judgments for large sets of objects. The subject
      has to perform multiple arrangements of item subsets adaptively
      designed for optimal measurement efficiency and for estimating the
      representational dissimilarity matrix (RDM) by combining the
      evidence from the subset arrangements.</p>
      <p>The procedure is illustrated in
      <xref alt="Figure 2.1" rid="fig-multi-arrangement">Figure 2.1</xref>.</p>
      <fig id="fig-multi-arrangement">
        <caption><p>Figure 2.1: <bold>Acquiring similarity judgements
        with the multi-arrangement method. (A)</bold> Subjects are asked
        to arrange items according to their similarity, using mouse
        drag-and-drop on a computer. The similarity measure is taken as
        the distances between the items: similar items are closer, while
        dissimilar items are further apart. The upper part of the figure
        shows screenshots at different moments of the acquisition for
        one subject. Columns are trials and rows show the object
        arrangements over time, running from the start (top row) to the
        end (last row). The first trial contains all items; subsequent
        trials contain subsets of items that are adaptively selected to
        optimally estimate judged similarity for each subject.
        <bold>(B)</bold> Once acquisition of the final judgements is
        completed, inter-item distances in the final trial arrangements
        are combined over trials by rescaling and averaging to yield a
        single dissimilarity estimate for each object pair. The process
        is illustrated in this figure for two example item pairs: a
        boy’s face and a hand (red), and carrots and a stop sign (blue).
        Their single-trial dissimilarity estimates (arrows) are combined
        into a single dissimilarity estimate, which is placed at the
        corresponding entry of the RDM (lower panel). Mirror-symmetric
        entries are indicated by lighter colors (figure from
        <xref alt="Mur et al. 2013" rid="ref-murHumanObjectSimilarityJudgments2013" ref-type="bibr">Mur
        et al. 2013</xref>).</p></caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="images/multi-arrangement-method-mur-2013.png" />
      </fig>
      <p>A key strength of this method that sets it as particularly
      effective is the “adaptive” part. The goal of the process is to
      acquire similarity judgements as precisely as possible while
      minimizing the total amount of trials. To do so, starting from the
      second trial, selected subsets of the items to be compared are
      presented to the subject: these items are the ones that were very
      close on-screen in previous trials and thus had their distance
      evaluated with lower accuracy by the subject. As the subject has
      to fill the entire “arena” with the items, these subsequent trials
      will necessarily increase the level of precision in the similarity
      judgement between pairs of items. The second key benefit of this
      method is the time and effort gain compared to others. For
      example, to compare every pair of items among 64 different items
      would require <inline-formula><alternatives>
      <tex-math><![CDATA[\frac{64 \times (64-1)}{2} = 2016]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>64</mml:mn><mml:mo>×</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mn>64</mml:mn><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>=</mml:mo><mml:mn>2016</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
      comparisons (i.e. trials). This would be extremely time-consuming,
      while also losing the <italic>context-independence</italic>
      afforded by the MA method due to the presence of other items
      around every time the subject mentally performs a pairwise
      comparison.</p>
      <p>Historically, when referring to the projection of the
      representations of stimuli (e.g., coordinates in geometric space)
      from a high-dimensional space into a lower-dimensional space,
      inference algorithms were commonly called multidimensional scaling
      (<xref alt="Roads and Love 2024" rid="ref-roads2024" ref-type="bibr">Roads
      and Love 2024</xref>). By analogy, the process of combining
      several lower-dimensional (2D) similarity judgements on-screen to
      form one higher dimensional similarity representation (in the RDM)
      can be conceptually seen as “inverse” multidimensional scaling,
      hence the name given to the method by Kriegeskorte and Mur
      (<xref alt="2012" rid="ref-kriegeskorteInverseMDSInferring2012" ref-type="bibr">2012</xref>).</p>
    </sec>
    <sec id="principle">
      <title>Principle</title>
      <p>The idea is simple: for a given set of items that have distinct
      and very pictorial visual properties, we would ask a wide range of
      aphantasics, phantasics or hyperphantasics to imagine, mentally
      compare and make similarity judgements between the items. To
      compare these representations with actual perceptual
      representations, the subjects would also perform the same task
      afterwards, this time with actual pictures to compare. Subjects
      would also fill our usual psychometric imagery questionnaires.</p>
      <p>To “compare imagined items”, we could use a “word” version of
      the MA paradigm. An example from Majewska et al.
      (<xref alt="2020" rid="ref-majewskaSpatialMultiarrangementClustering2020" ref-type="bibr">2020</xref>)
      - <italic>who used the method to build large-scale semantic
      similarity resources for Natural Language Processing
      systems</italic> - is represented in
      <xref alt="Figure 2.2" rid="fig-majewska">Figure 2.2</xref>.</p>
      <fig id="fig-majewska">
        <caption><p>Figure 2.2: Arena layout of the MA protocol used by
        to acquire similarity judgements on word pairs (figure from
        <xref alt="Majewska et al. 2020" rid="ref-majewskaSpatialMultiarrangementClustering2020" ref-type="bibr">Majewska
        et al. 2020</xref>).</p></caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="images/majewska-spam.png" />
      </fig>
      <p>We could have the stimuli rated by another set of participants
      on several features.</p>
      <disp-quote>
        <p>« <italic>We deliberately did not specify which object
        properties to focus on, to avoid biasing participants’
        spontaneous mental representation of the similarities between
        objects. Our aim was to obtain similarity judgments that reflect
        the natural representation of objects without forcing
        participants to rely on one given dimension. However,
        participants were asked after having performed the task, what
        dimension(s) they used in judging object similarity.</italic> »
        (<xref alt="Jozwik, Kriegeskorte, and Mur 2016" rid="ref-jozwik2016" ref-type="bibr">Jozwik,
        Kriegeskorte, and Mur 2016</xref>)</p>
      </disp-quote>
      <disp-quote>
        <p>« <bold><italic>All but one of the 16 participants reported
        arranging the images according to a categorical
        structure.</italic></bold> »
        (<xref alt="Jozwik et al. 2017" rid="ref-jozwik2017" ref-type="bibr">Jozwik
        et al. 2017</xref>)</p>
      </disp-quote>
      <p>This result of Jozwik et al.
      (<xref alt="2017" rid="ref-jozwik2017" ref-type="bibr">2017</xref>)
      suggests that we should give an explicit instruction about the
      features to focus on, otherwise everyone might bypass visual
      features and mental images in favour of concepts and categories,
      regardless of their mental imagery profile.</p>
      <p>In contrast, if we ask to focus specifically on the visual
      features, then ask subjects about the strategy they used to
      evaluate the similarities, then on the subjectively felt mental
      format of these strategies, we might grasp better insight on the
      sensory representations of subjects.</p>
      <p>We could even go for several comparisons - even though this
      would increase quadratically the number of trials - e.g. :</p>
      <list list-type="bullet">
        <list-item>
          <p>Evaluate to what extent the <bold>shape</bold> <italic>of
          these animals are</italic>
          <bold><italic>similar</italic></bold> <bold>at rest, ignoring
          size differences.</bold></p>
        </list-item>
        <list-item>
          <p>Evaluate to what extent these animals <bold>sound like each
          other.</bold></p>
        </list-item>
        <list-item>
          <p>Etc.</p>
        </list-item>
      </list>
      <disp-quote>
        <p><italic>Note to be added: if you do not know the animal, just
        guess its placement, as this situation is quite unlikely to
        happen (animals chosen are fairly common
        knowledge).</italic></p>
      </disp-quote>
      <p>Kawakita et al.
      (<xref alt="2023" rid="ref-kawakita2023" ref-type="bibr">2023</xref>):
      To assess whether the color dissimilarity structures from
      different participants can be aligned in an unsupervised manner,
      we divided color pair similarity data from a large pool of 426
      participants into five participant groups (85 or 86 participants
      per group) to obtain five independent and complete sets of
      pairwise dissimilarity ratings for 93 color stimuli (Fig. 3a).
      Each participant provided a pairwise dissimilarity judgment for a
      randomly allocated subset of the 4371 possible color pairs. We
      computed the mean of all judgments for each color pair in each
      group, generating five full dissimilarity matrices referred to as
      Group 1 to Group 5.</p>
    </sec>
    <sec id="stimuli">
      <title>Stimuli</title>
      <p>We would have a list of animal items, that would have several
      characteristics:</p>
      <list list-type="bullet">
        <list-item>
          <p>A name</p>
        </list-item>
        <list-item>
          <p>A category</p>
        </list-item>
        <list-item>
          <p>A shape</p>
        </list-item>
      </list>
      <p>We need orthogonal data:</p>
      <list list-type="bullet">
        <list-item>
          <p>Each class of animal should include each shape
          (roughly)</p>
        </list-item>
        <list-item>
          <p>Each shape should have an animal</p>
        </list-item>
      </list>
      <p>This would imply that category cannot be derived from shape,
      and vice-versa. Thus, a <bold>sorting by shape would reveal to be
      innately visual</bold> (or maybe spatial, if shape concerns this
      type of imagery), and a <bold>sorting by category would reveal an
      abstraction</bold> from these shapes. We expect that the two will
      be mixed to some degree in every subject, but that low-imagery
      would rather tend towards category sorting, while high-imagery
      would tend towards shape sorting.</p>
      <p>Shapes could be very tricky stimuli to discuss. Gardenfors
      (<xref alt="2004b" rid="ref-gardenfors2004" ref-type="bibr">2004b</xref>)
      noted that we only have a very sketchy understanding of how we
      perceive and conceptualize things according to their shapes. The
      works of Marr, Nishihara, and Brenner
      (<xref alt="1997" rid="ref-marr1997" ref-type="bibr">1997</xref>)
      highlight this difficulty when analysing the complexity of the
      hierarchical judgements of shapes and volumes, as shown in
      <xref alt="Figure 2.3" rid="fig-marr">Figure 2.3</xref>.</p>
      <fig id="fig-marr">
        <caption><p>Figure 2.3: Representing the characteristics of
        shapes with cylinders (figure from
        <xref alt="Marr, Nishihara, and Brenner 1997" rid="ref-marr1997" ref-type="bibr">Marr,
        Nishihara, and Brenner 1997</xref>).</p></caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="images/shapes-marr.png" />
      </fig>
    </sec>
  </sec>
</sec>
<sec id="data-analysis-plan">
  <title>3. Data analysis plan</title>
  <sec id="unsupervised-alignment-rationale">
    <title>3.1 Unsupervised alignment rationale</title>
    <p>Visual images can be represented as points in a multidimensional
    psychological space. Embedding algorithms can be used to infer
    latent representations from human similarity judgments. While there
    are an infinite number of potential visual features, an embedding
    algorithm can be used to identify the subset of salient features
    that accurately model human-perceived similarity. (<italic>From
    Roads’ CV</italic>)</p>
    <p>Using an optimization algorithm, the free parameters of a
    psychological space are found by maximizing goodness of fit (i.e.,
    the loss function) to the observed data. Historically, when
    referring specifically to the free parameters that correspond to the
    representation of stimuli (e.g., coordinates in geometric space),
    inference algorithms were commonly called multidimensional scaling
    (MDS), or simply scaling, algorithms.</p>
    <p>In the machine learning literature, analogous inference
    algorithms are often called embedding algorithms. The term
    “embedding” denotes a higher-dimensional representation that is
    embedded in a lower-dimensional space. For that reason, the inferred
    mental representations of a psychological space could also be called
    a psychological embedding.</p>
    <p>Numerous techniques exist, and each has limitations. Popular
    techniques for comparing representations include RSA Kriegeskorte,
    Mur, and Bandettini
    (<xref alt="2008" rid="ref-kriegeskorte2008" ref-type="bibr">2008</xref>)
    and canonical correlation analysis (CCA) (Hotelling 1936). Briefly,
    RSA is a method for comparing two representations that assesses the
    correlation between the implied pairwise similarity matrices. CCA is
    a method that compares two representations by finding a pair of
    latent variables (one for each domain) that are maximally
    correlated.</p>
    <p>One might be tempted to compare two dissimilarity matrices
    assuming stimulus-level “external” correspondence: my “red”
    corresponds to your “red”(Fig. 1d). This type of supervised
    comparison between dissimilarity matrices, known as Representational
    Similarity Analysis (RSA), has been widely used in neuroscience to
    compare various similarity matrices obtained from behavioural and
    neural data. However, there is no guarantee that the same stimulus
    will necessarily evoke the same subjective experience across
    different participants. Accordingly, when considering which stimuli
    evoke which qualia for different individuals, we need to consider
    all possibilities of correspondence: my “red” might correspond to
    your “red”, “green”, “purple”, or might lie somewhere between your
    “orange” and “pink”(Fig. 1e). Thus, we compare qualia structures in
    a purely unsupervised manner, without assuming any correspondence
    between individual qualia across participants.</p>
  </sec>
  <sec id="gromov-wasserstein-optimal-transport">
    <title>3.2 Gromov-Wasserstein optimal transport</title>
    <p>To account for all possible correspondences, we use an
    unsupervised alignment method for quantifying the degree of
    similarity between qualia structures. As shown in Fig. 2a, in
    unsupervised alignment, we do not attach any external (stimuli)
    labels to the qualia embeddings. Instead, we try to find the best
    matching between qualia structures based only on their internal
    relationships (see Methods). After finding the optimal alignment, we
    can use external labels, such as the identity of a color stimulus
    (Fig. 2b), to evaluate how the embeddings of different individuals
    relate to each other. This allows us to determine which color
    embeddings correspond to the same color embeddings across
    individuals or which do not. Checking the assumption that these
    external labels are consistent across individuals allows us to
    assess the plausibility of determining accurate inter-individual
    correspondences between qualia structures of different
    participants.</p>
    <p>To this end, we used the Gromov-Wasserstein optimal transport
    (GWOT) method, which has been applied with great success in various
    fields. GWOT aims to find the optimal mapping between two point
    clouds in different domains based on the distance between points
    within each domain. Importantly, the distances (or correspondences)
    between points “across” different domains are not given while those
    “within” the same domain are given. GWOT aligns the point clouds
    according to the principle that a point in one domain should
    correspond to another point in the other domain that has a similar
    relationship to other points. The principle of the method is
    illustrated in
    <xref alt="Figure 3.1" rid="fig-gwot-kawa">Figure 3.1</xref></p>
    <fig id="fig-gwot-kawa">
      <caption><p>Figure 3.1: Gromov-Wassertein optimal transport
      principle (figure from
      <xref alt="Kawakita et al. 2023" rid="ref-kawakita2023" ref-type="bibr">Kawakita
      et al. 2023</xref>).</p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="images/kawa-gwot-2.PNG" />
    </fig>
    <p>We first computed the GWD for all pairs of the dissimilarity
    matrices of the 5 groups (Group 1-5) using the optimized
    <inline-formula><alternatives>
    <tex-math><![CDATA[\epsilon]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ϵ</mml:mi></mml:math></alternatives></inline-formula>.
    In Fig. 3b, we show the optimized mapping
    <inline-formula><alternatives>
    <tex-math><![CDATA[\Gamma*]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>Γ</mml:mi><mml:mo>*</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
    between Group 1 and Groups 2-5 (see Supplementary Figure S1 for the
    other pairs). As shown in Fig. 3b, most of the diagonal elements in
    <inline-formula><alternatives>
    <tex-math><![CDATA[\Gamma*]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>Γ</mml:mi><mml:mo>*</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
    show high values, indicating that most colors in one group
    correspond to the same colors in the other groups with high
    probability. We next performed unsupervised alignment of the vector
    embeddings of qualia structures. Although
    <inline-formula><alternatives>
    <tex-math><![CDATA[\Gamma*]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>Γ</mml:mi><mml:mo>*</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
    provides the rough correspondence between the embeddings of qualia
    structures, we should find a more precise mathematical mapping
    between qualia structures in terms of their vector embeddings to
    more accurately assess the similarity between the qualia structures.
    Here, we consider aligning the embeddings of all the groups in a
    common space.</p>
    <p>By applying MDS, we obtained the 3-dimensional embeddings of
    Group 1 and Groups 2-5, referred to as X and Yi, where i = 2, …, 5
    (Fig. 3c). We then aligned Yi to X with the orthogonal rotation
    matrix Qi, which was obtained by solving a Procrustes-type problem
    using the optimized transportation plan
    <inline-formula><alternatives>
    <tex-math><![CDATA[\Gamma*]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>Γ</mml:mi><mml:mo>*</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
    obtained through GWOT (see Methods). Fig. 3d shows the aligned
    embed- dings of Group 2-5 (QiYi) and the embedding of Group 1 (X)
    plotted in the embedded space of X. Each color represents the label
    of a corresponding external color stimulus. Note that even though
    the color labels are shown in Fig. 3d, this is only for the
    visualization purpose and the whole alignment procedure is performed
    in a purely unsupervised manner without relying on the color labels.
    As can be seen in Fig. 3d, the embeddings of similar colors from the
    five groups are located close to each other, indicating that similar
    colors are ‘correctly’ aligned by the unsupervised alignment
    method.</p>
    <p>To evaluate the performance of the unsupervised alignment, we
    computed the k-nearest color matching rate in the aligned space. If
    the same colors from two groups are within the k-nearest colors in
    the aligned space, we consider that the colors are correctly
    matched. We evaluated the matching rates between all the pairs of
    Groups 1-5. The averaged matching rates are 51% when k = 1, 83% when
    k = 3, and 92% when k = 5, respectively. This demonstrates the
    effectiveness of the GW alignment for correctly aligning the qualia
    structures of different participants in an unsupervised manner.</p>
    <p>However, as can be seen in Fig. 4b, the optimized mapping
    <inline-formula><alternatives>
    <tex-math><![CDATA[\Gamma*]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>Γ</mml:mi><mml:mo>*</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
    is not lined up diagonally unlike the optimized map- pings between
    color-neurotypical participants groups shown in Fig. 3b (see
    Supplementary Figure S1 for the other pairs). Accordingly, top k
    matching rate between Group 1-5 and Group 6 is 3.0% when k = 1 (Fig.
    4c), which is only slightly above chance
    (<inline-formula><alternatives>
    <tex-math><![CDATA[\approx]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mo>≈</mml:mo></mml:math></alternatives></inline-formula>
    1%). The matching rate did not improve even when we relaxed the
    criterion (6.9% and 11% for k = 3 and k = 5, respectively).
    Moreover, all of the GWD values between Group 1-5 and Group 6 are
    larger than any of the GWD values between color-neurotypical
    participant groups (Fig. 4d).</p>
    <p>These results indicate that the difference between the qualia
    structures of neuro-typical and atypical participants is
    significantly larger than the difference between the qualia
    structures of neuro-typical participants.</p>
    <fig>
      <caption><p>The two conditions for one subject.</p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="images/my-protocol-1.png" />
    </fig>
    <fig>
      <caption><p>The comparison between the representational structure
      of aphantasics and phantasics. This figure illustrates the
      principle, but in reality all pairs of subjects will be compared
      to assess their representational structure alignment. This is
      computationnally heavy, but analytically very
      powerful.</p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="images/my-protocol-2.png" />
    </fig>
  </sec>
  <sec id="hypotheses">
    <title>3.3 Hypotheses</title>
    <sec id="aphantasic-and-phantasic-psychological-spaces">
      <title>Aphantasic and phantasic psychological spaces</title>
      <p>The most representative members of a category are called
      prototypical members.</p>
      <p>Prototype theory builds on the observation that among the
      instances of a property, some are more representative than others.
      The most representative one is the prototype of the property.</p>
      <p>Thus, following the concepts illustrated by Gardenfors
      (<xref alt="2004b" rid="ref-gardenfors2004" ref-type="bibr">2004b</xref>),
      we would expect that aphantasics, when doing shape similarity
      judgements, would be more inclined to group items close to the
      prototypical items due to a lower definition of the mental image.
      In comparison, phantasics would have a much more distributed
      conceptual space of item shapes due to their higher-resolution
      mental images of said items.</p>
    </sec>
    <sec id="subjective-imagery-and-psychological-spaces">
      <title>Subjective imagery and psychological spaces</title>
      <p>In the proposed view of visual imagery as the subjective
      expression of a given type of psychological space, we mentioned
      earlier that <italic>spatial</italic> imagery could also
      constitute a subjective expression of other dimensions of
      psychological spaces. Hence, the <italic>verbal</italic> dimension
      of the simplified model of imagery we outlined in my thesis
      project could also represent different dimensions.</p>
      <p>This conception leads to the following theoretical hypothesis:
      provided that our visual-spatial-verbal model correctly fits
      subjective imagery, the imagery profile of individuals should map
      on their psychological spaces.</p>
      <p>Operationally, this would be evaluated by the fact that
      <bold>individuals with similar imagery profiles</bold> (visual,
      spatial, verbal, or any combination of the three) <bold>should
      have similar representations</bold> in their given psychological
      space, <bold>quantifiable by the degree of alignment between their
      similarity structures.</bold></p>
    </sec>
  </sec>
</sec>
<sec id="study-simulation-and-analysis">
  <title>4. Study simulation and analysis</title>
  <sec id="visual-spatial-verbal-model-of-cognitive-profiles">
    <title>4.1 Visual-spatial-verbal model of cognitive profiles</title>
    <p>One of the objectives of the study would be to link the
    subjective cognitive profiles of individuals with their
    representational structures. To evaluate these profiles, we are
    going to use psychometric questionnaires evaluating the
    visual-object, spatial, and verbal dimensions of imagery which will
    yield three scores, one for each dimension.</p>
    <p>We are going to simulate 30 participants presenting four
    different cognitive profiles, that I defined as, respectively,
    <italic>verbal</italic> aphantasics, <italic>spatial</italic>
    aphantasics, <italic>spatial</italic> phantasics, and
    <italic>visual</italic> phantasics. Their imagery abilities are
    summarised in
    <xref alt="Table 4.1" rid="tbl-imageries">Table 4.1</xref>.</p>
    <p>To simulate these four sub-groups, we use the
    <monospace>holodeck</monospace> R package to generate multivariate
    normal distributions of scores on these three dimensions for each
    sub-group. For instance, verbal aphantasics have normally
    distributed visual imagery scores centered around a mean of 0
    (normalized, so negative scores are possible), 0.4 for spatial
    imagery, and 0.7 for verbal style; Spatial aphantasics have means of
    0 for visual, 0.75 spatial, and 0.3 for verbal; etc. The numbers are
    arbitrary, but have been chosen by trial-and-error to obtain a model
    that is both well-defined and not exaggerated. The 30 subjects’
    imagery profiles are represented in the three dimensional space of
    the visual-spatial-verbal dimensions in
    <xref alt="Figure 4.1" rid="fig-plot-osv-model">Figure 4.1</xref>.</p>
    <fig id="tbl-imageries">
      <caption><p>Table 4.1: Imagery abilities of the four hypothesized
      cognitive profiles.</p></caption>
      <table-wrap>
        <table>
          <thead>
            <tr>
              <th>Cognitive profile</th>
              <th align="center">Visual imagery</th>
              <th align="center">Spatial imagery</th>
              <th align="center">Verbal style</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Verbal aphantasic</td>
              <td align="center">–</td>
              <td align="center">-</td>
              <td align="center">++</td>
            </tr>
            <tr>
              <td>Spatial aphantasic</td>
              <td align="center">–</td>
              <td align="center">++</td>
              <td align="center">-</td>
            </tr>
            <tr>
              <td>Spatial phantasic</td>
              <td align="center">+</td>
              <td align="center">++</td>
              <td align="center">-</td>
            </tr>
            <tr>
              <td>Visual phantasic</td>
              <td align="center">++</td>
              <td align="center">-</td>
              <td align="center">+</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </fig>
    <fig id="fig-plot-osv-model">
      <caption><p>Figure 4.1: Imagery profiles generated for 30 subjects
      on the three object, spatial, and verbal dimensions.</p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-plot-osv-model-1.png" />
    </fig>
  </sec>
  <sec id="data-simulation-creating-representational-structures">
    <title>4.2 Data simulation: Creating representational
    structures</title>
    <p>Gardenfors
    (<xref alt="2004b" rid="ref-gardenfors2004" ref-type="bibr">2004b</xref>)
    invokes two scientific concepts, to wit, prototypes and Voronoi
    tessellations. Prototype theory builds on the observation that among
    the instances of a property, some are more representative than
    others. The most representative one is the prototype of the
    property. <italic>We hypothesize that aphantasics will be more
    inclined to categorize items according to prototypes than
    phantasics.</italic></p>
    <p>A Voronoi tesselation of a given space divides that space into a
    number of cells such that each cell has a center and consists of all
    and only those points that lie no closer to the center of any other
    cell than to its own center; the centers of the various cells are
    called the generator points of the tesselation. This principle will
    underlie our data simulation, as we will build representations in a
    3D space based on distances to “centroids”, namely, prototypes.
    These representations will thus be located inside of the
    tessellations around these prototypes, more or less close to the
    centroid depending on the subject’s representational structures.</p>
    <sec id="generating-prototype-embeddings-from-a-sphere">
      <title>Generating “prototype” embeddings from a sphere</title>
      <p>A function will be used to generate embeddings. These spherical
      embeddings are displayed in
      <xref alt="Figure 4.2" rid="fig-perfect-embeddings">Figure 4.2</xref>
      We get 8 nicely distributed clusters. We’ll retrieve the centroids
      of each cluster, which would be the “perfect” categories of each
      species group (say, generated by a computational model on
      categorical criteria).</p>
      <code language="python">generate_sphere &lt;- function(n){
  z     &lt;- 2*runif(n) - 1          # uniform on [-1, 1]
  theta &lt;- 2*pi*runif(n) - pi      # uniform on [-pi, pi]
  x     &lt;- sin(theta)*sqrt(1-z^2)  # based on angle
  y     &lt;- cos(theta)*sqrt(1-z^2) 
  
  df &lt;- tibble(x = x, y = y, z = z)
  
  return(df)
}

# 1000 random observations with embeddings uniformly distributed on a sphere
df_embeds &lt;- generate_sphere(1000)

# Clustering the observations in 8 groups based on their coordinates
clusters &lt;- Mclust(df_embeds, G = 8)

# adding the classification to the data
df_embeds &lt;- df_embeds |&gt; mutate(group = as.factor(clusters$classification))

# getting the centroids of each cluster
df_centroids &lt;- 
  df_embeds |&gt; 
  group_by(group) |&gt; 
  summarise(
    x_centroid = mean(x),
    y_centroid = mean(y),
    z_centroid = mean(z)
  )

# adding them to the data
df_embeds_2 &lt;- left_join(df_embeds, df_centroids, by = &quot;group&quot;)</code>
      <fig id="fig-perfect-embeddings">
        <caption><p>Figure 4.2: Generated spherical distribution of 1000
        observations grouped in 8 equal clusers with Gaussian Mixture
        Clustering to represent the theoretical embeddings of 8 groups
        (i.e. groups of species here). <bold><italic>Interact with the
        figures to see the details.</italic></bold></p></caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-perfect-embeddings-1.png" />
      </fig>
      <p>Now we want two sets of embeddings: one where the observations
      are very concentrated around the centroids, which would be the
      <bold>categorical model</bold>, and one where the observations are
      more spread out, which would be the <bold>visual model</bold>.</p>
      <p>We need to select 8 observations per cluster, which would be
      our animals per group. These observations will be subsets of the
      1000 observations we generated.</p>
    </sec>
    <sec id="categorical-model-embeddings">
      <title>Categorical model embeddings</title>
      <p>The selection procedure for the <bold>categorical model</bold>
      will consist of selecting points that are rather <italic>close to
      the centroids</italic>. Thus, we will filter the observations of
      the large sets to keep only points for which the distance to the
      centroid is inferior to a given value. That is, points for which
      the Euclidean norm of the vector from the observation to the
      centroid:</p>
      <p><disp-formula><alternatives>
      <tex-math><![CDATA[d(centroid, observation) = \sqrt{(x_{c} - x_{o})^{2} + (y_{c} - y_{o})^{2} + (z_{c} - z_{o})^{2}}]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>d</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:math></alternatives></disp-formula></p>
      <p>This can be done using the function
      <monospace>norm(coordinates, type = &quot;2&quot;)</monospace> in
      R.</p>
      <code language="python"># Function to filter points of the sphere based on the distance to the centroids
generate_embeddings &lt;- function(df, n_embeddings, distance_quantile){
  df &lt;- 
    df |&gt; 
    # computing the euclidean distance to the centroids for each observation
    rowwise() |&gt; 
    mutate(
      distance = norm(
        c((x_centroid - x), (y_centroid - y), (z_centroid - z)),
        type = &quot;2&quot;)
      ) |&gt; 
    # filtering by distance to the centroid by group
    group_by(group) |&gt; 
    # selecting the X% closest (specified with &quot;distance_quantile&quot;)
    filter(distance &lt; quantile(distance, probs = distance_quantile)) |&gt; 
    # selecting X random observations per cluster in these 
    # (specified with &quot;n_embeddings&quot;)
    slice(1:n_embeddings) |&gt; 
    select(group, x, y, z) |&gt;
    ungroup()
}

df_embeds_categ &lt;- generate_embeddings(df_embeds_2, 8, 0.5)</code>
      <fig id="fig-categorical-embeddings">
        <caption><p>Figure 4.3: Selection of 64 points to represent
        prototypical categorical embeddings, based on the distances to
        each groups’ centroid. These will be the bases of the verbal
        aphantasics’ embeddings. <bold><italic>Interact with the figure
        to see the details.</italic></bold></p></caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-categorical-embeddings-1.png" />
      </fig>
    </sec>
    <sec id="visual-model-embeddings">
      <title>Visual model embeddings</title>
      <p>In the case of the <bold>visual model</bold>, we would like
      approximately evenly distributed embeddings, that could also dive
      <italic>inside</italic> the sphere, i.e. representing species that
      are visually close although diametrically opposed when it comes to
      taxonomy. To do this we could try to simulate multivariate normal
      distributions around the
      centroids<xref ref-type="fn" rid="fn2">2</xref>. This can be done
      with the <monospace>holodeck</monospace> package.</p>
      <code language="python"># defining the variance and covariance of the distributions
var2 &lt;- 0.05
cov2 &lt;- 0

# generating multivariate distributions around the categorical 3D means
df_embeds_visual &lt;-
  tibble(
    id = as.factor(seq(1,6400)),
    category = as.factor(rep(seq(1:64), each = 100))
  )|&gt; 
  group_by(category) |&gt; 
  sim_discr(
    n_vars = 1, 
    var = var2, 
    cov = cov2, 
    group_means = df_embeds_categ$x, 
    name = &quot;x&quot;) |&gt; 
  sim_discr(
    n_vars = 1, 
    var = var2, 
    cov = cov2, 
    group_means = df_embeds_categ$y, 
    name = &quot;y&quot;) |&gt; 
  sim_discr(
    n_vars = 1, 
    var = var2, 
    cov = cov2, 
    group_means = df_embeds_categ$z, 
    name = &quot;z&quot;) |&gt; 
  # keeping only 8 points per distribution
  slice(1) |&gt; 
  ungroup() |&gt; 
  mutate(group = as.factor(rep(seq(1, 8), each = 8))) |&gt; 
  rename(x = x_1, y = y_1, z = z_1) |&gt; 
  select(group, x, y, z)</code>
      <fig id="fig-visual-embeddings">
        <caption><p>Figure 4.4: Selection of 64 points to represent
        prototypical visual embeddings, chosen randomly in multivariate
        distributions centered around each categorical embedding. The
        visual embeddings are overlaid as diamonds along with
        categorical ones as dots. The two distributions keep the group
        structure, but are pretty far apart at times.
        <bold><italic>Interact with the figure to see the
        details.</italic></bold></p></caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-visual-embeddings-1.png" />
      </fig>
    </sec>
    <sec id="intermediate-embeddings">
      <title>Intermediate embeddings</title>
      <fig id="fig-distances-graph">
        <caption><p>Figure 4.5: Model of the distances between
        participants’ representations. Note that here d is a
        one-dimensional distance between the representations, but it
        will be computed as a three-dimensional distance in our
        toy-model. The verbal aphantasic profile is hypothesized to be
        very categorical, thus diametrically opposed to the visual
        phantasic profile, by a given distance d. Spatial profiles are
        in-between: they are close to each other (10% x d), but the
        spatial aphantasic profile is a bit closer to the verbal
        aphantasic one (45% x d), and the spatial phantasic is a bit
        closer to the visual phantasic one (45% x d).</p></caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/mermaid-figure-2.png" />
      </fig>
      <code language="r script">dist_c = 0.45
dist_v = 0.55

df_embeddings &lt;-
  df_embeds_categ |&gt;
  rename(
    group_c = group,
    x_c = x,
    y_c = y,
    z_c = z
  ) |&gt;
  bind_cols(df_embeds_visual) |&gt;
  rename(
    group_v = group,
    x_v = x,
    y_v = y,
    z_v = z
  ) |&gt; 
  select(!group_v) |&gt; 
  rename(group = group_c) |&gt; 
  mutate(
    x_cs = x_c + dist_c*(x_v - x_c),
    y_cs = y_c + dist_c*(y_v - y_c),
    z_cs = z_c + dist_c*(z_v - z_c),
    x_vs = x_c + dist_v*(x_v - x_c),
    y_vs = y_c + dist_v*(y_v - y_c),
    z_vs = z_c + dist_v*(z_v - z_c)
  )</code>
      <fig id="fig-intermediate-embeddings">
        <caption><p>Figure 4.6: Space of embeddings with 128 additional
        points based on the euclidean distances between the visual and
        categorical embeddings. The empty dots are the
        <italic>aphantasics-spatial</italic> ones, and the empty
        diamonds are the <italic>phantasic-spatial</italic> ones. Some
        can be very close together, and sometimes further apart due to
        the various pairs of visual and categorical points used to
        create them. A network-like structure seems to appear, with
        empty points seemingly ‘connecting’ the dots and diamonds.
        <bold><italic>Interact with the figure to see the
        details.</italic></bold></p></caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-intermediate-embeddings-1.png" />
      </fig>
    </sec>
    <sec id="labelling-the-species">
      <title>Labelling the species</title>
      <p>The distributions created are still gathered around the
      centroids of each group, but are much more widespread, each group
      getting close to each other and even reaching inside the
      sphere.</p>
      <p>Perfect! Now we have two 3D embeddings per animal, in a
      categorical or a visual description of their features. Let’s add
      labels for each species in a group:</p>
      <code language="r script">df_embeddings &lt;- 
  df_embeddings |&gt; 
  mutate(
    group = case_when(
    group == 1 ~ &quot;a&quot;, 
    group == 2 ~ &quot;b&quot;,
    group == 3 ~ &quot;c&quot;,
    group == 4 ~ &quot;d&quot;,
    group == 5 ~ &quot;e&quot;,
    group == 6 ~ &quot;f&quot;,
    group == 7 ~ &quot;g&quot;,
    group == 8 ~ &quot;h&quot;,
    TRUE ~ group
    )
  ) |&gt; 
  group_by(group) |&gt; 
  mutate(
    species = paste0(&quot;species_&quot;, group, 1:8),
    species = as.factor(species),
    group   = as.factor(group)
    ) |&gt; 
  select(group, species, everything())</code>
      <p>Now we have four sets of coherent coordinates, that we need to
      assign to the 30 participants: i.e. generating 8 points for C
      (aph_spa_low), 7 points for CS (aph_spa_high), 7 points for VS
      (phant_spa_high), and 8 points for V (phant_spa_low).</p>
    </sec>
    <sec id="generating-the-subject-embeddings">
      <title>Generating the subject embeddings</title>
      <p>We have four “reference” sets of embeddings which represent
      animals either judged according to their similarity in categorical
      terms (namely, species), or in visual terms (namely shape or color
      similarities, assuming that these similarities are more evenly
      distributed, e.g. the crab looks like a spider, but is also pretty
      close to a scorpion, etc.).</p>
      <p>To generate the embeddings of each subject in each condition,
      we will start from these reference embeddings and generate random
      noise around <italic>each item</italic>, i.e. for all 64 animals.
      For 100 subjects, we would thus generate 100 noisy points around
      each animal, each point corresponding to a given subject.</p>
      <p>The visual and verbal groups will be generated with slightly
      more intra-group variance, so as to try to make the spatial groups
      as coherent as possible (and avoid blurring everything and making
      the groups disappear in noise).</p>
      <p>Although the groupings in this distribution sound simple when
      we color it using the knowledge about how we built it, the
      algorithm will only be fed with the data for each subject, without
      any labeling or additional information. Thus,
      <xref alt="Figure 4.8" rid="fig-subject-embeddings-b">Figure 4.8</xref>
      here is what the algorithm will “see” (and what it will try to
      decrypt). Admittedly, that looks a lot more complicated.</p>
      <code language="python"># creating dfs with participants
df_subjects_7 &lt;- 
  tibble(subject = seq(1, 7, 1)) |&gt; 
  mutate(subject = paste0(&quot;subject_&quot;, subject))

df_subjects_8 &lt;- 
  tibble(subject = seq(1, 8, 1)) |&gt; 
  mutate(subject = paste0(&quot;subject_&quot;, subject))

# splitting df_embeddings
df_embed_c  &lt;- df_embeddings |&gt; select(group, species,  x_c:z_c)
df_embed_cs &lt;- df_embeddings |&gt; select(group, species, x_cs:z_cs)
df_embed_vs &lt;- df_embeddings |&gt; select(group, species, x_vs:z_vs)
df_embed_v  &lt;- df_embeddings |&gt; select(group, species,  x_v:z_v)

# function to create embeddings per subject with normal random noise
generate_subject_embeddings &lt;- function(df, df_subjects, var){
  df &lt;-
    df |&gt; 
    mutate(subject = list(df_subjects)) |&gt; 
    unnest(subject) |&gt; 
    group_by(species) |&gt; 
    # simulating x coordinates
    sim_discr(
      n_vars = 1, 
      var = var, 
      cov = 0, 
      group_means = pull(df, 3), 
      name = &quot;x&quot;) |&gt; 
    # simulating y coordinates
    sim_discr(
      n_vars = 1, 
      var = var, 
      cov = 0, 
      group_means = pull(df, 4), 
      name = &quot;y&quot;) |&gt; 
    # simulating z coordinates
    sim_discr(
      n_vars = 1, 
      var = var, 
      cov = 0, 
      group_means = pull(df, 5), 
      name = &quot;z&quot;) |&gt;
  select(group, species, subject, 7:9) |&gt; 
  rename(x = 4, y = 5, z = 6) |&gt; 
  ungroup()
  
  return(df)
}

var_s1 = 0.001
var_s2 = 0.0005

df_embed_c_sub  &lt;- generate_subject_embeddings(df_embed_c,  df_subjects_4, var_s1)
df_embed_cs_sub &lt;- generate_subject_embeddings(df_embed_cs, df_subjects_4, var_s2)
df_embed_vs_sub &lt;- generate_subject_embeddings(df_embed_vs, df_subjects_4, var_s2)
df_embed_v_sub  &lt;- generate_subject_embeddings(df_embed_v,  df_subjects_4, var_s1)</code>
      <fig id="fig-subject-embeddings-a">
        <caption><p>Figure 4.7: Final distribution of the 64 embeddings
        of all the 30 subjects, amounting to 1920 points total.
        Embeddings are <bold><italic>colored by the species
        groups</italic></bold> they represent. The symbols represent the
        four imagery groups (Aph. verbal, spatial, etc.).
        <bold><italic>Interact with the figures to see the
        details.</italic></bold></p></caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-subject-embeddings-a-1.png" />
      </fig>
      <fig id="fig-subject-embeddings-b">
        <caption><p>Figure 4.8: Final distribution of the 64 embeddings
        of all the 30 subjects, amounting to 1920 points total.
        Embeddings are <bold><italic>lored by subject</italic></bold>.
        The symbols represent the four imagery groups (Aph. verbal,
        spatial, etc.). <bold><italic>Interact with the figures to see
        the details.</italic></bold></p></caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-subject-embeddings-b-1.png" />
      </fig>
      <p>To feed this data to the algorithm, we’ll group the 64
      embeddings per subject in matrices tied to each of them.</p>
      <code language="r script">df_embeddings_sub &lt;-
  bind_rows(
    # aphantasic spatial
    df_embed_cs_sub |&gt; 
      separate_wider_delim(
        subject,
        delim = &quot;_&quot;,
        names = c(&quot;subject&quot;, &quot;number&quot;)
      ) |&gt; 
      mutate(subject = paste0(subject,&quot;_a_aph_s&quot;, number), .keep = &quot;unused&quot;),
    
    # aphantasic verbal
    df_embed_c_sub  |&gt;
      separate_wider_delim(
        subject,
        delim = &quot;_&quot;,
        names = c(&quot;subject&quot;, &quot;number&quot;)
      ) |&gt; 
      mutate(subject = paste0(subject,&quot;_b_aph_v&quot;, number), .keep = &quot;unused&quot;),
    
    # phantasic spatial
    df_embed_vs_sub  |&gt;
      separate_wider_delim(
        subject,
        delim = &quot;_&quot;,
        names = c(&quot;subject&quot;, &quot;number&quot;)
      ) |&gt; 
      mutate(subject = paste0(subject,&quot;_c_phant_s&quot;, number), .keep = &quot;unused&quot;),
    
    # phantasic visual
    df_embed_v_sub  |&gt;
      separate_wider_delim(
        subject,
        delim = &quot;_&quot;,
        names = c(&quot;subject&quot;, &quot;number&quot;)
      ) |&gt; 
      mutate(subject = paste0(subject,&quot;_d_phant_v&quot;, number), .keep = &quot;unused&quot;)
  ) |&gt; 
  mutate(subject = as.factor(subject)) |&gt; 
  select(!c(group, species)) |&gt; 
  group_by(subject) |&gt; 
  nest() |&gt; 
  rename(embedding = data) |&gt; 
  rowwise() |&gt; 
  mutate(embedding = list(as.matrix(embedding)))</code>
    </sec>
  </sec>
  <sec id="data-analysis-aligning-representational-structures">
    <title>4.3 Data analysis: Aligning representational
    structures</title>
  </sec>
  <sec id="simulation-summary">
    <title>4.4 Simulation summary</title>
    <p>Kawakita et al.
    (<xref alt="2023" rid="ref-kawakita2023" ref-type="bibr">2023</xref>):
    These results indicate that the difference between the qualia
    structures of neuro-typical and atypical participants is
    significantly larger than the difference between the qualia
    structures of neuro-typical participants.</p>
    <p>A notable difference is that greenish colors and reddish colors
    are close in the embedding space of color atypical participants
    while they are distant in the embedding space of color neurotypical
    participants. This structural difference is likely to prevent the
    unsupervised alignment between the embeddings of color-neurotypical
    and atypical participants even though the correlation coefficient
    between the dissimilarity matrices of color neuro-typical and
    atypical participants is reasonably high.</p>
    <p>For a long time, assessing the similarity of subjective
    experiences across participants has been challenging. To address
    this problem, we proposed the “qualia structure” paradigm, which
    focuses on quantitative structural comparisons of subjective
    experiences. Using an unsupervised alignment method, we were able to
    match the qualia structures of colors and natural objects of
    different groups of participants based only on the way the qualia
    relate to each other, without using any external labels.</p>
    <p>Our results on color qualia structures are consistent with an
    idea that the relational properties of color qualia are universally
    shared by color-neurotypical individuals. Intriguingly, our results
    also suggest that individuals with color-atypical vision may have a
    different structure of their color experiences, rather than just
    failing to experience a certain subset of colors. Longstanding
    thought experiments that challenge the feasibility of
    inter-subjective color comparisons, such as individuals with color
    qualia inversion, should be resolvable with our relational
    unsupervised approach. Beyond traditional measures such as Pearson’s
    correlation coefficient, our method provides a more fundamental
    structural characterization of how two structures are similar or
    different, which will be crucial for future investigations of qualia
    structures across psychological, neuroscientific, and computational
    fields.</p>
  </sec>
</sec>
<sec id="feasibility">
  <title>5. Feasibility</title>
</sec>
<sec id="conclusion">
  <title>6. Conclusion</title>
  <p>Modern psychology builds on the relativistic framework of
  philosophy, accepting that humans cannot know reality in an absolute
  sense. Focusing on relative comparisons, or similarity, is more than a
  clever philosophical work-around. similarity is a common currency of
  perception and cognition. In addition to operating at all levels of
  cognition, similarity—or, more accurately, the second-order
  isomorphism defined by a set of similarity relations—has been a
  powerful tool for analyzing and comparing psychological spaces.</p>
</sec>
</body>

<back>
<ref-list>
  <title></title>
  <ref id="ref-roads2024">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Roads</surname><given-names>Brett D.</given-names></name>
        <name><surname>Love</surname><given-names>Bradley C.</given-names></name>
      </person-group>
      <article-title>Modeling similarity and psychological space</article-title>
      <source>Annual Review of Psychology</source>
      <year iso-8601-date="2024-01">2024</year><month>01</month>
      <volume>75</volume>
      <issue>1</issue>
      <issn>0066-4308, 1545-2085</issn>
      <uri>https://www.annualreviews.org/doi/10.1146/annurev-psych-040323-115131</uri>
      <pub-id pub-id-type="doi">10.1146/annurev-psych-040323-115131</pub-id>
      <fpage>215</fpage>
      <lpage>240</lpage>
    </element-citation>
  </ref>
  <ref id="ref-shepardSecondorderIsomorphismInternal1970">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Shepard</surname><given-names>Roger N</given-names></name>
        <name><surname>Chipman</surname><given-names>Susan</given-names></name>
      </person-group>
      <article-title>Second-order isomorphism of internal representations: Shapes of states</article-title>
      <source>Cognitive Psychology</source>
      <year iso-8601-date="1970-01">1970</year><month>01</month>
      <volume>1</volume>
      <issue>1</issue>
      <issn>0010-0285</issn>
      <uri>https://www.sciencedirect.com/science/article/pii/0010028570900022</uri>
      <pub-id pub-id-type="doi">10.1016/0010-0285(70)90002-2</pub-id>
      <fpage>1</fpage>
      <lpage>17</lpage>
    </element-citation>
  </ref>
  <ref id="ref-sasakiToolboxGromovWassersteinOptimal2023">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Sasaki</surname><given-names>Masaru</given-names></name>
        <name><surname>Takeda</surname><given-names>Ken</given-names></name>
        <name><surname>Abe</surname><given-names>Kota</given-names></name>
        <name><surname>Oizumi</surname><given-names>Masafumi</given-names></name>
      </person-group>
      <article-title>Toolbox for gromov-wasserstein optimal transport: Application to unsupervised alignment in neuroscience</article-title>
      <year iso-8601-date="2023">2023</year>
      <uri>https://www.biorxiv.org/content/10.1101/2023.09.15.558038v1</uri>
      <pub-id pub-id-type="doi">10.1101/2023.09.15.558038</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-bainbridgeQuantifyingAphantasiaDrawing2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bainbridge</surname><given-names>Wilma A.</given-names></name>
        <name><surname>Pounder</surname><given-names>Zoë</given-names></name>
        <name><surname>Eardley</surname><given-names>Alison F.</given-names></name>
        <name><surname>Baker</surname><given-names>Chris I.</given-names></name>
      </person-group>
      <article-title>Quantifying aphantasia through drawing: Those without visual imagery show deficits in object but not spatial memory</article-title>
      <source>Cortex</source>
      <year iso-8601-date="2021-02-01">2021</year><month>02</month><day>01</day>
      <volume>135</volume>
      <uri>https://www.sciencedirect.com/science/article/pii/S0010945220304317</uri>
      <pub-id pub-id-type="doi">10.1016/j.cortex.2020.11.014</pub-id>
      <fpage>159</fpage>
      <lpage>172</lpage>
    </element-citation>
  </ref>
  <ref id="ref-kriegeskorteInverseMDSInferring2012">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name>
        <name><surname>Mur</surname><given-names>Marieke</given-names></name>
      </person-group>
      <article-title>Inverse MDS: Inferring dissimilarity structure from multiple item arrangements</article-title>
      <source>Frontiers in Psychology</source>
      <year iso-8601-date="2012">2012</year>
      <volume>3</volume>
      <uri>https://www.frontiersin.org/articles/10.3389/fpsyg.2012.00245</uri>
    </element-citation>
  </ref>
  <ref id="ref-decockSimilarityGoodman2011">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Decock</surname><given-names>Lieven</given-names></name>
        <name><surname>Douven</surname><given-names>Igor</given-names></name>
      </person-group>
      <article-title>Similarity After Goodman</article-title>
      <source>Review of Philosophy and Psychology</source>
      <year iso-8601-date="2011-03-01">2011</year><month>03</month><day>01</day>
      <volume>2</volume>
      <issue>1</issue>
      <uri>https://doi.org/10.1007/s13164-010-0035-y</uri>
      <pub-id pub-id-type="doi">10.1007/s13164-010-0035-y</pub-id>
      <fpage>61</fpage>
      <lpage>75</lpage>
    </element-citation>
  </ref>
  <ref id="ref-goodmanSevenStricturesSimilarity1972">
    <element-citation publication-type="chapter">
      <person-group person-group-type="author">
        <name><surname>Goodman</surname><given-names>Nelson</given-names></name>
      </person-group>
      <article-title>Seven strictures on similarity</article-title>
      <publisher-name>Bobs-Merril</publisher-name>
      <year iso-8601-date="1972">1972</year>
    </element-citation>
  </ref>
  <ref id="ref-gardenforsConceptualSpacesFramework2004">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Gardenfors</surname><given-names>Peter</given-names></name>
      </person-group>
      <article-title>Conceptual spaces as a framework for knowledge representation</article-title>
      <year iso-8601-date="2004">2004</year>
    </element-citation>
  </ref>
  <ref id="ref-murHumanObjectSimilarityJudgments2013">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Mur</surname><given-names>Marieke</given-names></name>
        <name><surname>Meys</surname><given-names>Mirjam</given-names></name>
        <name><surname>Bodurka</surname><given-names>Jerzy</given-names></name>
        <name><surname>Goebel</surname><given-names>Rainer</given-names></name>
        <name><surname>Bandettini</surname><given-names>Peter</given-names></name>
        <name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name>
      </person-group>
      <article-title>Human object-similarity judgments reflect and transcend the primate-IT object representation</article-title>
      <source>Frontiers in Psychology</source>
      <year iso-8601-date="2013">2013</year>
      <volume>4</volume>
      <uri>https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00128</uri>
    </element-citation>
  </ref>
  <ref id="ref-majewskaSpatialMultiarrangementClustering2020">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Majewska</surname><given-names>O.</given-names></name>
        <name><surname>McCarthy</surname><given-names>D.</given-names></name>
        <name><surname>Bosch</surname><given-names>J. van den</given-names></name>
        <name><surname>Kriegeskorte</surname><given-names>N.</given-names></name>
        <name><surname>Vulic</surname><given-names>I.</given-names></name>
        <name><surname>Korhonen</surname><given-names>A.</given-names></name>
      </person-group>
      <source>Spatial multi-arrangement for clustering and multi-way similarity dataset construction</source>
      <publisher-name>European Language Resources Association</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <uri>https://www.repository.cam.ac.uk/handle/1810/306834</uri>
    </element-citation>
  </ref>
  <ref id="ref-gardenfors2004">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Gardenfors</surname><given-names>Peter</given-names></name>
      </person-group>
      <article-title>Conceptual Spaces as a Framework for Knowledge Representation</article-title>
      <year iso-8601-date="2004">2004</year>
    </element-citation>
  </ref>
  <ref id="ref-jozwik2016">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Jozwik</surname><given-names>Kamila M.</given-names></name>
        <name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name>
        <name><surname>Mur</surname><given-names>Marieke</given-names></name>
      </person-group>
      <article-title>Visual features as stepping stones toward semantics: Explaining object similarity in IT and perception with non-negative least squares</article-title>
      <source>Neuropsychologia</source>
      <year iso-8601-date="2016-03-01">2016</year><month>03</month><day>01</day>
      <volume>83</volume>
      <uri>https://www.sciencedirect.com/science/article/pii/S0028393215301998</uri>
      <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2015.10.023</pub-id>
      <fpage>201</fpage>
      <lpage>226</lpage>
    </element-citation>
  </ref>
  <ref id="ref-jozwik2017">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Jozwik</surname><given-names>Kamila M.</given-names></name>
        <name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name>
        <name><surname>Storrs</surname><given-names>Katherine R.</given-names></name>
        <name><surname>Mur</surname><given-names>Marieke</given-names></name>
      </person-group>
      <article-title>Deep convolutional neural networks outperform feature-based but not categorical models in explaining object similarity judgments</article-title>
      <source>Frontiers in Psychology</source>
      <year iso-8601-date="2017">2017</year>
      <volume>8</volume>
      <uri>https://www.frontiersin.org/articles/10.3389/fpsyg.2017.01726</uri>
    </element-citation>
  </ref>
  <ref id="ref-marr1997">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Marr</surname><given-names>D.</given-names></name>
        <name><surname>Nishihara</surname><given-names>H. K.</given-names></name>
        <name><surname>Brenner</surname><given-names>Sydney</given-names></name>
      </person-group>
      <article-title>Representation and recognition of the spatial organization of three-dimensional shapes</article-title>
      <source>Proceedings of the Royal Society of London. Series B. Biological Sciences</source>
      <year iso-8601-date="1997-01">1997</year><month>01</month>
      <volume>200</volume>
      <issue>1140</issue>
      <uri>https://royalsocietypublishing.org/doi/abs/10.1098/rspb.1978.0020</uri>
      <pub-id pub-id-type="doi">10.1098/rspb.1978.0020</pub-id>
      <fpage>269</fpage>
      <lpage>294</lpage>
    </element-citation>
  </ref>
  <ref id="ref-kriegeskorte2008">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name>
        <name><surname>Mur</surname><given-names>Marieke</given-names></name>
        <name><surname>Bandettini</surname><given-names>Peter</given-names></name>
      </person-group>
      <article-title>Representational similarity analysis - connecting the branches of systems neuroscience</article-title>
      <source>Frontiers in Systems Neuroscience</source>
      <year iso-8601-date="2008">2008</year>
      <volume>2</volume>
      <uri>https://www.frontiersin.org/articles/10.3389/neuro.06.004.2008</uri>
    </element-citation>
  </ref>
  <ref id="ref-kawakita2023">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kawakita</surname><given-names>Genji</given-names></name>
        <name><surname>Zeleznikow-Johnston</surname><given-names>Ariel</given-names></name>
        <name><surname>Takeda</surname><given-names>Ken</given-names></name>
        <name><surname>Tsuchiya</surname><given-names>Naotsugu</given-names></name>
        <name><surname>Oizumi</surname><given-names>Masafumi</given-names></name>
      </person-group>
      <article-title>Is my&quot; red&quot; your&quot; red&quot;?: Unsupervised alignment of qualia structures via optimal transport</article-title>
      <year iso-8601-date="2023">2023</year>
    </element-citation>
  </ref>
</ref-list>
<fn-group>
  <fn id="fn1">
    <label>1</label><p>A claim dismissed since then by propositions of
    robust mathematical models of similarity, e.g. Gardenfors
    (<xref alt="2004a" rid="ref-gardenforsConceptualSpacesFramework2004" ref-type="bibr">2004a</xref>),
    Decock and Douven
    (<xref alt="2011" rid="ref-decockSimilarityGoodman2011" ref-type="bibr">2011</xref>).</p>
  </fn>
  <fn id="fn2">
    <label>2</label><p>A simpler alternative would be generating the
    visual embeddings with the same code as the categorical ones,
    selecting 8 points per cluster but much more spread out
    (e.g. selecting 8 among the 90% closest to the centroids, which
    would create more variability than the categorical one set to 60%).
    I chose otherwise because this wouldn’t have had points reaching
    <italic>inside</italic> the sphere.</p>
  </fn>
</fn-group>
</back>

<sub-article article-type="notebook" id="nb-12-nb-article">
<front-stub>
<title-group>
<article-title>Unravelling mental representations in aphantasia through
unsupervised alignment</article-title>
<subtitle>Project design and data analysis simulation</subtitle>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Delem</surname>
<given-names>Maël</given-names>
</name>
<string-name>Maël Delem</string-name>

</contrib>
</contrib-group>
<abstract>
<p>Research on aphantasia is confronted with a long-standing conundrum
of all research on consciousness and representations, namely the
theoretical inaccessibility of subjective representations. Drawing on
concepts from similarity and representation research, I endorse the view
that the study of an individual’s mental representations is made
possible by exploiting second-order isomorphism. The concept of
second-order isomorphism means that correspondence should not be sought
in the first-order relation between (a) an external object and (b) the
corresponding internal representation, but in the second-order relation
between (a) the perceived similarities between various external objects
and (b) the similarities between their corresponding internal
representations. Building on this idea, this study project report is
divided into five parts. <bold>First</bold>, I outline the central ideas
underlying similarity research and its applicability to aphantasia
research. <bold>Second</bold>, I present a methodological rationale and
protocol based on inverse multidimensional scaling that can be
implemented online to conduct such large-scale research with high
efficiency. <bold>Third</bold>, I present a data analysis plan using a
state-of-the-art method for similarity analysis, unsupervised alignment
with Gromov-Wasserstein optimal transport (GWOT). <bold>Fourth</bold>, I
report a data simulation of a potential outcome of this project and the
successful analysis of this synthetic data using GWOT alignment.
<bold>Fifth</bold>, I analyse the feasability of such a project given
the material constraints of my thesis. I conclude with the expected
utility and benefits of this project.</p>
</abstract>
</front-stub>

<body>
<boxed-text>
  <disp-quote>
    <p><bold>Project inception</bold></p>
    <p>This project stems from several elements:</p>
    <list list-type="order">
      <list-item>
        <p>The long standing knowledge of the fact that internal
        representations seem impossible to reach due to their subjective
        nature.</p>
      </list-item>
      <list-item>
        <p>The discovery of the article of Shepard and Chipman
        (<xref alt="1970" rid="ref-shepardSecondorderIsomorphismInternal1970-nb-article" ref-type="bibr">1970</xref>)
        that expose the idea of “second-order isomorphism”.</p>
      </list-item>
      <list-item>
        <p>The discovery of state-of-the-art and accessible unsupervised
        analytic methods to study this principle in an astonishing way.
        The last two discoveries (and many more) are the fruit of
        amazing discussions and recommendations from Ladislas when he
        came here. These motivated me to try to implement GWOT in R on
        data that I wanted to create myself to emulate a study we could
        do.</p>
      </list-item>
    </list>
    <p><italic>I promise that I did this mostly on my spare time, we
    have too many other things to do elsewhere.</italic></p>
  </disp-quote>
</boxed-text>
<sec id="theoretical-context-nb-article">
  <title>1. Theoretical context</title>
  <sec id="psychological-spaces-and-aphantasia-nb-article">
    <title>1.1 Psychological spaces and aphantasia</title>
    <p>While attempting to demonstrate the uselessness of the concept of
    similarity as a philosophical and scientific
    notion<xref ref-type="fn" rid="fn1-nb-article">1</xref>, Goodman
    (<xref alt="1972" rid="ref-goodmanSevenStricturesSimilarity1972-nb-article" ref-type="bibr">1972</xref>)
    has inadvertently expressed an aspect of similarity judgements of
    primary importance to us aphantasia researchers:</p>
    <disp-quote>
      <p>Comparative judgments of similarity often require not merely
      selection of relevant properties but a weighting of their relative
      importance, and variation in both relevance and importance can be
      rapid and enormous. Consider baggage at an airport checking
      station. The spectator may notice shape, size, color, material,
      and even make of luggage; the pilot is more concerned with weight,
      and the passenger with destination and ownership. Which pieces are
      more alike than others depends not only upon what properties they
      share, but upon who makes the comparison, and when. . . .
      Circumstances alter similarities.</p>
    </disp-quote>
    <p>This can be easily reversed as an argument in favor of the
    <bold>potential of similarity analyses to highlight the
    inter-individual differences in sensory mental
    representations</bold>. For example, should we ask individuals to
    judge the similarities in shape or color between various objects,
    the <italic>differences between the similarity structures</italic>
    of individuals will be precisely the most important phenomenon for
    us, far less than the constancy between these structures. If we can
    account for the context dependence, as we will propose here with
    explicit instructions, clever task design, and hypothesis-neutral
    analysis, we could overcome the limitations of the inherently
    subjective nature of similarity judgements.</p>
    <p>This idea of a difference in similarity judgements in aphantasia
    seems to transpire in the results of Bainbridge et al.
    (<xref alt="2021" rid="ref-bainbridgeQuantifyingAphantasiaDrawing2021-nb-article" ref-type="bibr">2021</xref>)
    on their drawing study. They have shown that aphantasics had more
    schematic representations during recall, accurate in their spatial
    positioning, but with less sensory details. This difference can be
    seen from two perspectives: (1) a memory deficit for sensory
    properties; (2) a different representational structure of the items
    in their psychological spaces. In the latter case, aphantasics would
    have greater/faster abstraction of their representation of a
    perceived scene, reducing the amount of encoded sensory details
    unconsciously considered to be relevant. Both (1) and (2) can
    theoretically explain the same behavioural response, i.e. less
    sensory elements and correct spatial recall accuracy in aphantasic
    drawings, but <bold>the two have drastically different consequences
    on how we define, characterize, and judge aphantasia.</bold></p>
    <p>The dominant hypothesis seems to be that aphantasics simply have
    an episodic or general memory deficit. Conversely, I hypothesize
    that aphantasics have different representational structures than
    phantasics in certain dimensions of their psychological spaces
    (notably sensory, but potentially abstract too). More generally, I
    hypothesize that the concept of visual imagery evaluates in reality
    the continuous spectrum of representational structures in
    <italic>sensory</italic> dimensions of psychological spaces.
    Mirroring visual imagery, spatial imagery could also be a rough
    psychometric evaluation of the continuous spectrum of structural
    differences in <italic>conceptual/abstract</italic> dimensions of
    psychological spaces. In this view, the psychological space of
    aphantasics would constrain internal representations to particularly
    abstract forms from a very early stage, thus selectively limiting
    the item properties thereafter encoded in long-term memory. In other
    terms, <bold>I hypothesize that aphantasia would not be
    characterized by an episodic memory deficit, but by an episodic
    memory <italic>selectivity</italic> caused by the specific
    characteristics of their representational structures and
    psychological spaces.</bold> This selectivity would have, as we
    already hypothesized several times, benefits and drawbacks.</p>
    <p>Gardenfors
    (<xref alt="2004a" rid="ref-gardenforsConceptualSpacesFramework2004-nb-article" ref-type="bibr">2004a</xref>)
    proposed that differences in psychological (in his terms,
    conceptual) spaces could arise from various sources, whether innate,
    due to learning, or broader cultural or social differences. All
    these hypotheses could be coherent to explain the sources of
    aphantasia. Nevertheless, the study of these sources should be the
    subject of very large-scale or longitudinal studies, which are out
    of the scope of this project.</p>
    <p>Here, we shall rather attempt to <bold>develop a method to
    characterize the differences in aphantasics’ representational
    structures and psychological spaces.</bold></p>
  </sec>
</sec>
<sec id="methodology-nb-article">
  <title>2. Methodology</title>
  <p>Roads and Love
  (<xref alt="2024" rid="ref-roads2024-nb-article" ref-type="bibr">2024</xref>), in
  a recent review on the state and perspectives of similarity research,
  highlighted two challenges that studies in this field had to face: (1)
  The high cost of collecting behavioral data on a large number of
  stimuli; (2) The lack of software packages being a high barrier to
  entry, making the task of coding models difficult for the
  uninitiated.</p>
  <p>To solve these problems, we present here two solutions,
  respectively for (1) experimental design and (2) data analysis:</p>
  <list list-type="order">
    <list-item>
      <p>A recent method to efficiently acquire similarity judgements,
      the “multiple arrangement of items” and “inverse multidimensional
      scaling” developed by Kriegeskorte and Mur
      (<xref alt="2012" rid="ref-kriegeskorteInverseMDSInferring2012-nb-article" ref-type="bibr">2012</xref>).</p>
    </list-item>
    <list-item>
      <p>An accessible and robust Python toolbox provided by Sasaki et
      al.
      (<xref alt="2023" rid="ref-sasakiToolboxGromovWassersteinOptimal2023-nb-article" ref-type="bibr">2023</xref>)
      to conduct unsupervised alignment analysis using
      Gromov-Wasserstein optimal transport.</p>
    </list-item>
  </list>
  <sec id="experimental-design-nb-article">
    <title>2.1 Experimental design</title>
    <sec id="multi-arrangement-and-inverse-multidimensional-scaling-nb-article">
      <title>Multi-arrangement and inverse multidimensional
      scaling</title>
      <p>Assuming a geometric model of representational similarities,
      Kriegeskorte and Mur
      (<xref alt="2012" rid="ref-kriegeskorteInverseMDSInferring2012-nb-article" ref-type="bibr">2012</xref>)
      developed a multi-arrangement (MA) method to efficiently acquire
      (dis)similarity judgments for large sets of objects. The subject
      has to perform multiple arrangements of item subsets adaptively
      designed for optimal measurement efficiency and for estimating the
      representational dissimilarity matrix (RDM) by combining the
      evidence from the subset arrangements.</p>
      <p>The procedure is illustrated in
      <xref alt="Figure 2.1" rid="fig-multi-arrangement-nb-article">Figure 2.1</xref>.</p>
      <fig id="fig-multi-arrangement-nb-article">
        <caption><p>Figure 2.1: <bold>Acquiring similarity judgements
        with the multi-arrangement method. (A)</bold> Subjects are asked
        to arrange items according to their similarity, using mouse
        drag-and-drop on a computer. The similarity measure is taken as
        the distances between the items: similar items are closer, while
        dissimilar items are further apart. The upper part of the figure
        shows screenshots at different moments of the acquisition for
        one subject. Columns are trials and rows show the object
        arrangements over time, running from the start (top row) to the
        end (last row). The first trial contains all items; subsequent
        trials contain subsets of items that are adaptively selected to
        optimally estimate judged similarity for each subject.
        <bold>(B)</bold> Once acquisition of the final judgements is
        completed, inter-item distances in the final trial arrangements
        are combined over trials by rescaling and averaging to yield a
        single dissimilarity estimate for each object pair. The process
        is illustrated in this figure for two example item pairs: a
        boy’s face and a hand (red), and carrots and a stop sign (blue).
        Their single-trial dissimilarity estimates (arrows) are combined
        into a single dissimilarity estimate, which is placed at the
        corresponding entry of the RDM (lower panel). Mirror-symmetric
        entries are indicated by lighter colors (figure from
        <xref alt="Mur et al. 2013" rid="ref-murHumanObjectSimilarityJudgments2013-nb-article" ref-type="bibr">Mur
        et al. 2013</xref>).</p></caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="images/multi-arrangement-method-mur-2013.png" />
      </fig>
      <p>A key strength of this method that sets it as particularly
      effective is the “adaptive” part. The goal of the process is to
      acquire similarity judgements as precisely as possible while
      minimizing the total amount of trials. To do so, starting from the
      second trial, selected subsets of the items to be compared are
      presented to the subject: these items are the ones that were very
      close on-screen in previous trials and thus had their distance
      evaluated with lower accuracy by the subject. As the subject has
      to fill the entire “arena” with the items, these subsequent trials
      will necessarily increase the level of precision in the similarity
      judgement between pairs of items. The second key benefit of this
      method is the time and effort gain compared to others. For
      example, to compare every pair of items among 64 different items
      would require <inline-formula><alternatives>
      <tex-math><![CDATA[\frac{64 \times (64-1)}{2} = 2016]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>64</mml:mn><mml:mo>×</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mn>64</mml:mn><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>=</mml:mo><mml:mn>2016</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
      comparisons (i.e. trials). This would be extremely time-consuming,
      while also losing the <italic>context-independence</italic>
      afforded by the MA method due to the presence of other items
      around every time the subject mentally performs a pairwise
      comparison.</p>
      <p>Historically, when referring to the projection of the
      representations of stimuli (e.g., coordinates in geometric space)
      from a high-dimensional space into a lower-dimensional space,
      inference algorithms were commonly called multidimensional scaling
      (<xref alt="Roads and Love 2024" rid="ref-roads2024-nb-article" ref-type="bibr">Roads
      and Love 2024</xref>). By analogy, the process of combining
      several lower-dimensional (2D) similarity judgements on-screen to
      form one higher dimensional similarity representation (in the RDM)
      can be conceptually seen as “inverse” multidimensional scaling,
      hence the name given to the method by Kriegeskorte and Mur
      (<xref alt="2012" rid="ref-kriegeskorteInverseMDSInferring2012-nb-article" ref-type="bibr">2012</xref>).</p>
    </sec>
    <sec id="principle-nb-article">
      <title>Principle</title>
      <p>The idea is simple: for a given set of items that have distinct
      and very pictorial visual properties, we would ask a wide range of
      aphantasics, phantasics or hyperphantasics to imagine, mentally
      compare and make similarity judgements between the items. To
      compare these representations with actual perceptual
      representations, the subjects would also perform the same task
      afterwards, this time with actual pictures to compare. Subjects
      would also fill our usual psychometric imagery questionnaires.</p>
      <p>To “compare imagined items”, we could use a “word” version of
      the MA paradigm. An example from Majewska et al.
      (<xref alt="2020" rid="ref-majewskaSpatialMultiarrangementClustering2020-nb-article" ref-type="bibr">2020</xref>)
      - <italic>who used the method to build large-scale semantic
      similarity resources for Natural Language Processing
      systems</italic> - is represented in
      <xref alt="Figure 2.2" rid="fig-majewska-nb-article">Figure 2.2</xref>.</p>
      <fig id="fig-majewska-nb-article">
        <caption><p>Figure 2.2: Arena layout of the MA protocol used by
        to acquire similarity judgements on word pairs (figure from
        <xref alt="Majewska et al. 2020" rid="ref-majewskaSpatialMultiarrangementClustering2020-nb-article" ref-type="bibr">Majewska
        et al. 2020</xref>).</p></caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="images/majewska-spam.png" />
      </fig>
      <p>We could have the stimuli rated by another set of participants
      on several features.</p>
      <disp-quote>
        <p>« <italic>We deliberately did not specify which object
        properties to focus on, to avoid biasing participants’
        spontaneous mental representation of the similarities between
        objects. Our aim was to obtain similarity judgments that reflect
        the natural representation of objects without forcing
        participants to rely on one given dimension. However,
        participants were asked after having performed the task, what
        dimension(s) they used in judging object similarity.</italic> »
        (<xref alt="Jozwik, Kriegeskorte, and Mur 2016" rid="ref-jozwik2016-nb-article" ref-type="bibr">Jozwik,
        Kriegeskorte, and Mur 2016</xref>)</p>
      </disp-quote>
      <disp-quote>
        <p>« <bold><italic>All but one of the 16 participants reported
        arranging the images according to a categorical
        structure.</italic></bold> »
        (<xref alt="Jozwik et al. 2017" rid="ref-jozwik2017-nb-article" ref-type="bibr">Jozwik
        et al. 2017</xref>)</p>
      </disp-quote>
      <p>This result of Jozwik et al.
      (<xref alt="2017" rid="ref-jozwik2017-nb-article" ref-type="bibr">2017</xref>)
      suggests that we should give an explicit instruction about the
      features to focus on, otherwise everyone might bypass visual
      features and mental images in favour of concepts and categories,
      regardless of their mental imagery profile.</p>
      <p>In contrast, if we ask to focus specifically on the visual
      features, then ask subjects about the strategy they used to
      evaluate the similarities, then on the subjectively felt mental
      format of these strategies, we might grasp better insight on the
      sensory representations of subjects.</p>
      <p>We could even go for several comparisons - even though this
      would increase quadratically the number of trials - e.g. :</p>
      <list list-type="bullet">
        <list-item>
          <p>Evaluate to what extent the <bold>shape</bold> <italic>of
          these animals are</italic>
          <bold><italic>similar</italic></bold> <bold>at rest, ignoring
          size differences.</bold></p>
        </list-item>
        <list-item>
          <p>Evaluate to what extent these animals <bold>sound like each
          other.</bold></p>
        </list-item>
        <list-item>
          <p>Etc.</p>
        </list-item>
      </list>
      <disp-quote>
        <p><italic>Note to be added: if you do not know the animal, just
        guess its placement, as this situation is quite unlikely to
        happen (animals chosen are fairly common
        knowledge).</italic></p>
      </disp-quote>
      <p>Kawakita et al.
      (<xref alt="2023" rid="ref-kawakita2023-nb-article" ref-type="bibr">2023</xref>):
      To assess whether the color dissimilarity structures from
      different participants can be aligned in an unsupervised manner,
      we divided color pair similarity data from a large pool of 426
      participants into five participant groups (85 or 86 participants
      per group) to obtain five independent and complete sets of
      pairwise dissimilarity ratings for 93 color stimuli (Fig. 3a).
      Each participant provided a pairwise dissimilarity judgment for a
      randomly allocated subset of the 4371 possible color pairs. We
      computed the mean of all judgments for each color pair in each
      group, generating five full dissimilarity matrices referred to as
      Group 1 to Group 5.</p>
    </sec>
    <sec id="stimuli-nb-article">
      <title>Stimuli</title>
      <p>We would have a list of animal items, that would have several
      characteristics:</p>
      <list list-type="bullet">
        <list-item>
          <p>A name</p>
        </list-item>
        <list-item>
          <p>A category</p>
        </list-item>
        <list-item>
          <p>A shape</p>
        </list-item>
      </list>
      <p>We need orthogonal data:</p>
      <list list-type="bullet">
        <list-item>
          <p>Each class of animal should include each shape
          (roughly)</p>
        </list-item>
        <list-item>
          <p>Each shape should have an animal</p>
        </list-item>
      </list>
      <p>This would imply that category cannot be derived from shape,
      and vice-versa. Thus, a <bold>sorting by shape would reveal to be
      innately visual</bold> (or maybe spatial, if shape concerns this
      type of imagery), and a <bold>sorting by category would reveal an
      abstraction</bold> from these shapes. We expect that the two will
      be mixed to some degree in every subject, but that low-imagery
      would rather tend towards category sorting, while high-imagery
      would tend towards shape sorting.</p>
      <p>Shapes could be very tricky stimuli to discuss. Gardenfors
      (<xref alt="2004b" rid="ref-gardenfors2004-nb-article" ref-type="bibr">2004b</xref>)
      noted that we only have a very sketchy understanding of how we
      perceive and conceptualize things according to their shapes. The
      works of Marr, Nishihara, and Brenner
      (<xref alt="1997" rid="ref-marr1997-nb-article" ref-type="bibr">1997</xref>)
      highlight this difficulty when analysing the complexity of the
      hierarchical judgements of shapes and volumes, as shown in
      <xref alt="Figure 2.3" rid="fig-marr-nb-article">Figure 2.3</xref>.</p>
      <fig id="fig-marr-nb-article">
        <caption><p>Figure 2.3: Representing the characteristics of
        shapes with cylinders (figure from
        <xref alt="Marr, Nishihara, and Brenner 1997" rid="ref-marr1997-nb-article" ref-type="bibr">Marr,
        Nishihara, and Brenner 1997</xref>).</p></caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="images/shapes-marr.png" />
      </fig>
    </sec>
  </sec>
</sec>
<sec id="data-analysis-plan-nb-article">
  <title>3. Data analysis plan</title>
  <sec id="unsupervised-alignment-rationale-nb-article">
    <title>3.1 Unsupervised alignment rationale</title>
    <p>Visual images can be represented as points in a multidimensional
    psychological space. Embedding algorithms can be used to infer
    latent representations from human similarity judgments. While there
    are an infinite number of potential visual features, an embedding
    algorithm can be used to identify the subset of salient features
    that accurately model human-perceived similarity. (<italic>From
    Roads’ CV</italic>)</p>
    <p>Using an optimization algorithm, the free parameters of a
    psychological space are found by maximizing goodness of fit (i.e.,
    the loss function) to the observed data. Historically, when
    referring specifically to the free parameters that correspond to the
    representation of stimuli (e.g., coordinates in geometric space),
    inference algorithms were commonly called multidimensional scaling
    (MDS), or simply scaling, algorithms.</p>
    <p>In the machine learning literature, analogous inference
    algorithms are often called embedding algorithms. The term
    “embedding” denotes a higher-dimensional representation that is
    embedded in a lower-dimensional space. For that reason, the inferred
    mental representations of a psychological space could also be called
    a psychological embedding.</p>
    <p>Numerous techniques exist, and each has limitations. Popular
    techniques for comparing representations include RSA Kriegeskorte,
    Mur, and Bandettini
    (<xref alt="2008" rid="ref-kriegeskorte2008-nb-article" ref-type="bibr">2008</xref>)
    and canonical correlation analysis (CCA) (Hotelling 1936). Briefly,
    RSA is a method for comparing two representations that assesses the
    correlation between the implied pairwise similarity matrices. CCA is
    a method that compares two representations by finding a pair of
    latent variables (one for each domain) that are maximally
    correlated.</p>
    <p>One might be tempted to compare two dissimilarity matrices
    assuming stimulus-level “external” correspondence: my “red”
    corresponds to your “red”(Fig. 1d). This type of supervised
    comparison between dissimilarity matrices, known as Representational
    Similarity Analysis (RSA), has been widely used in neuroscience to
    compare various similarity matrices obtained from behavioural and
    neural data. However, there is no guarantee that the same stimulus
    will necessarily evoke the same subjective experience across
    different participants. Accordingly, when considering which stimuli
    evoke which qualia for different individuals, we need to consider
    all possibilities of correspondence: my “red” might correspond to
    your “red”, “green”, “purple”, or might lie somewhere between your
    “orange” and “pink”(Fig. 1e). Thus, we compare qualia structures in
    a purely unsupervised manner, without assuming any correspondence
    between individual qualia across participants.</p>
  </sec>
  <sec id="gromov-wasserstein-optimal-transport-nb-article">
    <title>3.2 Gromov-Wasserstein optimal transport</title>
    <p>To account for all possible correspondences, we use an
    unsupervised alignment method for quantifying the degree of
    similarity between qualia structures. As shown in Fig. 2a, in
    unsupervised alignment, we do not attach any external (stimuli)
    labels to the qualia embeddings. Instead, we try to find the best
    matching between qualia structures based only on their internal
    relationships (see Methods). After finding the optimal alignment, we
    can use external labels, such as the identity of a color stimulus
    (Fig. 2b), to evaluate how the embeddings of different individuals
    relate to each other. This allows us to determine which color
    embeddings correspond to the same color embeddings across
    individuals or which do not. Checking the assumption that these
    external labels are consistent across individuals allows us to
    assess the plausibility of determining accurate inter-individual
    correspondences between qualia structures of different
    participants.</p>
    <p>To this end, we used the Gromov-Wasserstein optimal transport
    (GWOT) method, which has been applied with great success in various
    fields. GWOT aims to find the optimal mapping between two point
    clouds in different domains based on the distance between points
    within each domain. Importantly, the distances (or correspondences)
    between points “across” different domains are not given while those
    “within” the same domain are given. GWOT aligns the point clouds
    according to the principle that a point in one domain should
    correspond to another point in the other domain that has a similar
    relationship to other points. The principle of the method is
    illustrated in
    <xref alt="Figure 3.1" rid="fig-gwot-kawa-nb-article">Figure 3.1</xref></p>
    <fig id="fig-gwot-kawa-nb-article">
      <caption><p>Figure 3.1: Gromov-Wassertein optimal transport
      principle (figure from
      <xref alt="Kawakita et al. 2023" rid="ref-kawakita2023-nb-article" ref-type="bibr">Kawakita
      et al. 2023</xref>).</p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="images/kawa-gwot-2.PNG" />
    </fig>
    <p>We first computed the GWD for all pairs of the dissimilarity
    matrices of the 5 groups (Group 1-5) using the optimized
    <inline-formula><alternatives>
    <tex-math><![CDATA[\epsilon]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ϵ</mml:mi></mml:math></alternatives></inline-formula>.
    In Fig. 3b, we show the optimized mapping
    <inline-formula><alternatives>
    <tex-math><![CDATA[\Gamma*]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>Γ</mml:mi><mml:mo>*</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
    between Group 1 and Groups 2-5 (see Supplementary Figure S1 for the
    other pairs). As shown in Fig. 3b, most of the diagonal elements in
    <inline-formula><alternatives>
    <tex-math><![CDATA[\Gamma*]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>Γ</mml:mi><mml:mo>*</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
    show high values, indicating that most colors in one group
    correspond to the same colors in the other groups with high
    probability. We next performed unsupervised alignment of the vector
    embeddings of qualia structures. Although
    <inline-formula><alternatives>
    <tex-math><![CDATA[\Gamma*]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>Γ</mml:mi><mml:mo>*</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
    provides the rough correspondence between the embeddings of qualia
    structures, we should find a more precise mathematical mapping
    between qualia structures in terms of their vector embeddings to
    more accurately assess the similarity between the qualia structures.
    Here, we consider aligning the embeddings of all the groups in a
    common space.</p>
    <p>By applying MDS, we obtained the 3-dimensional embeddings of
    Group 1 and Groups 2-5, referred to as X and Yi, where i = 2, …, 5
    (Fig. 3c). We then aligned Yi to X with the orthogonal rotation
    matrix Qi, which was obtained by solving a Procrustes-type problem
    using the optimized transportation plan
    <inline-formula><alternatives>
    <tex-math><![CDATA[\Gamma*]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>Γ</mml:mi><mml:mo>*</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
    obtained through GWOT (see Methods). Fig. 3d shows the aligned
    embed- dings of Group 2-5 (QiYi) and the embedding of Group 1 (X)
    plotted in the embedded space of X. Each color represents the label
    of a corresponding external color stimulus. Note that even though
    the color labels are shown in Fig. 3d, this is only for the
    visualization purpose and the whole alignment procedure is performed
    in a purely unsupervised manner without relying on the color labels.
    As can be seen in Fig. 3d, the embeddings of similar colors from the
    five groups are located close to each other, indicating that similar
    colors are ‘correctly’ aligned by the unsupervised alignment
    method.</p>
    <p>To evaluate the performance of the unsupervised alignment, we
    computed the k-nearest color matching rate in the aligned space. If
    the same colors from two groups are within the k-nearest colors in
    the aligned space, we consider that the colors are correctly
    matched. We evaluated the matching rates between all the pairs of
    Groups 1-5. The averaged matching rates are 51% when k = 1, 83% when
    k = 3, and 92% when k = 5, respectively. This demonstrates the
    effectiveness of the GW alignment for correctly aligning the qualia
    structures of different participants in an unsupervised manner.</p>
    <p>However, as can be seen in Fig. 4b, the optimized mapping
    <inline-formula><alternatives>
    <tex-math><![CDATA[\Gamma*]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>Γ</mml:mi><mml:mo>*</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
    is not lined up diagonally unlike the optimized map- pings between
    color-neurotypical participants groups shown in Fig. 3b (see
    Supplementary Figure S1 for the other pairs). Accordingly, top k
    matching rate between Group 1-5 and Group 6 is 3.0% when k = 1 (Fig.
    4c), which is only slightly above chance
    (<inline-formula><alternatives>
    <tex-math><![CDATA[\approx]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mo>≈</mml:mo></mml:math></alternatives></inline-formula>
    1%). The matching rate did not improve even when we relaxed the
    criterion (6.9% and 11% for k = 3 and k = 5, respectively).
    Moreover, all of the GWD values between Group 1-5 and Group 6 are
    larger than any of the GWD values between color-neurotypical
    participant groups (Fig. 4d).</p>
    <p>These results indicate that the difference between the qualia
    structures of neuro-typical and atypical participants is
    significantly larger than the difference between the qualia
    structures of neuro-typical participants.</p>
    <fig>
      <caption><p>The two conditions for one subject.</p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="images/my-protocol-1.png" />
    </fig>
    <fig>
      <caption><p>The comparison between the representational structure
      of aphantasics and phantasics. This figure illustrates the
      principle, but in reality all pairs of subjects will be compared
      to assess their representational structure alignment. This is
      computationnally heavy, but analytically very
      powerful.</p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="images/my-protocol-2.png" />
    </fig>
  </sec>
  <sec id="hypotheses-nb-article">
    <title>3.3 Hypotheses</title>
    <sec id="aphantasic-and-phantasic-psychological-spaces-nb-article">
      <title>Aphantasic and phantasic psychological spaces</title>
      <p>The most representative members of a category are called
      prototypical members.</p>
      <p>Prototype theory builds on the observation that among the
      instances of a property, some are more representative than others.
      The most representative one is the prototype of the property.</p>
      <p>Thus, following the concepts illustrated by Gardenfors
      (<xref alt="2004b" rid="ref-gardenfors2004-nb-article" ref-type="bibr">2004b</xref>),
      we would expect that aphantasics, when doing shape similarity
      judgements, would be more inclined to group items close to the
      prototypical items due to a lower definition of the mental image.
      In comparison, phantasics would have a much more distributed
      conceptual space of item shapes due to their higher-resolution
      mental images of said items.</p>
    </sec>
    <sec id="subjective-imagery-and-psychological-spaces-nb-article">
      <title>Subjective imagery and psychological spaces</title>
      <p>In the proposed view of visual imagery as the subjective
      expression of a given type of psychological space, we mentioned
      earlier that <italic>spatial</italic> imagery could also
      constitute a subjective expression of other dimensions of
      psychological spaces. Hence, the <italic>verbal</italic> dimension
      of the simplified model of imagery we outlined in my thesis
      project could also represent different dimensions.</p>
      <p>This conception leads to the following theoretical hypothesis:
      provided that our visual-spatial-verbal model correctly fits
      subjective imagery, the imagery profile of individuals should map
      on their psychological spaces.</p>
      <p>Operationally, this would be evaluated by the fact that
      <bold>individuals with similar imagery profiles</bold> (visual,
      spatial, verbal, or any combination of the three) <bold>should
      have similar representations</bold> in their given psychological
      space, <bold>quantifiable by the degree of alignment between their
      similarity structures.</bold></p>
    </sec>
  </sec>
</sec>
<sec id="study-simulation-and-analysis-nb-article">
  <title>4. Study simulation and analysis</title>
  <sec id="nb-code-cell-1-nb-article" specific-use="notebook-code">
  <code language="r script"># ═══ Packages ═════════════════════════════════════════════════════════════════

if (!require(librarian)) install.packages(librarian) </code>
  <code id="annotated-cell-2-nb-article" language="r script">library(librarian)                                     

# now putting packages on our library's shelves:
shelf(
  # ─── data management ─────────────────
  holodeck,       # simulating multivariate data
  cluster,        # dissimilarity matrices
  
  # ─── modelling ───────────────────────
  mclust,         # mixture clustering
  
  #  data visualization ──────────────
  # palettes
  viridis,        # colour-blind friendly palettes
  # interactive
  plotly,         # interactive plots
  ggdendro,       # dendrograms
  seriation,      # dissimilarity plots
  webshot2,       # HTML screenshots for Word render
  webshot,
  
  # ─── essential package collections ───
  doParallel,     # parallel execution
  easystats,      # data analysis ecosystem
  reticulate,     # R to Python                    
  tidyverse,      # modern R ecosystem
)

# ─── Global cosmetic theme ───
theme_set(theme_modern(base_size = 14))

pal_okabe_ito &lt;- c(
  &quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;,                            
  &quot;#F5C710&quot;, &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;, &quot;#6c0009&quot;)      

# We'll need 9 colors at some point
pal_okabe_ito_extended &lt;- c(                                 
  &quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;,                           
  &quot;#F5C710&quot;, &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;, &quot;#6c0009&quot;, &quot;#414487FF&quot;)

# We'll need 30 colors at another moment
cool_30_colors &lt;- c(                                                   
  &quot;#3d51b4&quot;, &quot;#414487FF&quot;, &quot;#003d73&quot;, &quot;#440154FF&quot;, &quot;#6c0009&quot;, &quot;#b64e4e&quot;,
  &quot;#D55E00&quot;, &quot;#E69F00&quot;, &quot;#F5C710&quot;, &quot;#FDE725FF&quot;, &quot;#f2bb7b&quot;, &quot;#f1afad&quot;, &quot;#CC79A7&quot;, 
  &quot;#e57774&quot;, &quot;#7AD151FF&quot;, &quot;#57b571&quot;, &quot;#318a4a&quot;, &quot;#009E73&quot;, &quot;#22A884FF&quot;, 
  &quot;#2A788EFF&quot;, &quot;#0072B2&quot;, &quot;#2da6b5&quot;, &quot;#56B4E9&quot;, &quot;#889be0&quot;, &quot;#6677e0&quot;,   
  &quot;#3d51b4&quot;, &quot;#414487FF&quot;, &quot;#003d73&quot;, &quot;#440154FF&quot;, &quot;#6c0009&quot;, &quot;#b64e4e&quot;  
  )

path = &quot;notebooks/data/&quot;

df &lt;- read_rds(paste0(path, &quot;df.RDS&quot;))
df_embeds &lt;- read_rds(paste0(path, &quot;df_embeds.RDS&quot;))
# Categorical and visual embeddings
df_embeds_categ  &lt;- read_rds(paste0(path, &quot;df_embeds_categ.RDS&quot;))
df_embeds_visual &lt;- read_rds(paste0(path, &quot;df_embeds_visual.RDS&quot;))
# Subject embeddings per sub-group
df_embed_c_sub  &lt;- read_rds(paste0(path, &quot;df_embed_c_sub.RDS&quot;))
df_embed_cs_sub &lt;- read_rds(paste0(path, &quot;df_embed_cs_sub.RDS&quot;))
df_embed_v_sub  &lt;- read_rds(paste0(path, &quot;df_embed_v_sub.RDS&quot;))
df_embed_vs_sub &lt;- read_rds(paste0(path, &quot;df_embed_vs_sub.RDS&quot;))
# Accuracy of the unsupervised alignment (bad = not tidy data)
df_accuracy_all_bad &lt;- read_rds(paste0(path, &quot;df_accuracy_all_bad.RDS&quot;))
df_accuracy_cat_bad &lt;- read_rds(paste0(path, &quot;df_accuracy_cat_bad.RDS&quot;))
# Coordinates of the aligned embeddings from the Python output
coordinates_aligned_embeddings &lt;- read_rds(paste0(path, &quot;coordinates_aligned_embeddings.RDS&quot;))</code>
  <sec id="nb-code-cell-1-output-0-nb-article" specific-use="notebook-output">
  <preformat>Le chargement a nécessité le package : librarian</preformat>
  </sec>
  </sec>
  <sec id="visual-spatial-verbal-model-of-cognitive-profiles-nb-article">
    <title>4.1 Visual-spatial-verbal model of cognitive profiles</title>
    <p>One of the objectives of the study would be to link the
    subjective cognitive profiles of individuals with their
    representational structures. To evaluate these profiles, we are
    going to use psychometric questionnaires evaluating the
    visual-object, spatial, and verbal dimensions of imagery which will
    yield three scores, one for each dimension.</p>
    <p>We are going to simulate 30 participants presenting four
    different cognitive profiles, that I defined as, respectively,
    <italic>verbal</italic> aphantasics, <italic>spatial</italic>
    aphantasics, <italic>spatial</italic> phantasics, and
    <italic>visual</italic> phantasics. Their imagery abilities are
    summarised in
    <xref alt="Table 4.1" rid="tbl-imageries-nb-article">Table 4.1</xref>.</p>
    <p>To simulate these four sub-groups, we use the
    <monospace>holodeck</monospace> R package to generate multivariate
    normal distributions of scores on these three dimensions for each
    sub-group. For instance, verbal aphantasics have normally
    distributed visual imagery scores centered around a mean of 0
    (normalized, so negative scores are possible), 0.4 for spatial
    imagery, and 0.7 for verbal style; Spatial aphantasics have means of
    0 for visual, 0.75 spatial, and 0.3 for verbal; etc. The numbers are
    arbitrary, but have been chosen by trial-and-error to obtain a model
    that is both well-defined and not exaggerated. The 30 subjects’
    imagery profiles are represented in the three dimensional space of
    the visual-spatial-verbal dimensions in
    <xref alt="Figure 4.1" rid="fig-plot-osv-model-nb-article">Figure 4.1</xref>.</p>
    <fig id="tbl-imageries-nb-article">
      <caption><p>Table 4.1: Imagery abilities of the four hypothesized
      cognitive profiles.</p></caption>
      <table-wrap>
        <table>
          <thead>
            <tr>
              <th>Cognitive profile</th>
              <th align="center">Visual imagery</th>
              <th align="center">Spatial imagery</th>
              <th align="center">Verbal style</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Verbal aphantasic</td>
              <td align="center">–</td>
              <td align="center">-</td>
              <td align="center">++</td>
            </tr>
            <tr>
              <td>Spatial aphantasic</td>
              <td align="center">–</td>
              <td align="center">++</td>
              <td align="center">-</td>
            </tr>
            <tr>
              <td>Spatial phantasic</td>
              <td align="center">+</td>
              <td align="center">++</td>
              <td align="center">-</td>
            </tr>
            <tr>
              <td>Visual phantasic</td>
              <td align="center">++</td>
              <td align="center">-</td>
              <td align="center">+</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </fig>
    <sec id="cell-fig-plot-osv-model-nb-article" specific-use="notebook-code">
    <code language="r script">plotting_osv_model &lt;- function(df, grouping_variable, size){
  df |&gt; 
    plot_ly(
      x = ~visual_imagery,
      y = ~spatial_imagery,
      z = ~verbal_profile,
      color = ~df[[grouping_variable]],
      text  = ~df[[grouping_variable]],
      colors = c(&quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;, &quot;#F5C710&quot;),
      type = &quot;scatter3d&quot;,
      mode = &quot;markers+text&quot;,
      marker = list(size = size),
      textfont = list(size = size + 4)
    ) |&gt; 
    layout(
      scene = list(
        xaxis = list(
          title = list(text = &quot;Visual imagery&quot;, font = list(color = &quot;grey&quot;)),
          tickfont = list(color = &quot;grey&quot;)
          ),
        yaxis = list(
          title = list(text = &quot;Spatial imagery&quot;, font = list(color = &quot;grey&quot;)),
          tickfont = list(color = &quot;grey&quot;)
          ),
        zaxis = list(
          title = list(text = &quot;Verbal profile&quot;, font = list(color = &quot;grey&quot;)),
          tickfont = list(color = &quot;grey&quot;)
          )
      ),
      legend = list(title = list(text = &quot;Group&quot;)),
      paper_bgcolor = &quot;transparent&quot;
      )
}

df |&gt; 
  mutate(vis_spa_group = case_when(
    vis_spa_group == &quot;aph_spa_high&quot; ~ &quot;Aph. spatial&quot;,
    vis_spa_group == &quot;aph_spa_low&quot;  ~ &quot;Aph. verbal&quot;,
    vis_spa_group == &quot;phant_spa_high&quot; ~ &quot;Phant. spatial&quot;,
    vis_spa_group == &quot;phant_spa_low&quot;  ~ &quot;Phant. visual&quot;
  )) |&gt; 
  plotting_osv_model(grouping_variable = &quot;vis_spa_group&quot;, size = 4)</code>
    <fig id="fig-plot-osv-model-nb-article">
      <caption><p>Figure 4.1: Imagery profiles generated for 30 subjects
      on the three object, spatial, and verbal dimensions.</p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-plot-osv-model-1.png" />
    </fig>
    <sec id="cell-fig-plot-osv-model-output-0-nb-article" specific-use="notebook-output">
    <preformat>PhantomJS not found. You can install it with webshot::install_phantomjs(). If it is installed, please make sure the phantomjs executable can be found via the PATH variable.</preformat>
    </sec>
    </sec>
  </sec>
  <sec id="data-simulation-creating-representational-structures-nb-article">
    <title>4.2 Data simulation: Creating representational
    structures</title>
    <p>Gardenfors
    (<xref alt="2004b" rid="ref-gardenfors2004-nb-article" ref-type="bibr">2004b</xref>)
    invokes two scientific concepts, to wit, prototypes and Voronoi
    tessellations. Prototype theory builds on the observation that among
    the instances of a property, some are more representative than
    others. The most representative one is the prototype of the
    property. <italic>We hypothesize that aphantasics will be more
    inclined to categorize items according to prototypes than
    phantasics.</italic></p>
    <p>A Voronoi tesselation of a given space divides that space into a
    number of cells such that each cell has a center and consists of all
    and only those points that lie no closer to the center of any other
    cell than to its own center; the centers of the various cells are
    called the generator points of the tesselation. This principle will
    underlie our data simulation, as we will build representations in a
    3D space based on distances to “centroids”, namely, prototypes.
    These representations will thus be located inside of the
    tessellations around these prototypes, more or less close to the
    centroid depending on the subject’s representational structures.</p>
    <sec id="generating-prototype-embeddings-from-a-sphere-nb-article">
      <title>Generating “prototype” embeddings from a sphere</title>
      <sec id="nb-code-cell-2-nb-article" specific-use="notebook-code">
      <code language="r script"># getting the centroids of each cluster
df_centroids &lt;- 
  df_embeds |&gt; 
  group_by(group) |&gt; 
  summarise(
    x_centroid = mean(x),
    y_centroid = mean(y),
    z_centroid = mean(z)
  )

# adding them to the data
df_embeds_2 &lt;- left_join(df_embeds, df_centroids, by = &quot;group&quot;)</code>
      </sec>
      <p>A function will be used to generate embeddings. These spherical
      embeddings are displayed in
      <xref alt="Figure 4.2" rid="fig-perfect-embeddings-nb-article">Figure 4.2</xref>
      We get 8 nicely distributed clusters. We’ll retrieve the centroids
      of each cluster, which would be the “perfect” categories of each
      species group (say, generated by a computational model on
      categorical criteria).</p>
      <sec id="generating-sphere-nb-article" specific-use="notebook-code">
      <code language="python">generate_sphere &lt;- function(n){
  z     &lt;- 2*runif(n) - 1          # uniform on [-1, 1]
  theta &lt;- 2*pi*runif(n) - pi      # uniform on [-pi, pi]
  x     &lt;- sin(theta)*sqrt(1-z^2)  # based on angle
  y     &lt;- cos(theta)*sqrt(1-z^2) 
  
  df &lt;- tibble(x = x, y = y, z = z)
  
  return(df)
}

# 1000 random observations with embeddings uniformly distributed on a sphere
df_embeds &lt;- generate_sphere(1000)

# Clustering the observations in 8 groups based on their coordinates
clusters &lt;- Mclust(df_embeds, G = 8)

# adding the classification to the data
df_embeds &lt;- df_embeds |&gt; mutate(group = as.factor(clusters$classification))

# getting the centroids of each cluster
df_centroids &lt;- 
  df_embeds |&gt; 
  group_by(group) |&gt; 
  summarise(
    x_centroid = mean(x),
    y_centroid = mean(y),
    z_centroid = mean(z)
  )

# adding them to the data
df_embeds_2 &lt;- left_join(df_embeds, df_centroids, by = &quot;group&quot;)</code>
      </sec>
      <sec id="cell-fig-perfect-embeddings-nb-article" specific-use="notebook-code">
      <code language="r script">##| fig-subcap: 
##|   - &quot;Generated spherical distribution of 1000 observations grouped in 8 equal clusers with Gaussian Mixture Clustering.&quot;
##|   - &quot;Centroids of the 8 clusters created on the sphere.&quot;
##| layout-ncol: 2

# function for 3D plotting up to 8 groups (due to the palette)
plotting_3d &lt;- function(df, size, opacity){
  df |&gt; 
    plot_ly(
      type = &quot;scatter3d&quot;,
      mode = &quot;markers&quot;,
      x = ~x,
      y = ~y,
      z = ~z,
      color = ~paste0(&quot;Species group &quot;, group),
      colors = pal_okabe_ito,
      marker = list(size = size, opacity = opacity)
    ) |&gt; 
    layout(paper_bgcolor = &quot;transparent&quot;)
}

plotting_3d(df_embeds, 3, 1) |&gt; 
  layout(legend = list(
    yanchor = &quot;top&quot;,
    y = 1,
    xanchor = &quot;right&quot;,
    x = 0
    ))</code>
      <fig id="fig-perfect-embeddings-nb-article">
        <caption><p>Figure 4.2: Generated spherical distribution of 1000
        observations grouped in 8 equal clusers with Gaussian Mixture
        Clustering to represent the theoretical embeddings of 8 groups
        (i.e. groups of species here). <bold><italic>Interact with the
        figures to see the details.</italic></bold></p></caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-perfect-embeddings-1.png" />
      </fig>
      <code language="r script">centroid_figure &lt;- # quarto manuscript can render columns for now
  df_centroids |&gt; 
  plot_ly(
    type = &quot;scatter3d&quot;,
    mode = &quot;markers+text&quot;,
    x = ~x_centroid,
    y = ~y_centroid,
    z = ~z_centroid,
    text = ~paste0(&quot;Species group &quot;, group),
    color = ~paste0(&quot;Species group &quot;, group),
    colors = pal_okabe_ito,
    marker = list(size = 12, opacity = 1)
    ) |&gt; 
  layout(
    scene = list(
      xaxis = list(title = &quot;x&quot;),
      yaxis = list(title = &quot;y&quot;),
      zaxis = list(title = &quot;z&quot;)
    ),
    paper_bgcolor = &quot;transparent&quot;,
    showlegend = FALSE
    )</code>
      </sec>
      <p>Now we want two sets of embeddings: one where the observations
      are very concentrated around the centroids, which would be the
      <bold>categorical model</bold>, and one where the observations are
      more spread out, which would be the <bold>visual model</bold>.</p>
      <p>We need to select 8 observations per cluster, which would be
      our animals per group. These observations will be subsets of the
      1000 observations we generated.</p>
    </sec>
    <sec id="categorical-model-embeddings-nb-article">
      <title>Categorical model embeddings</title>
      <p>The selection procedure for the <bold>categorical model</bold>
      will consist of selecting points that are rather <italic>close to
      the centroids</italic>. Thus, we will filter the observations of
      the large sets to keep only points for which the distance to the
      centroid is inferior to a given value. That is, points for which
      the Euclidean norm of the vector from the observation to the
      centroid:</p>
      <p><disp-formula><alternatives>
      <tex-math><![CDATA[d(centroid, observation) = \sqrt{(x_{c} - x_{o})^{2} + (y_{c} - y_{o})^{2} + (z_{c} - z_{o})^{2}}]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>d</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:math></alternatives></disp-formula></p>
      <p>This can be done using the function
      <monospace>norm(coordinates, type = &quot;2&quot;)</monospace> in
      R.</p>
      <sec id="categorical-embeddings-nb-article" specific-use="notebook-code">
      <code language="python"># Function to filter points of the sphere based on the distance to the centroids
generate_embeddings &lt;- function(df, n_embeddings, distance_quantile){
  df &lt;- 
    df |&gt; 
    # computing the euclidean distance to the centroids for each observation
    rowwise() |&gt; 
    mutate(
      distance = norm(
        c((x_centroid - x), (y_centroid - y), (z_centroid - z)),
        type = &quot;2&quot;)
      ) |&gt; 
    # filtering by distance to the centroid by group
    group_by(group) |&gt; 
    # selecting the X% closest (specified with &quot;distance_quantile&quot;)
    filter(distance &lt; quantile(distance, probs = distance_quantile)) |&gt; 
    # selecting X random observations per cluster in these 
    # (specified with &quot;n_embeddings&quot;)
    slice(1:n_embeddings) |&gt; 
    select(group, x, y, z) |&gt;
    ungroup()
}

df_embeds_categ &lt;- generate_embeddings(df_embeds_2, 8, 0.5)</code>
      </sec>
      <sec id="cell-fig-categorical-embeddings-nb-article" specific-use="notebook-code">
      <code language="r script"># Plotting these observations
plotting_3d(df_embeds_categ, 6, 1)</code>
      <fig id="fig-categorical-embeddings-nb-article">
        <caption><p>Figure 4.3: Selection of 64 points to represent
        prototypical categorical embeddings, based on the distances to
        each groups’ centroid. These will be the bases of the verbal
        aphantasics’ embeddings. <bold><italic>Interact with the figure
        to see the details.</italic></bold></p></caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-categorical-embeddings-1.png" />
      </fig>
      </sec>
    </sec>
    <sec id="visual-model-embeddings-nb-article">
      <title>Visual model embeddings</title>
      <p>In the case of the <bold>visual model</bold>, we would like
      approximately evenly distributed embeddings, that could also dive
      <italic>inside</italic> the sphere, i.e. representing species that
      are visually close although diametrically opposed when it comes to
      taxonomy. To do this we could try to simulate multivariate normal
      distributions around the
      centroids<xref ref-type="fn" rid="fn2-nb-article">2</xref>. This can be done
      with the <monospace>holodeck</monospace> package.</p>
      <sec id="visual-embeddings-nb-article" specific-use="notebook-code">
      <code language="python"># defining the variance and covariance of the distributions
var2 &lt;- 0.05
cov2 &lt;- 0

# generating multivariate distributions around the categorical 3D means
df_embeds_visual &lt;-
  tibble(
    id = as.factor(seq(1,6400)),
    category = as.factor(rep(seq(1:64), each = 100))
  )|&gt; 
  group_by(category) |&gt; 
  sim_discr(
    n_vars = 1, 
    var = var2, 
    cov = cov2, 
    group_means = df_embeds_categ$x, 
    name = &quot;x&quot;) |&gt; 
  sim_discr(
    n_vars = 1, 
    var = var2, 
    cov = cov2, 
    group_means = df_embeds_categ$y, 
    name = &quot;y&quot;) |&gt; 
  sim_discr(
    n_vars = 1, 
    var = var2, 
    cov = cov2, 
    group_means = df_embeds_categ$z, 
    name = &quot;z&quot;) |&gt; 
  # keeping only 8 points per distribution
  slice(1) |&gt; 
  ungroup() |&gt; 
  mutate(group = as.factor(rep(seq(1, 8), each = 8))) |&gt; 
  rename(x = x_1, y = y_1, z = z_1) |&gt; 
  select(group, x, y, z)</code>
      </sec>
      <sec id="cell-fig-visual-embeddings-nb-article" specific-use="notebook-code">
      <code language="r script">plotting_3d(df_embeds_categ, 4, 1) |&gt; 
  add_trace(
      data = df_embeds_visual,
      type = &quot;scatter3d&quot;,
      mode = &quot;markers&quot;,
      x = ~x,
      y = ~y,
      z = ~z,
      color = ~paste0(&quot;Species group &quot;, group),
      colors = pal_okabe_ito,
      marker = list(size = 4, opacity = 1, symbol = &quot;diamond&quot;)
  )</code>
      <fig id="fig-visual-embeddings-nb-article">
        <caption><p>Figure 4.4: Selection of 64 points to represent
        prototypical visual embeddings, chosen randomly in multivariate
        distributions centered around each categorical embedding. The
        visual embeddings are overlaid as diamonds along with
        categorical ones as dots. The two distributions keep the group
        structure, but are pretty far apart at times.
        <bold><italic>Interact with the figure to see the
        details.</italic></bold></p></caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-visual-embeddings-1.png" />
      </fig>
      </sec>
    </sec>
    <sec id="intermediate-embeddings-nb-article">
      <title>Intermediate embeddings</title>
      <sec id="nb-code-cell-3-nb-article" specific-use="notebook-code">
      <fig id="fig-distances-graph-nb-article">
        <caption><p>Figure 4.5: Model of the distances between
        participants’ representations. Note that here d is a
        one-dimensional distance between the representations, but it
        will be computed as a three-dimensional distance in our
        toy-model. The verbal aphantasic profile is hypothesized to be
        very categorical, thus diametrically opposed to the visual
        phantasic profile, by a given distance d. Spatial profiles are
        in-between: they are close to each other (10% x d), but the
        spatial aphantasic profile is a bit closer to the verbal
        aphantasic one (45% x d), and the spatial phantasic is a bit
        closer to the visual phantasic one (45% x d).</p></caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/mermaid-figure-2.png" />
      </fig>
      </sec>
      <sec id="nb-code-cell-4-nb-article" specific-use="notebook-code">
      <code language="r script">dist_c = 0.45
dist_v = 0.55

df_embeddings &lt;-
  df_embeds_categ |&gt;
  rename(
    group_c = group,
    x_c = x,
    y_c = y,
    z_c = z
  ) |&gt;
  bind_cols(df_embeds_visual) |&gt;
  rename(
    group_v = group,
    x_v = x,
    y_v = y,
    z_v = z
  ) |&gt; 
  select(!group_v) |&gt; 
  rename(group = group_c) |&gt; 
  mutate(
    x_cs = x_c + dist_c*(x_v - x_c),
    y_cs = y_c + dist_c*(y_v - y_c),
    z_cs = z_c + dist_c*(z_v - z_c),
    x_vs = x_c + dist_v*(x_v - x_c),
    y_vs = y_c + dist_v*(y_v - y_c),
    z_vs = z_c + dist_v*(z_v - z_c)
  )</code>
      <sec id="nb-code-cell-4-output-0-nb-article" specific-use="notebook-output">
      <preformat>New names:
• `species` -&gt; `species...2`
• `species` -&gt; `species...7`</preformat>
      </sec>
      </sec>
      <sec id="cell-fig-intermediate-embeddings-nb-article" specific-use="notebook-code">
      <code language="r script">size = 3

df_embeddings |&gt; 
  plot_ly(
    type = &quot;scatter3d&quot;, 
    mode = &quot;marker&quot;,
    color  = ~paste0(&quot;Species group &quot;, group),
    colors = ~pal_okabe_ito
    ) |&gt; 
  add_markers(
    x = ~x_c, y = ~y_c, z = ~z_c, 
    mode = &quot;marker&quot;,
    marker = list(size = size, symbol = &quot;circle&quot;)
  ) |&gt; 
  add_markers(
    x = ~x_v, y = ~y_v, z = ~z_v, 
    mode = &quot;marker&quot;,
    marker = list(size = size, symbol = &quot;diamond&quot;)
  ) |&gt; 
  add_markers(
    x = ~x_cs, y = ~y_cs, z = ~z_cs, 
    mode = &quot;marker&quot;,
    marker = list(size = size, symbol = &quot;circle-open&quot;)
  ) |&gt; 
  add_markers(
    x = ~x_vs, y = ~y_vs, z = ~z_vs, 
    mode = &quot;marker&quot;,
    marker = list(size = size, symbol = &quot;diamond-open&quot;)
  ) |&gt; 
  layout(
    scene = list(
      xaxis = list(title = &quot;x&quot;),
      yaxis = list(title = &quot;y&quot;),
      zaxis = list(title = &quot;z&quot;)
    ),
    paper_bgcolor = &quot;transparent&quot;)</code>
      <fig id="fig-intermediate-embeddings-nb-article">
        <caption><p>Figure 4.6: Space of embeddings with 128 additional
        points based on the euclidean distances between the visual and
        categorical embeddings. The empty dots are the
        <italic>aphantasics-spatial</italic> ones, and the empty
        diamonds are the <italic>phantasic-spatial</italic> ones. Some
        can be very close together, and sometimes further apart due to
        the various pairs of visual and categorical points used to
        create them. A network-like structure seems to appear, with
        empty points seemingly ‘connecting’ the dots and diamonds.
        <bold><italic>Interact with the figure to see the
        details.</italic></bold></p></caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-intermediate-embeddings-1.png" />
      </fig>
      </sec>
    </sec>
    <sec id="labelling-the-species-nb-article">
      <title>Labelling the species</title>
      <p>The distributions created are still gathered around the
      centroids of each group, but are much more widespread, each group
      getting close to each other and even reaching inside the
      sphere.</p>
      <p>Perfect! Now we have two 3D embeddings per animal, in a
      categorical or a visual description of their features. Let’s add
      labels for each species in a group:</p>
      <sec id="nb-code-cell-5-nb-article" specific-use="notebook-code">
      <code language="r script">df_embeddings &lt;- 
  df_embeddings |&gt; 
  mutate(
    group = case_when(
    group == 1 ~ &quot;a&quot;, 
    group == 2 ~ &quot;b&quot;,
    group == 3 ~ &quot;c&quot;,
    group == 4 ~ &quot;d&quot;,
    group == 5 ~ &quot;e&quot;,
    group == 6 ~ &quot;f&quot;,
    group == 7 ~ &quot;g&quot;,
    group == 8 ~ &quot;h&quot;,
    TRUE ~ group
    )
  ) |&gt; 
  group_by(group) |&gt; 
  mutate(
    species = paste0(&quot;species_&quot;, group, 1:8),
    species = as.factor(species),
    group   = as.factor(group)
    ) |&gt; 
  select(group, species, everything())</code>
      </sec>
      <p>Now we have four sets of coherent coordinates, that we need to
      assign to the 30 participants: i.e. generating 8 points for C
      (aph_spa_low), 7 points for CS (aph_spa_high), 7 points for VS
      (phant_spa_high), and 8 points for V (phant_spa_low).</p>
    </sec>
    <sec id="generating-the-subject-embeddings-nb-article">
      <title>Generating the subject embeddings</title>
      <p>We have four “reference” sets of embeddings which represent
      animals either judged according to their similarity in categorical
      terms (namely, species), or in visual terms (namely shape or color
      similarities, assuming that these similarities are more evenly
      distributed, e.g. the crab looks like a spider, but is also pretty
      close to a scorpion, etc.).</p>
      <p>To generate the embeddings of each subject in each condition,
      we will start from these reference embeddings and generate random
      noise around <italic>each item</italic>, i.e. for all 64 animals.
      For 100 subjects, we would thus generate 100 noisy points around
      each animal, each point corresponding to a given subject.</p>
      <p>The visual and verbal groups will be generated with slightly
      more intra-group variance, so as to try to make the spatial groups
      as coherent as possible (and avoid blurring everything and making
      the groups disappear in noise).</p>
      <p>Although the groupings in this distribution sound simple when
      we color it using the knowledge about how we built it, the
      algorithm will only be fed with the data for each subject, without
      any labeling or additional information. Thus,
      <xref alt="Figure 4.8" rid="fig-subject-embeddings-b-nb-article">Figure 4.8</xref>
      here is what the algorithm will “see” (and what it will try to
      decrypt). Admittedly, that looks a lot more complicated.</p>
      <sec id="subject-embeddings-nb-article" specific-use="notebook-code">
      <code language="python"># creating dfs with participants
df_subjects_7 &lt;- 
  tibble(subject = seq(1, 7, 1)) |&gt; 
  mutate(subject = paste0(&quot;subject_&quot;, subject))

df_subjects_8 &lt;- 
  tibble(subject = seq(1, 8, 1)) |&gt; 
  mutate(subject = paste0(&quot;subject_&quot;, subject))

# splitting df_embeddings
df_embed_c  &lt;- df_embeddings |&gt; select(group, species,  x_c:z_c)
df_embed_cs &lt;- df_embeddings |&gt; select(group, species, x_cs:z_cs)
df_embed_vs &lt;- df_embeddings |&gt; select(group, species, x_vs:z_vs)
df_embed_v  &lt;- df_embeddings |&gt; select(group, species,  x_v:z_v)

# function to create embeddings per subject with normal random noise
generate_subject_embeddings &lt;- function(df, df_subjects, var){
  df &lt;-
    df |&gt; 
    mutate(subject = list(df_subjects)) |&gt; 
    unnest(subject) |&gt; 
    group_by(species) |&gt; 
    # simulating x coordinates
    sim_discr(
      n_vars = 1, 
      var = var, 
      cov = 0, 
      group_means = pull(df, 3), 
      name = &quot;x&quot;) |&gt; 
    # simulating y coordinates
    sim_discr(
      n_vars = 1, 
      var = var, 
      cov = 0, 
      group_means = pull(df, 4), 
      name = &quot;y&quot;) |&gt; 
    # simulating z coordinates
    sim_discr(
      n_vars = 1, 
      var = var, 
      cov = 0, 
      group_means = pull(df, 5), 
      name = &quot;z&quot;) |&gt;
  select(group, species, subject, 7:9) |&gt; 
  rename(x = 4, y = 5, z = 6) |&gt; 
  ungroup()
  
  return(df)
}

var_s1 = 0.001
var_s2 = 0.0005

df_embed_c_sub  &lt;- generate_subject_embeddings(df_embed_c,  df_subjects_4, var_s1)
df_embed_cs_sub &lt;- generate_subject_embeddings(df_embed_cs, df_subjects_4, var_s2)
df_embed_vs_sub &lt;- generate_subject_embeddings(df_embed_vs, df_subjects_4, var_s2)
df_embed_v_sub  &lt;- generate_subject_embeddings(df_embed_v,  df_subjects_4, var_s1)</code>
      </sec>
      <sec id="cell-fig-subject-embeddings-a-nb-article" specific-use="notebook-code">
      <code language="r script">size = 2

plot_ly(
    type = &quot;scatter3d&quot;, 
    mode = &quot;marker&quot;,
    colors = ~pal_okabe_ito
    ) |&gt; 
  add_markers(
    data = df_embed_c_sub, 
    x = ~x, y = ~y, z = ~z, color  = ~paste0(&quot;Species group &quot;, group),
    mode = &quot;marker&quot;,
    marker = list(size = size, symbol = &quot;circle&quot;)
  ) |&gt; 
  add_markers(
    data = df_embed_v_sub, 
    x = ~x, y = ~y, z = ~z, color  = ~paste0(&quot;Species group &quot;, group),
    mode = &quot;marker&quot;,
    marker = list(size = size, symbol = &quot;diamond&quot;)
  ) |&gt; 
  add_markers(
    data = df_embed_cs_sub, 
    x = ~x, y = ~y, z = ~z, color  = ~paste0(&quot;Species group &quot;, group), 
    mode = &quot;marker&quot;,
    marker = list(size = size, symbol = &quot;circle-open&quot;)
  ) |&gt; 
  add_markers(
    data = df_embed_vs_sub, 
    x = ~x, y = ~y, z = ~z, color  = ~paste0(&quot;Species group &quot;, group), 
    mode = &quot;marker&quot;,
    marker = list(size = size, symbol = &quot;diamond-open&quot;)
  ) |&gt; 
  layout(legend = list(
    yanchor = &quot;top&quot;,
    y = 1,
    xanchor = &quot;right&quot;,
    x = 0
    ),
    paper_bgcolor = &quot;transparent&quot;,
    showlegend = FALSE
    )</code>
      <fig id="fig-subject-embeddings-a-nb-article">
        <caption><p>Figure 4.7: Final distribution of the 64 embeddings
        of all the 30 subjects, amounting to 1920 points total.
        Embeddings are <bold><italic>colored by the species
        groups</italic></bold> they represent. The symbols represent the
        four imagery groups (Aph. verbal, spatial, etc.).
        <bold><italic>Interact with the figures to see the
        details.</italic></bold></p></caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-subject-embeddings-a-1.png" />
      </fig>
      </sec>
      <sec id="cell-fig-subject-embeddings-b-nb-article" specific-use="notebook-code">
      <code language="r script">bind_rows(
  # aph_spatial
  df_embed_cs_sub |&gt; 
    separate_wider_delim(
      subject,
      delim = &quot;_&quot;,
      names = c(&quot;subject&quot;, &quot;number&quot;)
    ) |&gt; 
    mutate(subject = paste0(subject,&quot;_a_aph_s&quot;, number), .keep = &quot;unused&quot;),
  
  # aph_verbal
  df_embed_c_sub  |&gt;
    separate_wider_delim(
      subject,
      delim = &quot;_&quot;,
      names = c(&quot;subject&quot;, &quot;number&quot;)
    ) |&gt; 
    mutate(subject = paste0(subject,&quot;_b_aph_v&quot;, number), .keep = &quot;unused&quot;),
  
  # phant spatial
  df_embed_vs_sub  |&gt;
    separate_wider_delim(
      subject,
      delim = &quot;_&quot;,
      names = c(&quot;subject&quot;, &quot;number&quot;)
    ) |&gt; 
    mutate(subject = paste0(subject,&quot;_c_phant_s&quot;, number), .keep = &quot;unused&quot;),
  
  # phant visual
  df_embed_v_sub  |&gt;
    separate_wider_delim(
      subject,
      delim = &quot;_&quot;,
      names = c(&quot;subject&quot;, &quot;number&quot;)
    ) |&gt; 
    mutate(subject = paste0(subject,&quot;_d_phant_v&quot;, number), .keep = &quot;unused&quot;)
  ) |&gt;
  plot_ly() |&gt; 
  add_markers(
      x = ~x, y = ~y, z = ~z,
      color = ~subject,
      colors = cool_30_colors,
      marker = list(size = 3)
    ) |&gt; 
  layout(
    paper_bgcolor = &quot;transparent&quot;,
    showlegend = FALSE
    )</code>
      <fig id="fig-subject-embeddings-b-nb-article">
        <caption><p>Figure 4.8: Final distribution of the 64 embeddings
        of all the 30 subjects, amounting to 1920 points total.
        Embeddings are <bold><italic>lored by subject</italic></bold>.
        The symbols represent the four imagery groups (Aph. verbal,
        spatial, etc.). <bold><italic>Interact with the figures to see
        the details.</italic></bold></p></caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-subject-embeddings-b-1.png" />
      </fig>
      </sec>
      <p>To feed this data to the algorithm, we’ll group the 64
      embeddings per subject in matrices tied to each of them.</p>
      <sec id="nb-code-cell-6-nb-article" specific-use="notebook-code">
      <code language="r script">df_embeddings_sub &lt;-
  bind_rows(
    # aphantasic spatial
    df_embed_cs_sub |&gt; 
      separate_wider_delim(
        subject,
        delim = &quot;_&quot;,
        names = c(&quot;subject&quot;, &quot;number&quot;)
      ) |&gt; 
      mutate(subject = paste0(subject,&quot;_a_aph_s&quot;, number), .keep = &quot;unused&quot;),
    
    # aphantasic verbal
    df_embed_c_sub  |&gt;
      separate_wider_delim(
        subject,
        delim = &quot;_&quot;,
        names = c(&quot;subject&quot;, &quot;number&quot;)
      ) |&gt; 
      mutate(subject = paste0(subject,&quot;_b_aph_v&quot;, number), .keep = &quot;unused&quot;),
    
    # phantasic spatial
    df_embed_vs_sub  |&gt;
      separate_wider_delim(
        subject,
        delim = &quot;_&quot;,
        names = c(&quot;subject&quot;, &quot;number&quot;)
      ) |&gt; 
      mutate(subject = paste0(subject,&quot;_c_phant_s&quot;, number), .keep = &quot;unused&quot;),
    
    # phantasic visual
    df_embed_v_sub  |&gt;
      separate_wider_delim(
        subject,
        delim = &quot;_&quot;,
        names = c(&quot;subject&quot;, &quot;number&quot;)
      ) |&gt; 
      mutate(subject = paste0(subject,&quot;_d_phant_v&quot;, number), .keep = &quot;unused&quot;)
  ) |&gt; 
  mutate(subject = as.factor(subject)) |&gt; 
  select(!c(group, species)) |&gt; 
  group_by(subject) |&gt; 
  nest() |&gt; 
  rename(embedding = data) |&gt; 
  rowwise() |&gt; 
  mutate(embedding = list(as.matrix(embedding)))</code>
      </sec>
    </sec>
  </sec>
  <sec id="data-analysis-aligning-representational-structures-nb-article">
    <title>4.3 Data analysis: Aligning representational
    structures</title>
  </sec>
  <sec id="simulation-summary-nb-article">
    <title>4.4 Simulation summary</title>
    <p>Kawakita et al.
    (<xref alt="2023" rid="ref-kawakita2023-nb-article" ref-type="bibr">2023</xref>):
    These results indicate that the difference between the qualia
    structures of neuro-typical and atypical participants is
    significantly larger than the difference between the qualia
    structures of neuro-typical participants.</p>
    <p>A notable difference is that greenish colors and reddish colors
    are close in the embedding space of color atypical participants
    while they are distant in the embedding space of color neurotypical
    participants. This structural difference is likely to prevent the
    unsupervised alignment between the embeddings of color-neurotypical
    and atypical participants even though the correlation coefficient
    between the dissimilarity matrices of color neuro-typical and
    atypical participants is reasonably high.</p>
    <p>For a long time, assessing the similarity of subjective
    experiences across participants has been challenging. To address
    this problem, we proposed the “qualia structure” paradigm, which
    focuses on quantitative structural comparisons of subjective
    experiences. Using an unsupervised alignment method, we were able to
    match the qualia structures of colors and natural objects of
    different groups of participants based only on the way the qualia
    relate to each other, without using any external labels.</p>
    <p>Our results on color qualia structures are consistent with an
    idea that the relational properties of color qualia are universally
    shared by color-neurotypical individuals. Intriguingly, our results
    also suggest that individuals with color-atypical vision may have a
    different structure of their color experiences, rather than just
    failing to experience a certain subset of colors. Longstanding
    thought experiments that challenge the feasibility of
    inter-subjective color comparisons, such as individuals with color
    qualia inversion, should be resolvable with our relational
    unsupervised approach. Beyond traditional measures such as Pearson’s
    correlation coefficient, our method provides a more fundamental
    structural characterization of how two structures are similar or
    different, which will be crucial for future investigations of qualia
    structures across psychological, neuroscientific, and computational
    fields.</p>
  </sec>
</sec>
<sec id="feasibility-nb-article">
  <title>5. Feasibility</title>
</sec>
<sec id="conclusion-nb-article">
  <title>6. Conclusion</title>
  <p>Modern psychology builds on the relativistic framework of
  philosophy, accepting that humans cannot know reality in an absolute
  sense. Focusing on relative comparisons, or similarity, is more than a
  clever philosophical work-around. similarity is a common currency of
  perception and cognition. In addition to operating at all levels of
  cognition, similarity—or, more accurately, the second-order
  isomorphism defined by a set of similarity relations—has been a
  powerful tool for analyzing and comparing psychological spaces.</p>
</sec>
</body>



<back>
<ref-list>
  <title></title>
  <ref id="ref-roads2024-nb-article">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Roads</surname><given-names>Brett D.</given-names></name>
        <name><surname>Love</surname><given-names>Bradley C.</given-names></name>
      </person-group>
      <article-title>Modeling similarity and psychological space</article-title>
      <source>Annual Review of Psychology</source>
      <year iso-8601-date="2024-01">2024</year><month>01</month>
      <volume>75</volume>
      <issue>1</issue>
      <issn>0066-4308, 1545-2085</issn>
      <uri>https://www.annualreviews.org/doi/10.1146/annurev-psych-040323-115131</uri>
      <pub-id pub-id-type="doi">10.1146/annurev-psych-040323-115131</pub-id>
      <fpage>215</fpage>
      <lpage>240</lpage>
    </element-citation>
  </ref>
  <ref id="ref-shepardSecondorderIsomorphismInternal1970-nb-article">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Shepard</surname><given-names>Roger N</given-names></name>
        <name><surname>Chipman</surname><given-names>Susan</given-names></name>
      </person-group>
      <article-title>Second-order isomorphism of internal representations: Shapes of states</article-title>
      <source>Cognitive Psychology</source>
      <year iso-8601-date="1970-01">1970</year><month>01</month>
      <volume>1</volume>
      <issue>1</issue>
      <issn>0010-0285</issn>
      <uri>https://www.sciencedirect.com/science/article/pii/0010028570900022</uri>
      <pub-id pub-id-type="doi">10.1016/0010-0285(70)90002-2</pub-id>
      <fpage>1</fpage>
      <lpage>17</lpage>
    </element-citation>
  </ref>
  <ref id="ref-sasakiToolboxGromovWassersteinOptimal2023-nb-article">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Sasaki</surname><given-names>Masaru</given-names></name>
        <name><surname>Takeda</surname><given-names>Ken</given-names></name>
        <name><surname>Abe</surname><given-names>Kota</given-names></name>
        <name><surname>Oizumi</surname><given-names>Masafumi</given-names></name>
      </person-group>
      <article-title>Toolbox for gromov-wasserstein optimal transport: Application to unsupervised alignment in neuroscience</article-title>
      <year iso-8601-date="2023">2023</year>
      <uri>https://www.biorxiv.org/content/10.1101/2023.09.15.558038v1</uri>
      <pub-id pub-id-type="doi">10.1101/2023.09.15.558038</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-bainbridgeQuantifyingAphantasiaDrawing2021-nb-article">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bainbridge</surname><given-names>Wilma A.</given-names></name>
        <name><surname>Pounder</surname><given-names>Zoë</given-names></name>
        <name><surname>Eardley</surname><given-names>Alison F.</given-names></name>
        <name><surname>Baker</surname><given-names>Chris I.</given-names></name>
      </person-group>
      <article-title>Quantifying aphantasia through drawing: Those without visual imagery show deficits in object but not spatial memory</article-title>
      <source>Cortex</source>
      <year iso-8601-date="2021-02-01">2021</year><month>02</month><day>01</day>
      <volume>135</volume>
      <uri>https://www.sciencedirect.com/science/article/pii/S0010945220304317</uri>
      <pub-id pub-id-type="doi">10.1016/j.cortex.2020.11.014</pub-id>
      <fpage>159</fpage>
      <lpage>172</lpage>
    </element-citation>
  </ref>
  <ref id="ref-kriegeskorteInverseMDSInferring2012-nb-article">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name>
        <name><surname>Mur</surname><given-names>Marieke</given-names></name>
      </person-group>
      <article-title>Inverse MDS: Inferring dissimilarity structure from multiple item arrangements</article-title>
      <source>Frontiers in Psychology</source>
      <year iso-8601-date="2012">2012</year>
      <volume>3</volume>
      <uri>https://www.frontiersin.org/articles/10.3389/fpsyg.2012.00245</uri>
    </element-citation>
  </ref>
  <ref id="ref-decockSimilarityGoodman2011-nb-article">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Decock</surname><given-names>Lieven</given-names></name>
        <name><surname>Douven</surname><given-names>Igor</given-names></name>
      </person-group>
      <article-title>Similarity After Goodman</article-title>
      <source>Review of Philosophy and Psychology</source>
      <year iso-8601-date="2011-03-01">2011</year><month>03</month><day>01</day>
      <volume>2</volume>
      <issue>1</issue>
      <uri>https://doi.org/10.1007/s13164-010-0035-y</uri>
      <pub-id pub-id-type="doi">10.1007/s13164-010-0035-y</pub-id>
      <fpage>61</fpage>
      <lpage>75</lpage>
    </element-citation>
  </ref>
  <ref id="ref-goodmanSevenStricturesSimilarity1972-nb-article">
    <element-citation publication-type="chapter">
      <person-group person-group-type="author">
        <name><surname>Goodman</surname><given-names>Nelson</given-names></name>
      </person-group>
      <article-title>Seven strictures on similarity</article-title>
      <publisher-name>Bobs-Merril</publisher-name>
      <year iso-8601-date="1972">1972</year>
    </element-citation>
  </ref>
  <ref id="ref-gardenforsConceptualSpacesFramework2004-nb-article">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Gardenfors</surname><given-names>Peter</given-names></name>
      </person-group>
      <article-title>Conceptual spaces as a framework for knowledge representation</article-title>
      <year iso-8601-date="2004">2004</year>
    </element-citation>
  </ref>
  <ref id="ref-murHumanObjectSimilarityJudgments2013-nb-article">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Mur</surname><given-names>Marieke</given-names></name>
        <name><surname>Meys</surname><given-names>Mirjam</given-names></name>
        <name><surname>Bodurka</surname><given-names>Jerzy</given-names></name>
        <name><surname>Goebel</surname><given-names>Rainer</given-names></name>
        <name><surname>Bandettini</surname><given-names>Peter</given-names></name>
        <name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name>
      </person-group>
      <article-title>Human object-similarity judgments reflect and transcend the primate-IT object representation</article-title>
      <source>Frontiers in Psychology</source>
      <year iso-8601-date="2013">2013</year>
      <volume>4</volume>
      <uri>https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00128</uri>
    </element-citation>
  </ref>
  <ref id="ref-majewskaSpatialMultiarrangementClustering2020-nb-article">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Majewska</surname><given-names>O.</given-names></name>
        <name><surname>McCarthy</surname><given-names>D.</given-names></name>
        <name><surname>Bosch</surname><given-names>J. van den</given-names></name>
        <name><surname>Kriegeskorte</surname><given-names>N.</given-names></name>
        <name><surname>Vulic</surname><given-names>I.</given-names></name>
        <name><surname>Korhonen</surname><given-names>A.</given-names></name>
      </person-group>
      <source>Spatial multi-arrangement for clustering and multi-way similarity dataset construction</source>
      <publisher-name>European Language Resources Association</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <uri>https://www.repository.cam.ac.uk/handle/1810/306834</uri>
    </element-citation>
  </ref>
  <ref id="ref-gardenfors2004-nb-article">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Gardenfors</surname><given-names>Peter</given-names></name>
      </person-group>
      <article-title>Conceptual Spaces as a Framework for Knowledge Representation</article-title>
      <year iso-8601-date="2004">2004</year>
    </element-citation>
  </ref>
  <ref id="ref-jozwik2016-nb-article">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Jozwik</surname><given-names>Kamila M.</given-names></name>
        <name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name>
        <name><surname>Mur</surname><given-names>Marieke</given-names></name>
      </person-group>
      <article-title>Visual features as stepping stones toward semantics: Explaining object similarity in IT and perception with non-negative least squares</article-title>
      <source>Neuropsychologia</source>
      <year iso-8601-date="2016-03-01">2016</year><month>03</month><day>01</day>
      <volume>83</volume>
      <uri>https://www.sciencedirect.com/science/article/pii/S0028393215301998</uri>
      <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2015.10.023</pub-id>
      <fpage>201</fpage>
      <lpage>226</lpage>
    </element-citation>
  </ref>
  <ref id="ref-jozwik2017-nb-article">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Jozwik</surname><given-names>Kamila M.</given-names></name>
        <name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name>
        <name><surname>Storrs</surname><given-names>Katherine R.</given-names></name>
        <name><surname>Mur</surname><given-names>Marieke</given-names></name>
      </person-group>
      <article-title>Deep convolutional neural networks outperform feature-based but not categorical models in explaining object similarity judgments</article-title>
      <source>Frontiers in Psychology</source>
      <year iso-8601-date="2017">2017</year>
      <volume>8</volume>
      <uri>https://www.frontiersin.org/articles/10.3389/fpsyg.2017.01726</uri>
    </element-citation>
  </ref>
  <ref id="ref-marr1997-nb-article">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Marr</surname><given-names>D.</given-names></name>
        <name><surname>Nishihara</surname><given-names>H. K.</given-names></name>
        <name><surname>Brenner</surname><given-names>Sydney</given-names></name>
      </person-group>
      <article-title>Representation and recognition of the spatial organization of three-dimensional shapes</article-title>
      <source>Proceedings of the Royal Society of London. Series B. Biological Sciences</source>
      <year iso-8601-date="1997-01">1997</year><month>01</month>
      <volume>200</volume>
      <issue>1140</issue>
      <uri>https://royalsocietypublishing.org/doi/abs/10.1098/rspb.1978.0020</uri>
      <pub-id pub-id-type="doi">10.1098/rspb.1978.0020</pub-id>
      <fpage>269</fpage>
      <lpage>294</lpage>
    </element-citation>
  </ref>
  <ref id="ref-kriegeskorte2008-nb-article">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name>
        <name><surname>Mur</surname><given-names>Marieke</given-names></name>
        <name><surname>Bandettini</surname><given-names>Peter</given-names></name>
      </person-group>
      <article-title>Representational similarity analysis - connecting the branches of systems neuroscience</article-title>
      <source>Frontiers in Systems Neuroscience</source>
      <year iso-8601-date="2008">2008</year>
      <volume>2</volume>
      <uri>https://www.frontiersin.org/articles/10.3389/neuro.06.004.2008</uri>
    </element-citation>
  </ref>
  <ref id="ref-kawakita2023-nb-article">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kawakita</surname><given-names>Genji</given-names></name>
        <name><surname>Zeleznikow-Johnston</surname><given-names>Ariel</given-names></name>
        <name><surname>Takeda</surname><given-names>Ken</given-names></name>
        <name><surname>Tsuchiya</surname><given-names>Naotsugu</given-names></name>
        <name><surname>Oizumi</surname><given-names>Masafumi</given-names></name>
      </person-group>
      <article-title>Is my&quot; red&quot; your&quot; red&quot;?: Unsupervised alignment of qualia structures via optimal transport</article-title>
      <year iso-8601-date="2023">2023</year>
    </element-citation>
  </ref>
</ref-list>
<fn-group>
  <fn id="fn1-nb-article">
    <label>1</label><p>A claim dismissed since then by propositions of
    robust mathematical models of similarity, e.g. Gardenfors
    (<xref alt="2004a" rid="ref-gardenforsConceptualSpacesFramework2004-nb-article" ref-type="bibr">2004a</xref>),
    Decock and Douven
    (<xref alt="2011" rid="ref-decockSimilarityGoodman2011-nb-article" ref-type="bibr">2011</xref>).</p>
  </fn>
  <fn id="fn2-nb-article">
    <label>2</label><p>A simpler alternative would be generating the
    visual embeddings with the same code as the categorical ones,
    selecting 8 points per cluster but much more spread out
    (e.g. selecting 8 among the 90% closest to the centroids, which
    would create more variability than the categorical one set to 60%).
    I chose otherwise because this wouldn’t have had points reaching
    <italic>inside</italic> the sphere.</p>
  </fn>
</fn-group>
</back>


</sub-article>
<sub-article article-type="notebook" id="nb-4-nb-1">
<front-stub>
<title-group>
<article-title>Unsupervised alignment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Delem</surname>
<given-names>Maël</given-names>
</name>
<string-name>Maël Delem</string-name>

</contrib>
</contrib-group>
</front-stub>

<body>
<boxed-text>
  <disp-quote>
    <p><bold>Packages and setup</bold></p>
    <p>Down below is the code to load necessary packages used for the
    simulation and analysis, along with some setups for the whole
    document (<italic>hover over the numbers on the far right for
    additional explanation of code and mechanics</italic>).</p>
    <sec id="nb-code-cell-1-nb-1" specific-use="notebook-code">
    <code id="annotated-cell-1-nb-1" language="r script">
# ═══ Packages ═════════════════════════════════════════════════════════════════

if (!require(librarian)) install.packages(librarian)
    </code>
    <def-list>
      <def-item>
        <term>Line 4</term>
        <def>
          <p>The package <monospace>librairian</monospace> eases package
          management with the “shelf” function, which automatically: (1)
          checks if a package is installed; (2) installs it if need be;
          (3) loads the package like the “library()” function would.</p>
        </def>
      </def-item>
      <def-item>
        <term>Line 28</term>
        <def>
          <p><monospace>reticulate</monospace> allows to translate and
          transfer objects and functions from R to Python and
          vice-versa, and was thus of primary importance for the
          successful use of the Python toolbox on our simulated
          data.</p>
        </def>
      </def-item>
      <def-item>
        <term>Lines 35,51</term>
        <def>
          <p>These are personal custom color palettes meant to extend my
          favourite palette, the color-atypical friendly Okabe-Ito color
          palette. The palette originally has only eight colors, but I
          will need nine, then up to 30 for later graphs, so I extended
          it with a hand-picked selection of mine.</p>
        </def>
      </def-item>
    </def-list>
    <code id="annotated-cell-2-nb-1" language="r script">
library(librarian)

# now putting packages on our library's shelves:
shelf(
  # ─── data management ─────────────────
  holodeck,       # simulating multivariate data
  cluster,        # dissimilarity matrices
  
  # ─── modelling ───────────────────────
  mclust,         # mixture clustering
  
  #  data visualization ──────────────
  # palettes
  viridis,        # colour-blind friendly palettes
  # interactive
  plotly,         # interactive plots
  ggdendro,       # dendrograms
  seriation,      # dissimilarity plots
  webshot2,       # HTML screenshots for Word render
  webshot,
  
  # ─── essential package collections ───
  doParallel,     # parallel execution
  easystats,      # data analysis ecosystem
  reticulate,     # R to Python
  tidyverse,      # modern R ecosystem
)

# ─── Global cosmetic theme ───
theme_set(theme_modern(base_size = 14))

pal_okabe_ito &lt;- c(
  &quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;,                            
  &quot;#F5C710&quot;, &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;, &quot;#6c0009&quot;)      

# We'll need 9 colors at some point
pal_okabe_ito_extended &lt;- c(                                 
  &quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;,                           
  &quot;#F5C710&quot;, &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;, &quot;#6c0009&quot;, &quot;#414487FF&quot;)

# We'll need 30 colors at another moment
cool_30_colors &lt;- c(                                                   
  &quot;#3d51b4&quot;, &quot;#414487FF&quot;, &quot;#003d73&quot;, &quot;#440154FF&quot;, &quot;#6c0009&quot;, &quot;#b64e4e&quot;,
  &quot;#D55E00&quot;, &quot;#E69F00&quot;, &quot;#F5C710&quot;, &quot;#FDE725FF&quot;, &quot;#f2bb7b&quot;, &quot;#f1afad&quot;, &quot;#CC79A7&quot;, 
  &quot;#e57774&quot;, &quot;#7AD151FF&quot;, &quot;#57b571&quot;, &quot;#318a4a&quot;, &quot;#009E73&quot;, &quot;#22A884FF&quot;, 
  &quot;#2A788EFF&quot;, &quot;#0072B2&quot;, &quot;#2da6b5&quot;, &quot;#56B4E9&quot;, &quot;#889be0&quot;, &quot;#6677e0&quot;,   
  &quot;#3d51b4&quot;, &quot;#414487FF&quot;, &quot;#003d73&quot;, &quot;#440154FF&quot;, &quot;#6c0009&quot;, &quot;#b64e4e&quot;  
  )
    </code>
    <sec id="nb-code-cell-1-output-0-nb-1" specific-use="notebook-output">
    <preformat>Le chargement a nécessité le package : librarian</preformat>
    </sec>
    </sec>
  </disp-quote>
</boxed-text>
</body>



<back>
</back>


</sub-article>
<sub-article article-type="notebook" id="nb-8-nb-2">
<front-stub>
<title-group>
<article-title>Study simulation</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Delem</surname>
<given-names>Maël</given-names>
</name>
<string-name>Maël Delem</string-name>

</contrib>
</contrib-group>
</front-stub>

<body>
<boxed-text>
  <disp-quote>
    <p><bold>Packages and setup</bold></p>
    <p>Down below is the code to load necessary packages used for the
    simulation and analysis, along with some setups for the whole
    document (<italic>hover over the numbers on the far right for
    additional explanation of code and mechanics</italic>).</p>
    <sec id="nb-code-cell-1-nb-2" specific-use="notebook-code">
    <code id="annotated-cell-6-nb-2" language="r script">
# ═══ Packages ═════════════════════════════════════════════════════════════════

if (!require(librarian)) install.packages(librarian)
    </code>
    <def-list>
      <def-item>
        <term>Line 4</term>
        <def>
          <p>The package <monospace>librairian</monospace> eases package
          management with the “shelf” function, which automatically: (1)
          checks if a package is installed; (2) installs it if need be;
          (3) loads the package like the “library()” function would.</p>
        </def>
      </def-item>
      <def-item>
        <term>Line 28</term>
        <def>
          <p><monospace>reticulate</monospace> allows to translate and
          transfer objects and functions from R to Python and
          vice-versa, and was thus of primary importance for the
          successful use of the Python toolbox on our simulated
          data.</p>
        </def>
      </def-item>
      <def-item>
        <term>Lines 35,51</term>
        <def>
          <p>These are personal custom color palettes meant to extend my
          favourite palette, the color-atypical friendly Okabe-Ito color
          palette. The palette originally has only eight colors, but I
          will need nine, then up to 30 for later graphs, so I extended
          it with a hand-picked selection of mine.</p>
        </def>
      </def-item>
      <def-item>
        <term>Lines 55,69</term>
        <def>
          <p>These are R objects that were the results of a previous run
          of the simulation.</p>
        </def>
      </def-item>
    </def-list>
    <code id="annotated-cell-7-nb-2" language="r script">
library(librarian)

# now putting packages on our library's shelves:
shelf(
  # ─── data management ─────────────────
  holodeck,       # simulating multivariate data
  cluster,        # dissimilarity matrices
  
  # ─── modelling ───────────────────────
  mclust,         # mixture clustering
  
  #  data visualization ──────────────
  # palettes
  viridis,        # colour-blind friendly palettes
  # interactive
  plotly,         # interactive plots
  ggdendro,       # dendrograms
  seriation,      # dissimilarity plots
  webshot2,       # HTML screenshots for Word render
  webshot,
  
  # ─── essential package collections ───
  doParallel,     # parallel execution
  easystats,      # data analysis ecosystem
  reticulate,     # R to Python
  tidyverse,      # modern R ecosystem
)

# ─── Global cosmetic theme ───
theme_set(theme_modern(base_size = 14))

pal_okabe_ito &lt;- c(
  &quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;,                            
  &quot;#F5C710&quot;, &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;, &quot;#6c0009&quot;)      

# We'll need 9 colors at some point
pal_okabe_ito_extended &lt;- c(                                 
  &quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;,                           
  &quot;#F5C710&quot;, &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;, &quot;#6c0009&quot;, &quot;#414487FF&quot;)

# We'll need 30 colors at another moment
cool_30_colors &lt;- c(                                                   
  &quot;#3d51b4&quot;, &quot;#414487FF&quot;, &quot;#003d73&quot;, &quot;#440154FF&quot;, &quot;#6c0009&quot;, &quot;#b64e4e&quot;,
  &quot;#D55E00&quot;, &quot;#E69F00&quot;, &quot;#F5C710&quot;, &quot;#FDE725FF&quot;, &quot;#f2bb7b&quot;, &quot;#f1afad&quot;, &quot;#CC79A7&quot;, 
  &quot;#e57774&quot;, &quot;#7AD151FF&quot;, &quot;#57b571&quot;, &quot;#318a4a&quot;, &quot;#009E73&quot;, &quot;#22A884FF&quot;, 
  &quot;#2A788EFF&quot;, &quot;#0072B2&quot;, &quot;#2da6b5&quot;, &quot;#56B4E9&quot;, &quot;#889be0&quot;, &quot;#6677e0&quot;,   
  &quot;#3d51b4&quot;, &quot;#414487FF&quot;, &quot;#003d73&quot;, &quot;#440154FF&quot;, &quot;#6c0009&quot;, &quot;#b64e4e&quot;  
  )

path = &quot;data/&quot;

df &lt;- read_rds(paste0(path, &quot;df.RDS&quot;))
df_embeds &lt;- read_rds(paste0(path, &quot;df_embeds.RDS&quot;))
# Categorical and visual embeddings
df_embeds_categ  &lt;- read_rds(paste0(path, &quot;df_embeds_categ.RDS&quot;))
df_embeds_visual &lt;- read_rds(paste0(path, &quot;df_embeds_visual.RDS&quot;))
# Subject embeddings per sub-group
df_embed_c_sub  &lt;- read_rds(paste0(path, &quot;df_embed_c_sub.RDS&quot;))
df_embed_cs_sub &lt;- read_rds(paste0(path, &quot;df_embed_cs_sub.RDS&quot;))
df_embed_v_sub  &lt;- read_rds(paste0(path, &quot;df_embed_v_sub.RDS&quot;))
df_embed_vs_sub &lt;- read_rds(paste0(path, &quot;df_embed_vs_sub.RDS&quot;))
# Accuracy of the unsupervised alignment (bad = not tidy data)
df_accuracy_all_bad &lt;- read_rds(paste0(path, &quot;df_accuracy_all_bad.RDS&quot;))
df_accuracy_cat_bad &lt;- read_rds(paste0(path, &quot;df_accuracy_cat_bad.RDS&quot;))
# Coordinates of the aligned embeddings from the Python output
coordinates_aligned_embeddings &lt;- read_rds(paste0(path, &quot;coordinates_aligned_embeddings.RDS&quot;))
    </code>
    <sec id="nb-code-cell-1-output-0-nb-2" specific-use="notebook-output">
    <preformat>Le chargement a nécessité le package : librarian</preformat>
    </sec>
    </sec>
  </disp-quote>
</boxed-text>
<sec id="visual-spatial-verbal-model-of-cognitive-profiles-nb-2">
  <title>Visual-spatial-verbal model of cognitive profiles</title>
  <p>We are going to simulate 30 participants presenting four different
  cognitive profiles, that I defined as, respectively,
  <italic>verbal</italic> aphantasics, <italic>spatial</italic>
  aphantasics, <italic>spatial</italic> phantasics, and
  <italic>visual</italic> phantasics. Their imagery abilities are
  summarised in @tbl-imageries.</p>
  <p>To simulate these four sub-groups, we use the
  <monospace>holodeck</monospace> R package to generate multivariate
  normal distributions of scores on these three dimensions for each
  sub-group. For instance, verbal aphantasics have normally distributed
  visual imagery scores centered around a mean of 0 (normalized, so
  negative scores are possible), 0.4 for spatial imagery, and 0.7 for
  verbal style; Spatial aphantasics have means of 0 for visual, 0.75
  spatial, and 0.3 for verbal; etc. The numbers are arbitrary, but have
  been chosen by trial-and-error to obtain a model that is both
  well-defined and not exaggerated.</p>
  <sec id="nb-code-cell-2-nb-2" specific-use="notebook-code">
  <code language="r script">
# The function takes the variance and covariance of the imagery distributions
# as arguments
generate_osv_model &lt;- function(var, cov){
  df &lt;- 
    tibble(group = rep(c(&quot;aph&quot;, &quot;phant&quot;), each = 8)) |&gt; 
    group_by(group) |&gt; 
    mutate(
      spatial_group = c(rep(&quot;spa_low&quot;, 4), rep(&quot;spa_high&quot;, 4)),
      vis_spa_group = paste0(group, &quot;_&quot;, spatial_group),
      verbal_group = &quot;verbal_low&quot;,
      verbal_group  = case_when(
        vis_spa_group == &quot;aph_spa_low&quot; ~ &quot;verbal_high&quot;, 
        vis_spa_group == &quot;phant_spa_low&quot; ~ &quot;verbal_mid&quot;,
        TRUE ~ verbal_group)
    ) |&gt; 
    group_by(vis_spa_group) |&gt; 
    # ─── visual ───
    sim_discr(
      n_vars = 1, 
      var = var, 
      cov = cov, 
      # aph_s, aph_v, phant_s, phant_v
      group_means = c(0, 0, 0.6, 0.87), 
      name = &quot;v&quot;) |&gt; 
    # ─── spatial ───
    sim_discr(
      n_vars = 1,  
      var = var, 
      cov = cov, 
      # aph_s, aph_v, phant_s, phant_v
      group_means = c(0.75, 0.4, 0.7, 0.3), 
      name = &quot;s&quot;) |&gt;
    # ─── verbal ───
    sim_discr(
      n_vars = 1,  
      var = var, 
      cov = cov, 
      # aph_s, aph_v, phant_s, phant_v
      group_means = c(0.3, 0.7, 0.3, 0.5), 
      name = &quot;i&quot;) |&gt;
    rename(
      visual_imagery  = v_1,
      spatial_imagery = s_1,
      verbal_profile  = i_1
      )
}

df &lt;- generate_osv_model(0.03, 0)
  </code>
  </sec>
  <sec id="generating-prototype-embeddings-from-a-sphere-nb-2">
    <title>Generating “prototype” embeddings from a sphere</title>
    <p>Proposal from
    <ext-link ext-link-type="uri" xlink:href="https://stats.stackexchange.com/questions/7977/how-to-generate-uniformly-distributed-points-on-the-surface-of-the-3-d-unit-sphe">StackExchange</ext-link>
    to generate points on a sphere:</p>
    <p>Let’s use a function to generate embeddings. We get 8 nicely
    distributed clusters. We’ll retrieve the centroids of each cluster,
    which would be the “perfect” categories of each species group (say,
    generated by a computational model on categorical criteria).</p>
    <sec id="nb-code-cell-3-nb-2" specific-use="notebook-code">
    <code language="r script">
generate_sphere &lt;- function(n){
  z     &lt;- 2*runif(n) - 1          # uniform on [-1, 1]
  theta &lt;- 2*pi*runif(n) - pi      # uniform on [-pi, pi]
  x     &lt;- sin(theta)*sqrt(1-z^2)  # based on angle
  y     &lt;- cos(theta)*sqrt(1-z^2) 
  
  df &lt;- tibble(x = x, y = y, z = z)
  
  return(df)
}

# 1000 random observations with embeddings uniformly distributed on a sphere
df_embeds &lt;- generate_sphere(1000)

# Clustering the observations in 8 groups based on their coordinates
clusters &lt;- Mclust(df_embeds, G = 8)

# adding the classification to the data
df_embeds &lt;- df_embeds |&gt; mutate(group = as.factor(clusters$classification))

# getting the centroids of each cluster
df_centroids &lt;- 
  df_embeds |&gt; 
  group_by(group) |&gt; 
  summarise(
    x_centroid = mean(x),
    y_centroid = mean(y),
    z_centroid = mean(z)
  )

# adding them to the data
df_embeds_2 &lt;- left_join(df_embeds, df_centroids, by = &quot;group&quot;)
    </code>
    </sec>
  </sec>
  <sec id="categorical-model-embeddings-nb-2">
    <title>Categorical model embeddings</title>
    <p>The selection procedure for the <bold>categorical model</bold>
    will consist of selecting points that are rather <italic>close to
    the centroids</italic>. Thus, we will filter the observations of the
    large sets to keep only points for which the distance to the
    centroid is inferior to a given value. That is, points for which the
    Euclidean norm of the vector from the observation to the
    centroid:</p>
    <p><disp-formula><alternatives>
    <tex-math><![CDATA[d(centroid, observation) = \sqrt{(x_{c} - x_{o})^{2} + (y_{c} - y_{o})^{2} + (z_{c} - z_{o})^{2}}]]></tex-math>
    <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>d</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:math></alternatives></disp-formula></p>
    <p>This can be done using the function
    <monospace>norm(coordinates, type = &quot;2&quot;)</monospace> in
    R.</p>
    <sec id="nb-code-cell-4-nb-2" specific-use="notebook-code">
    <code language="r script">
# Function to filter points of the sphere based on the distance to the centroids
generate_embeddings &lt;- function(df, n_embeddings, distance_quantile){
  df &lt;- 
    df |&gt; 
    # computing the euclidean distance to the centroids for each observation
    rowwise() |&gt; 
    mutate(
      distance = norm(
        c((x_centroid - x), (y_centroid - y), (z_centroid - z)),
        type = &quot;2&quot;)
      ) |&gt; 
    # filtering by distance to the centroid by group
    group_by(group) |&gt; 
    # selecting the X% closest (specified with &quot;distance_quantile&quot;)
    filter(distance &lt; quantile(distance, probs = distance_quantile)) |&gt; 
    # selecting X random observations per cluster in these 
    # (specified with &quot;n_embeddings&quot;)
    slice(1:n_embeddings) |&gt; 
    select(group, x, y, z) |&gt;
    ungroup()
}

df_embeds_categ &lt;- generate_embeddings(df_embeds_2, 8, 0.5)
    </code>
    </sec>
  </sec>
  <sec id="visual-model-embeddings-nb-2">
    <title>Visual model embeddings</title>
    <p>In the case of the <bold>visual model</bold>, we would like
    approximately evenly distributed embeddings, that could also dive
    <italic>inside</italic> the sphere, i.e. representing species that
    are visually close although diametrically opposed when it comes to
    taxonomy. To do this we could try to simulate multivariate normal
    distributions around the
    centroids<xref ref-type="fn" rid="fn1-nb-2">1</xref>. This can be done
    with the <monospace>holodeck</monospace> package.</p>
    <sec id="nb-code-cell-5-nb-2" specific-use="notebook-code">
    <code language="r script">
# defining the variance and covariance of the distributions
var2 &lt;- 0.05
cov2 &lt;- 0

# generating multivariate distributions around the categorical 3D means
df_embeds_visual &lt;-
  tibble(
    id = as.factor(seq(1,6400)),
    category = as.factor(rep(seq(1:64), each = 100))
  )|&gt; 
  group_by(category) |&gt; 
  sim_discr(
    n_vars = 1, 
    var = var2, 
    cov = cov2, 
    group_means = df_embeds_categ$x, 
    name = &quot;x&quot;) |&gt; 
  sim_discr(
    n_vars = 1, 
    var = var2, 
    cov = cov2, 
    group_means = df_embeds_categ$y, 
    name = &quot;y&quot;) |&gt; 
  sim_discr(
    n_vars = 1, 
    var = var2, 
    cov = cov2, 
    group_means = df_embeds_categ$z, 
    name = &quot;z&quot;) |&gt; 
  # keeping only 8 points per distribution
  slice(1) |&gt; 
  ungroup() |&gt; 
  mutate(group = as.factor(rep(seq(1, 8), each = 8))) |&gt; 
  rename(x = x_1, y = y_1, z = z_1) |&gt; 
  select(group, x, y, z)
    </code>
    </sec>
  </sec>
  <sec id="generating-the-subject-embeddings-nb-2">
    <title>Generating the subject embeddings</title>
    <p>We have four “reference” sets of embeddings which represent
    animals either judged according to their similarity in categorical
    terms (namely, species), or in visual terms (namely shape or color
    similarities, assuming that these similarities are more evenly
    distributed, e.g. the crab looks like a spider, but is also pretty
    close to a scorpion, etc.).</p>
    <p>To generate the embeddings of each subject in each condition, we
    will start from these reference embeddings and generate random noise
    around <italic>each item</italic>, i.e. for all 64 animals. For 100
    subjects, we would thus generate 100 noisy points around each
    animal, each point corresponding to a given subject.</p>
    <p>The visual and verbal groups will be generated with slightly more
    intra-group variance, so as to try to make the spatial groups as
    coherent as possible (and avoid blurring everything and making the
    groups disappear in noise).</p>
    <sec id="nb-code-cell-6-nb-2" specific-use="notebook-code">
    <code language="r script">
# creating dfs with participants
df_subjects_7 &lt;- 
  tibble(subject = seq(1, 7, 1)) |&gt; 
  mutate(subject = paste0(&quot;subject_&quot;, subject))

df_subjects_8 &lt;- 
  tibble(subject = seq(1, 8, 1)) |&gt; 
  mutate(subject = paste0(&quot;subject_&quot;, subject))

# splitting df_embeddings
df_embed_c  &lt;- df_embeddings |&gt; select(group, species,  x_c:z_c)
df_embed_cs &lt;- df_embeddings |&gt; select(group, species, x_cs:z_cs)
df_embed_vs &lt;- df_embeddings |&gt; select(group, species, x_vs:z_vs)
df_embed_v  &lt;- df_embeddings |&gt; select(group, species,  x_v:z_v)

# function to create embeddings per subject with normal random noise
generate_subject_embeddings &lt;- function(df, df_subjects, var){
  df &lt;-
    df |&gt; 
    mutate(subject = list(df_subjects)) |&gt; 
    unnest(subject) |&gt; 
    group_by(species) |&gt; 
    # simulating x coordinates
    sim_discr(
      n_vars = 1, 
      var = var, 
      cov = 0, 
      group_means = pull(df, 3), 
      name = &quot;x&quot;) |&gt; 
    # simulating y coordinates
    sim_discr(
      n_vars = 1, 
      var = var, 
      cov = 0, 
      group_means = pull(df, 4), 
      name = &quot;y&quot;) |&gt; 
    # simulating z coordinates
    sim_discr(
      n_vars = 1, 
      var = var, 
      cov = 0, 
      group_means = pull(df, 5), 
      name = &quot;z&quot;) |&gt;
  select(group, species, subject, 7:9) |&gt; 
  rename(x = 4, y = 5, z = 6) |&gt; 
  ungroup()
  
  return(df)
}

var_s1 = 0.001
var_s2 = 0.0005

df_embed_c_sub  &lt;- generate_subject_embeddings(df_embed_c,  df_subjects_4, var_s1)
df_embed_cs_sub &lt;- generate_subject_embeddings(df_embed_cs, df_subjects_4, var_s2)
df_embed_vs_sub &lt;- generate_subject_embeddings(df_embed_vs, df_subjects_4, var_s2)
df_embed_v_sub  &lt;- generate_subject_embeddings(df_embed_v,  df_subjects_4, var_s1)
    </code>
    </sec>
  </sec>
</sec>
</body>



<back>
<fn-group>
  <fn id="fn1-nb-2">
    <label>1</label><p>A simpler alternative would be generating the
    visual embeddings with the same code as the categorical ones,
    selecting 8 points per cluster but much more spread out
    (e.g. selecting 8 among the 90% closest to the centroids, which
    would create more variability than the categorical one set to 60%).
    I chose otherwise because this wouldn’t have had points reaching
    <italic>inside</italic> the sphere.</p>
  </fn>
</fn-group>
</back>


</sub-article>

</article>