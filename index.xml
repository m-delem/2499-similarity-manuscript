<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving
and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">

<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">

<front>


<article-meta>


<title-group>
<article-title>Unravelling mental representations in aphantasia through
unsupervised alignment</article-title>
<subtitle>Project design and data analysis simulation</subtitle>
</title-group>

<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Delem</surname>
<given-names>Maël</given-names>
</name>
<string-name>Maël Delem</string-name>

</contrib>
</contrib-group>









<history></history>


<abstract>
<p>Research on aphantasia is confronted with a long-standing conundrum
of all research on consciousness and representations, namely the
theoretical inaccessibility of subjective representations. Drawing on
concepts from similarity and representation research, I endorse the view
that the study of an individual’s mental representations is made
possible by exploiting second-order isomorphism. The concept of
second-order isomorphism means that correspondence should not be sought
in the first-order relation between (a) an external object and (b) the
corresponding internal representation, but in the second-order relation
between (a) the perceived similarities between various external objects
and (b) the similarities between their corresponding internal
representations. Building on this idea, this study project report is
divided into five parts. <bold>First</bold>, I outline the central ideas
underlying similarity research and its applicability to aphantasia
research. <bold>Second</bold>, I present a methodological rationale and
protocol based on inverse multidimensional scaling that can be
implemented online to conduct such large-scale research with high
efficiency. <bold>Third</bold>, I present a data analysis plan using a
state-of-the-art method for similarity analysis, unsupervised alignment
with Gromov-Wasserstein optimal transport (GWOT). <bold>Fourth</bold>, I
report a data simulation of a potential outcome of this project and the
successful analysis of this synthetic data using GWOT alignment.
<bold>Fifth</bold>, I analyse the feasability of such a project given
the material constraints of my thesis. I conclude with the expected
utility and benefits of this project.</p>
</abstract>




</article-meta>

</front>

<body>
<boxed-text>
<p><bold>Project inception</bold></p>
<p>This project stems from several elements:</p>
<list list-type="order">
  <list-item>
    <p>The long standing knowledge of the fact that internal
    representations seem impossible to reach due to their subjective
    nature.</p>
  </list-item>
  <list-item>
    <p>The discovery of the article of Shepard and Chipman
    (<xref alt="1970" rid="ref-shepardSecondorderIsomorphismInternal1970" ref-type="bibr">1970</xref>)
    that expose the idea of “second-order isomorphism”.</p>
  </list-item>
  <list-item>
    <p>The discovery of state-of-the-art and accessible unsupervised
    analytic methods to study this principle in an astonishing way. The
    last two discoveries (and many more) are the fruit of amazing
    discussions and recommendations from Ladislas when he came here.
    These motivated me to try to implement GWOT in R on data that I
    wanted to create myself to emulate a study we could do.</p>
  </list-item>
</list>
<p><italic>I promise that I did this mostly on my spare time, we have
too many other things to do elsewhere.</italic></p>
</boxed-text>
<sec id="theoretical-context">
  <title>1. Theoretical context</title>
  <sec id="psychological-spaces-and-aphantasia">
    <title>1.1 Psychological spaces and aphantasia</title>
    <p>While attempting to demonstrate the uselessness of the concept of
    similarity as a philosophical and scientific
    notion<xref ref-type="fn" rid="fn1">1</xref>, Goodman
    (<xref alt="1972" rid="ref-goodmanSevenStricturesSimilarity1972" ref-type="bibr">1972</xref>)
    has inadvertently expressed an aspect of similarity judgements of
    primary importance to us aphantasia researchers:</p>
    <disp-quote>
      <p>Comparative judgments of similarity often require not merely
      selection of relevant properties but a weighting of their relative
      importance, and variation in both relevance and importance can be
      rapid and enormous. Consider baggage at an airport checking
      station. The spectator may notice shape, size, color, material,
      and even make of luggage; the pilot is more concerned with weight,
      and the passenger with destination and ownership. Which pieces are
      more alike than others depends not only upon what properties they
      share, but upon who makes the comparison, and when. . . .
      Circumstances alter similarities.</p>
    </disp-quote>
    <p>This can be easily reversed as an argument in favor of the
    <bold>potential of similarity analyses to highlight the
    inter-individual differences in sensory mental
    representations</bold>. For example, should we ask individuals to
    judge the similarities in shape or color between various objects,
    the <italic>differences between the similarity structures</italic>
    of individuals will be precisely the most important phenomenon for
    us, far less than the constancy between these structures. If we can
    account for the context dependence, as we will propose here with
    explicit instructions, clever task design, and hypothesis-neutral
    analysis, we could overcome the limitations of the inherently
    subjective nature of similarity judgements.</p>
    <p>This idea of a difference in similarity judgements in aphantasia
    seems to transpire in the results of Bainbridge et al.
    (<xref alt="2021" rid="ref-bainbridgeQuantifyingAphantasiaDrawing2021" ref-type="bibr">2021</xref>)
    on their drawing study. They have shown that aphantasics had more
    schematic representations during recall, accurate in their spatial
    positioning, but with less sensory details. This difference can be
    seen from two perspectives: (1) a memory deficit for sensory
    properties; (2) a different representational structure of the items
    in their psychological spaces. In the latter case, aphantasics would
    have greater/faster abstraction of their representation of a
    perceived scene, reducing the amount of encoded sensory details
    unconsciously considered to be relevant. Both (1) and (2) can
    theoretically explain the same behavioural response, i.e. less
    sensory elements and correct spatial recall accuracy in aphantasic
    drawings, but <bold>the two have drastically different consequences
    on how we define, characterize, and judge aphantasia.</bold></p>
    <p>The dominant hypothesis seems to be that aphantasics simply have
    an episodic or general memory deficit. Conversely, I hypothesize
    that aphantasics have different representational structures than
    phantasics in certain dimensions of their psychological spaces
    (notably sensory, but potentially abstract too). More generally, I
    hypothesize that the concept of visual imagery evaluates in reality
    the continuous spectrum of representational structures in
    <italic>sensory</italic> dimensions of psychological spaces.
    Mirroring visual imagery, spatial imagery could also be a rough
    psychometric evaluation of the continuous spectrum of structural
    differences in <italic>conceptual/abstract</italic> dimensions of
    psychological spaces. In this view, the psychological space of
    aphantasics would constrain internal representations to particularly
    abstract forms from a very early stage, thus selectively limiting
    the item properties thereafter encoded in long-term memory. In other
    terms, <bold>I hypothesize that aphantasia would not be
    characterized by an episodic memory deficit, but by an episodic
    memory <italic>selectivity</italic> caused by the specific
    characteristics of their representational structures and
    psychological spaces.</bold> This selectivity would have, as we
    already hypothesized several times, benefits and drawbacks.</p>
    <p>Gardenfors
    (<xref alt="2004" rid="ref-gardenforsConceptualSpacesFramework2004" ref-type="bibr">2004</xref>)
    proposed that differences in psychological (in his terms,
    conceptual) spaces could arise from various sources, whether innate,
    due to learning, or broader cultural or social differences. All
    these hypotheses could be coherent to explain the sources of
    aphantasia. Nevertheless, the study of these sources should be the
    subject of very large-scale or longitudinal studies, which are out
    of the scope of this project.</p>
    <p>Here, we shall rather attempt to <bold>develop a method to
    characterize the differences in aphantasics’ representational
    structures and psychological spaces.</bold></p>
  </sec>
</sec>
<sec id="methodology">
  <title>2. Methodology</title>
  <p>Roads and Love
  (<xref alt="2024" rid="ref-roads2024" ref-type="bibr">2024</xref>), in
  a recent review on the state and perspectives of similarity research,
  highlighted two challenges that studies in this field had to face: (1)
  The high cost of collecting behavioral data on a large number of
  stimuli; (2) The lack of software packages being a high barrier to
  entry, making the task of coding models difficult for the
  uninitiated.</p>
  <p>To solve these problems, we present here two solutions,
  respectively for (1) experimental design and (2) data analysis:</p>
  <list list-type="order">
    <list-item>
      <p>A recent method to efficiently acquire similarity judgements,
      the “multiple arrangement of items” and “inverse multidimensional
      scaling” developed by Kriegeskorte and Mur
      (<xref alt="2012" rid="ref-kriegeskorteInverseMDSInferring2012" ref-type="bibr">2012</xref>).</p>
    </list-item>
    <list-item>
      <p>An accessible and robust Python toolbox provided by Sasaki et
      al.
      (<xref alt="2023" rid="ref-sasakiToolboxGromovWassersteinOptimal2023" ref-type="bibr">2023</xref>)
      to conduct unsupervised alignment analysis using
      Gromov-Wasserstein optimal transport.</p>
    </list-item>
  </list>
  <sec id="experimental-design">
    <title>2.1 Experimental design</title>
    <sec id="multi-arrangement-and-inverse-multidimensional-scaling">
      <title>Multi-arrangement and inverse multidimensional
      scaling</title>
      <p>Assuming a geometric model of representational similarities,
      Kriegeskorte and Mur
      (<xref alt="2012" rid="ref-kriegeskorteInverseMDSInferring2012" ref-type="bibr">2012</xref>)
      developed a multi-arrangement (MA) method to efficiently acquire
      (dis)similarity judgments for large sets of objects. The subject
      has to perform multiple arrangements of item subsets adaptively
      designed for optimal measurement efficiency and for estimating the
      representational dissimilarity matrix (RDM) by combining the
      evidence from the subset arrangements.</p>
      <p>The procedure is illustrated in
      <xref alt="Figure 2.1" rid="fig-multi-arrangement">Figure 2.1</xref>.</p>
      <fig id="fig-multi-arrangement">
        <caption><p>Figure 2.1: <bold>Acquiring similarity judgements
        with the multi-arrangement method. (A)</bold> Subjects are asked
        to arrange items according to their similarity, using mouse
        drag-and-drop on a computer. The similarity measure is taken as
        the distances between the items: similar items are closer, while
        dissimilar items are further apart. The upper part of the figure
        shows screenshots at different moments of the acquisition for
        one subject. Columns are trials and rows show the object
        arrangements over time, running from the start (top row) to the
        end (last row). The first trial contains all items; subsequent
        trials contain subsets of items that are adaptively selected to
        optimally estimate judged similarity for each subject.
        <bold>(B)</bold> Once acquisition of the final judgements is
        completed, inter-item distances in the final trial arrangements
        are combined over trials by rescaling and averaging to yield a
        single dissimilarity estimate for each object pair. The process
        is illustrated in this figure for two example item pairs: a
        boy’s face and a hand (red), and carrots and a stop sign (blue).
        Their single-trial dissimilarity estimates (arrows) are combined
        into a single dissimilarity estimate, which is placed at the
        corresponding entry of the RDM (lower panel). Mirror-symmetric
        entries are indicated by lighter colors. Figure taken from Mur
        et al.
        (<xref alt="2013" rid="ref-murHumanObjectSimilarityJudgments2013" ref-type="bibr">2013</xref>).</p></caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="images/multi-arrangement-method-mur-2013.png" />
      </fig>
      <p>A key strength of this method that sets it as particularly
      effective is the “adaptive” part. The goal of the process is to
      acquire similarity judgements as precisely as possible while
      minimizing the total amount of trials. To do so, starting from the
      second trial, selected subsets of the items to be compared are
      presented to the subject: these items are the ones that were very
      close on-screen in previous trials and thus had their distance
      evaluated with lower accuracy by the subject. As the subject has
      to fill the entire “arena” with the items, these subsequent trials
      will necessarily increase the level of precision in the similarity
      judgement between pairs of items. The second key benefit of this
      method is the time and effort gain compared to others. For
      example, to compare every pair of items among 64 different items
      would require <inline-formula><alternatives>
      <tex-math><![CDATA[\frac{64 \times (64-1)}{2} = 2016]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>64</mml:mn><mml:mo>×</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mn>64</mml:mn><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>=</mml:mo><mml:mn>2016</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
      comparisons (i.e. trials). This would be extremely time-consuming,
      while also losing the <italic>context-independence</italic>
      afforded by the MA method due to the presence of other items
      around every time the subject mentally performs a pairwise
      comparison.</p>
      <p>Historically, when referring to the projection of the
      representations of stimuli (e.g., coordinates in geometric space)
      from a high-dimensional space into a lower-dimensional space,
      inference algorithms were commonly called multidimensional scaling
      (<xref alt="Roads and Love 2024" rid="ref-roads2024" ref-type="bibr">Roads
      and Love 2024</xref>). By analogy, the process of combining
      several lower-dimensional (2D) similarity judgements on-screen to
      form one higher dimensional similarity representation (in the RDM)
      can be conceptually seen as “inverse” multidimensional scaling,
      hence the name given to the method by Kriegeskorte and Mur
      (<xref alt="2012" rid="ref-kriegeskorteInverseMDSInferring2012" ref-type="bibr">2012</xref>).</p>
    </sec>
    <sec id="principle">
      <title>Principle</title>
      <p>The idea is simple: for a given set of items that have distinct
      and very pictorial visual properties, we would ask a wide range of
      aphantasics, phantasics or hyperphantasics to imagine, mentally
      compare and make similarity judgements between the items. To
      compare these representations with actual perceptual
      representations, the subjects would also perform the same task
      afterwards, this time with actual pictures to compare. Subjects
      would also fill our usual psychometric imagery questionnaires.</p>
      <p>To “compare imagined items”, we could use a “word” version of
      the MA paradigm. An example from Majewska et al.
      (<xref alt="2020" rid="ref-majewskaSpatialMultiarrangementClustering2020" ref-type="bibr">2020</xref>)
      - <italic>who used the method to build large-scale semantic
      similarity resources for Natural Language Processing
      systems</italic> - is represented in
      <xref alt="Figure 2.2" rid="fig-majewska">Figure 2.2</xref>.</p>
      <fig id="fig-majewska">
        <caption><p>Figure 2.2: Arena layout of the MA protocol used by
        Majewska et al.
        (<xref alt="2020" rid="ref-majewskaSpatialMultiarrangementClustering2020" ref-type="bibr">2020</xref>)
        to acquire similarity judgements on word pairs.</p></caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="images/majewska-spam.png" />
      </fig>
    </sec>
  </sec>
  <sec id="hypotheses">
    <title>2.2 Hypotheses</title>
    <sec id="aphantasic-and-phantasic-psychological-spaces">
      <title>Aphantasic and phantasic psychological spaces</title>
      <p>The most representative members of a category are called
      prototypical members.</p>
      <p>Prototype theory builds on the observation that among the
      instances of a property, some are more representative than others.
      The most representative one is the prototype of the property.</p>
      <p>Thus, following the concepts illustrated by Gardenfors, we
      would expect that aphantasics, when doing shape similarity
      judgements, would be more inclined to group items close to the
      prototypical items due to a lower definition of the mental image.
      In comparison, phantasics would have a much more distributed
      conceptual space of item shapes due to their higher-resolution
      mental images of said items.</p>
    </sec>
    <sec id="subjective-imagery-and-psychological-spaces">
      <title>Subjective imagery and psychological spaces</title>
      <p>In the proposed view of visual imagery as the subjective
      expression of a given type of psychological space, we mentioned
      earlier that <italic>spatial</italic> imagery could also
      constitute a subjective expression of other dimensions of
      psychological spaces. Hence, the <italic>verbal</italic> dimension
      of the simplified model of imagery we outlined in my thesis
      project could also represent different dimensions.</p>
      <p>This conception leads to the following theoretical hypothesis:
      provided that our visual-spatial-verbal model correctly fits
      subjective imagery, the imagery profile of individuals should map
      on their psychological spaces.</p>
      <p>Operationally, this would be evaluated by the fact that
      <bold>individuals with similar imagery profiles</bold> (visual,
      spatial, verbal, or any combination of the three) <bold>should
      have similar representations</bold> in their given psychological
      space, <bold>quantifiable by the degree of alignment between their
      similarity structures.</bold></p>
      <fig>
        <caption><p>The two conditions for one subject.</p></caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="images/my-protocol-1.png" />
      </fig>
      <fig>
        <caption><p>The comparison between the representational
        structure of aphantasics and phantasics. This figure illustrates
        the principle, but in reality all pairs of subjects will be
        compared to assess their representational structure alignment.
        This is computationnally heavy, but analytically very
        powerful.</p></caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="images/my-protocol-2.png" />
      </fig>
    </sec>
  </sec>
</sec>
<sec id="study-data-simulation-and-analysis">
  <title>3. Study data simulation and analysis</title>
  <sec id="visual-spatial-verbal-model-of-cognitive-profiles">
    <title>3.1 Visual-spatial-verbal model of cognitive profiles</title>
    <p>One of the objectives of the study would be to link the
    subjective cogntive profiles of individuals with their
    representational structures. To evaluate these profiles, we are
    going to use psychometric questionnaires evaluating the
    visual-object, spatial, and verbal dimensions of imagery which will
    yield three scores, one for each dimension.</p>
    <p>We are going to simulate 30 participants presenting four
    different cognitive profiles, that I defined as, respectively,
    <italic>verbal</italic> aphantasics, <italic>spatial</italic>
    aphantasics, <italic>spatial</italic> phantasics, and
    <italic>visual</italic> phantasics. Their imagery abilities are
    summarised in
    <xref alt="Table 3.1" rid="tbl-imageries">Table 3.1</xref>.</p>
    <p>To simulate these four sub-groups, we use the
    <monospace>holodeck</monospace> R package to generate multivariate
    normal distributions of scores on these three dimensions for each
    sub-group. For instance, verbal aphantasics have normally
    distributed visual imagery scores centered around a mean of 0
    (normalized, so negative scores are possible), 0.4 for spatial
    imagery, and 0.7 for verbal style; Spatial aphantasics have means of
    0 for visual, 0.75 spatial, and 0.3 for verbal; etc. The numbers are
    arbitrary, but have been chosen by trial-and-error to obtain a model
    that is both well-defined and not exagerrated. The 30 subjects’
    imagery profiles are represented in the three dimensional space of
    the visual-spatial-verbal dimensions in
    <xref alt="Figure 3.1" rid="fig-plot-osv-model">Figure 3.1</xref>.</p>
    <fig id="tbl-imageries">
      <caption><p>Table 3.1: Imagery abilities of the four hypothesized
      cognitive profiles.</p></caption>
      <table-wrap>
        <table>
          <thead>
            <tr>
              <th>Cognitive profile</th>
              <th align="center">Visual imagery</th>
              <th align="center">Spatial imagery</th>
              <th align="center">Verbal style</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Verbal aphantasic</td>
              <td align="center">–</td>
              <td align="center">-</td>
              <td align="center">++</td>
            </tr>
            <tr>
              <td>Spatial aphantasic</td>
              <td align="center">–</td>
              <td align="center">++</td>
              <td align="center">-</td>
            </tr>
            <tr>
              <td>Spatial phantasic</td>
              <td align="center">+</td>
              <td align="center">++</td>
              <td align="center">-</td>
            </tr>
            <tr>
              <td>Visual phantasic</td>
              <td align="center">++</td>
              <td align="center">-</td>
              <td align="center">+</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </fig>
    <p>Down below is the code to generate these scores.</p>
    <code language="python"># The function takes the variance and covariance of the imagery distributions
# as arguments
generate_osv_model &lt;- function(var, cov){
  df &lt;- 
    tibble(group = rep(c(&quot;aph&quot;, &quot;phant&quot;), each = 8)) |&gt; 
    group_by(group) |&gt; 
    mutate(
      spatial_group = c(rep(&quot;spa_low&quot;, 4), rep(&quot;spa_high&quot;, 4)),
      vis_spa_group = paste0(group, &quot;_&quot;, spatial_group),
      verbal_group = &quot;verbal_low&quot;,
      verbal_group  = case_when(
        vis_spa_group == &quot;aph_spa_low&quot; ~ &quot;verbal_high&quot;, 
        vis_spa_group == &quot;phant_spa_low&quot; ~ &quot;verbal_mid&quot;,
        TRUE ~ verbal_group)
    ) |&gt; 
    group_by(vis_spa_group) |&gt; 
    # ─── visual ───
    sim_discr(
      n_vars = 1, 
      var = var, 
      cov = cov, 
      # aph_s, aph_v, phant_s, phant_v
      group_means = c(0, 0, 0.6, 0.87), 
      name = &quot;v&quot;) |&gt; 
    # ─── spatial ───
    sim_discr(
      n_vars = 1,  
      var = var, 
      cov = cov, 
      # aph_s, aph_v, phant_s, phant_v
      group_means = c(0.75, 0.4, 0.7, 0.3), 
      name = &quot;s&quot;) |&gt;
    # ─── verbal ───
    sim_discr(
      n_vars = 1,  
      var = var, 
      cov = cov, 
      # aph_s, aph_v, phant_s, phant_v
      group_means = c(0.3, 0.7, 0.3, 0.5), 
      name = &quot;i&quot;) |&gt;
    rename(
      visual_imagery  = v_1,
      spatial_imagery = s_1,
      verbal_profile  = i_1
      )
}

df &lt;- generate_osv_model(0.03, 0)</code>
    <fig id="fig-plot-osv-model">
      <caption><p>Figure 3.1: Imagery profiles generated for 30 subjects
      on the three object, spatial, and verbal dimensions.</p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-plot-osv-model-1.png" />
    </fig>
  </sec>
</sec>
</body>

<back>
<ref-list>
  <title></title>
  <ref id="ref-roads2024">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Roads</surname><given-names>Brett D.</given-names></name>
        <name><surname>Love</surname><given-names>Bradley C.</given-names></name>
      </person-group>
      <article-title>Modeling similarity and psychological space</article-title>
      <source>Annual Review of Psychology</source>
      <year iso-8601-date="2024-01">2024</year><month>01</month>
      <volume>75</volume>
      <issue>1</issue>
      <issn>0066-4308, 1545-2085</issn>
      <uri>https://www.annualreviews.org/doi/10.1146/annurev-psych-040323-115131</uri>
      <pub-id pub-id-type="doi">10.1146/annurev-psych-040323-115131</pub-id>
      <fpage>215</fpage>
      <lpage>240</lpage>
    </element-citation>
  </ref>
  <ref id="ref-shepardSecondorderIsomorphismInternal1970">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Shepard</surname><given-names>Roger N</given-names></name>
        <name><surname>Chipman</surname><given-names>Susan</given-names></name>
      </person-group>
      <article-title>Second-order isomorphism of internal representations: Shapes of states</article-title>
      <source>Cognitive Psychology</source>
      <year iso-8601-date="1970-01">1970</year><month>01</month>
      <volume>1</volume>
      <issue>1</issue>
      <issn>0010-0285</issn>
      <uri>https://www.sciencedirect.com/science/article/pii/0010028570900022</uri>
      <pub-id pub-id-type="doi">10.1016/0010-0285(70)90002-2</pub-id>
      <fpage>1</fpage>
      <lpage>17</lpage>
    </element-citation>
  </ref>
  <ref id="ref-sasakiToolboxGromovWassersteinOptimal2023">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Sasaki</surname><given-names>Masaru</given-names></name>
        <name><surname>Takeda</surname><given-names>Ken</given-names></name>
        <name><surname>Abe</surname><given-names>Kota</given-names></name>
        <name><surname>Oizumi</surname><given-names>Masafumi</given-names></name>
      </person-group>
      <article-title>Toolbox for gromov-wasserstein optimal transport: Application to unsupervised alignment in neuroscience</article-title>
      <year iso-8601-date="2023">2023</year>
      <uri>https://www.biorxiv.org/content/10.1101/2023.09.15.558038v1</uri>
      <pub-id pub-id-type="doi">10.1101/2023.09.15.558038</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-bainbridgeQuantifyingAphantasiaDrawing2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bainbridge</surname><given-names>Wilma A.</given-names></name>
        <name><surname>Pounder</surname><given-names>Zoë</given-names></name>
        <name><surname>Eardley</surname><given-names>Alison F.</given-names></name>
        <name><surname>Baker</surname><given-names>Chris I.</given-names></name>
      </person-group>
      <article-title>Quantifying aphantasia through drawing: Those without visual imagery show deficits in object but not spatial memory</article-title>
      <source>Cortex</source>
      <year iso-8601-date="2021-02-01">2021</year><month>02</month><day>01</day>
      <volume>135</volume>
      <uri>https://www.sciencedirect.com/science/article/pii/S0010945220304317</uri>
      <pub-id pub-id-type="doi">10.1016/j.cortex.2020.11.014</pub-id>
      <fpage>159</fpage>
      <lpage>172</lpage>
    </element-citation>
  </ref>
  <ref id="ref-kriegeskorteInverseMDSInferring2012">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name>
        <name><surname>Mur</surname><given-names>Marieke</given-names></name>
      </person-group>
      <article-title>Inverse MDS: Inferring dissimilarity structure from multiple item arrangements</article-title>
      <source>Frontiers in Psychology</source>
      <year iso-8601-date="2012">2012</year>
      <volume>3</volume>
      <uri>https://www.frontiersin.org/articles/10.3389/fpsyg.2012.00245</uri>
    </element-citation>
  </ref>
  <ref id="ref-decockSimilarityGoodman2011">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Decock</surname><given-names>Lieven</given-names></name>
        <name><surname>Douven</surname><given-names>Igor</given-names></name>
      </person-group>
      <article-title>Similarity After Goodman</article-title>
      <source>Review of Philosophy and Psychology</source>
      <year iso-8601-date="2011-03-01">2011</year><month>03</month><day>01</day>
      <volume>2</volume>
      <issue>1</issue>
      <uri>https://doi.org/10.1007/s13164-010-0035-y</uri>
      <pub-id pub-id-type="doi">10.1007/s13164-010-0035-y</pub-id>
      <fpage>61</fpage>
      <lpage>75</lpage>
    </element-citation>
  </ref>
  <ref id="ref-goodmanSevenStricturesSimilarity1972">
    <element-citation publication-type="chapter">
      <person-group person-group-type="author">
        <name><surname>Goodman</surname><given-names>Nelson</given-names></name>
      </person-group>
      <article-title>Seven strictures on similarity</article-title>
      <publisher-name>Bobs-Merril</publisher-name>
      <year iso-8601-date="1972">1972</year>
    </element-citation>
  </ref>
  <ref id="ref-gardenforsConceptualSpacesFramework2004">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Gardenfors</surname><given-names>Peter</given-names></name>
      </person-group>
      <article-title>Conceptual spaces as a framework for knowledge representation</article-title>
      <year iso-8601-date="2004">2004</year>
    </element-citation>
  </ref>
  <ref id="ref-murHumanObjectSimilarityJudgments2013">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Mur</surname><given-names>Marieke</given-names></name>
        <name><surname>Meys</surname><given-names>Mirjam</given-names></name>
        <name><surname>Bodurka</surname><given-names>Jerzy</given-names></name>
        <name><surname>Goebel</surname><given-names>Rainer</given-names></name>
        <name><surname>Bandettini</surname><given-names>Peter</given-names></name>
        <name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name>
      </person-group>
      <article-title>Human object-similarity judgments reflect and transcend the primate-IT object representation</article-title>
      <source>Frontiers in Psychology</source>
      <year iso-8601-date="2013">2013</year>
      <volume>4</volume>
      <uri>https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00128</uri>
    </element-citation>
  </ref>
  <ref id="ref-majewskaSpatialMultiarrangementClustering2020">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Majewska</surname><given-names>O.</given-names></name>
        <name><surname>McCarthy</surname><given-names>D.</given-names></name>
        <name><surname>Bosch</surname><given-names>J. van den</given-names></name>
        <name><surname>Kriegeskorte</surname><given-names>N.</given-names></name>
        <name><surname>Vulic</surname><given-names>I.</given-names></name>
        <name><surname>Korhonen</surname><given-names>A.</given-names></name>
      </person-group>
      <source>Spatial multi-arrangement for clustering and multi-way similarity dataset construction</source>
      <publisher-name>European Language Resources Association</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <uri>https://www.repository.cam.ac.uk/handle/1810/306834</uri>
    </element-citation>
  </ref>
</ref-list>
<fn-group>
  <fn id="fn1">
    <label>1</label><p>A claim dismissed since then by propositions of
    robust mathematical models of similarity, e.g. Gardenfors
    (<xref alt="2004" rid="ref-gardenforsConceptualSpacesFramework2004" ref-type="bibr">2004</xref>),
    Decock and Douven
    (<xref alt="2011" rid="ref-decockSimilarityGoodman2011" ref-type="bibr">2011</xref>).</p>
  </fn>
</fn-group>
</back>

<sub-article article-type="notebook" id="nb-12-nb-article">
<front-stub>
<title-group>
<article-title>Unravelling mental representations in aphantasia through
unsupervised alignment</article-title>
<subtitle>Project design and data analysis simulation</subtitle>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Delem</surname>
<given-names>Maël</given-names>
</name>
<string-name>Maël Delem</string-name>

</contrib>
</contrib-group>
<abstract>
<p>Research on aphantasia is confronted with a long-standing conundrum
of all research on consciousness and representations, namely the
theoretical inaccessibility of subjective representations. Drawing on
concepts from similarity and representation research, I endorse the view
that the study of an individual’s mental representations is made
possible by exploiting second-order isomorphism. The concept of
second-order isomorphism means that correspondence should not be sought
in the first-order relation between (a) an external object and (b) the
corresponding internal representation, but in the second-order relation
between (a) the perceived similarities between various external objects
and (b) the similarities between their corresponding internal
representations. Building on this idea, this study project report is
divided into five parts. <bold>First</bold>, I outline the central ideas
underlying similarity research and its applicability to aphantasia
research. <bold>Second</bold>, I present a methodological rationale and
protocol based on inverse multidimensional scaling that can be
implemented online to conduct such large-scale research with high
efficiency. <bold>Third</bold>, I present a data analysis plan using a
state-of-the-art method for similarity analysis, unsupervised alignment
with Gromov-Wasserstein optimal transport (GWOT). <bold>Fourth</bold>, I
report a data simulation of a potential outcome of this project and the
successful analysis of this synthetic data using GWOT alignment.
<bold>Fifth</bold>, I analyse the feasability of such a project given
the material constraints of my thesis. I conclude with the expected
utility and benefits of this project.</p>
</abstract>
</front-stub>

<body>
<boxed-text>
  <disp-quote>
    <p><bold>Project inception</bold></p>
    <p>This project stems from several elements:</p>
    <list list-type="order">
      <list-item>
        <p>The long standing knowledge of the fact that internal
        representations seem impossible to reach due to their subjective
        nature.</p>
      </list-item>
      <list-item>
        <p>The discovery of the article of Shepard and Chipman
        (<xref alt="1970" rid="ref-shepardSecondorderIsomorphismInternal1970-nb-article" ref-type="bibr">1970</xref>)
        that expose the idea of “second-order isomorphism”.</p>
      </list-item>
      <list-item>
        <p>The discovery of state-of-the-art and accessible unsupervised
        analytic methods to study this principle in an astonishing way.
        The last two discoveries (and many more) are the fruit of
        amazing discussions and recommendations from Ladislas when he
        came here. These motivated me to try to implement GWOT in R on
        data that I wanted to create myself to emulate a study we could
        do.</p>
      </list-item>
    </list>
    <p><italic>I promise that I did this mostly on my spare time, we
    have too many other things to do elsewhere.</italic></p>
  </disp-quote>
</boxed-text>
<sec id="theoretical-context-nb-article">
  <title>1. Theoretical context</title>
  <sec id="psychological-spaces-and-aphantasia-nb-article">
    <title>1.1 Psychological spaces and aphantasia</title>
    <p>While attempting to demonstrate the uselessness of the concept of
    similarity as a philosophical and scientific
    notion<xref ref-type="fn" rid="fn1-nb-article">1</xref>, Goodman
    (<xref alt="1972" rid="ref-goodmanSevenStricturesSimilarity1972-nb-article" ref-type="bibr">1972</xref>)
    has inadvertently expressed an aspect of similarity judgements of
    primary importance to us aphantasia researchers:</p>
    <disp-quote>
      <p>Comparative judgments of similarity often require not merely
      selection of relevant properties but a weighting of their relative
      importance, and variation in both relevance and importance can be
      rapid and enormous. Consider baggage at an airport checking
      station. The spectator may notice shape, size, color, material,
      and even make of luggage; the pilot is more concerned with weight,
      and the passenger with destination and ownership. Which pieces are
      more alike than others depends not only upon what properties they
      share, but upon who makes the comparison, and when. . . .
      Circumstances alter similarities.</p>
    </disp-quote>
    <p>This can be easily reversed as an argument in favor of the
    <bold>potential of similarity analyses to highlight the
    inter-individual differences in sensory mental
    representations</bold>. For example, should we ask individuals to
    judge the similarities in shape or color between various objects,
    the <italic>differences between the similarity structures</italic>
    of individuals will be precisely the most important phenomenon for
    us, far less than the constancy between these structures. If we can
    account for the context dependence, as we will propose here with
    explicit instructions, clever task design, and hypothesis-neutral
    analysis, we could overcome the limitations of the inherently
    subjective nature of similarity judgements.</p>
    <p>This idea of a difference in similarity judgements in aphantasia
    seems to transpire in the results of Bainbridge et al.
    (<xref alt="2021" rid="ref-bainbridgeQuantifyingAphantasiaDrawing2021-nb-article" ref-type="bibr">2021</xref>)
    on their drawing study. They have shown that aphantasics had more
    schematic representations during recall, accurate in their spatial
    positioning, but with less sensory details. This difference can be
    seen from two perspectives: (1) a memory deficit for sensory
    properties; (2) a different representational structure of the items
    in their psychological spaces. In the latter case, aphantasics would
    have greater/faster abstraction of their representation of a
    perceived scene, reducing the amount of encoded sensory details
    unconsciously considered to be relevant. Both (1) and (2) can
    theoretically explain the same behavioural response, i.e. less
    sensory elements and correct spatial recall accuracy in aphantasic
    drawings, but <bold>the two have drastically different consequences
    on how we define, characterize, and judge aphantasia.</bold></p>
    <p>The dominant hypothesis seems to be that aphantasics simply have
    an episodic or general memory deficit. Conversely, I hypothesize
    that aphantasics have different representational structures than
    phantasics in certain dimensions of their psychological spaces
    (notably sensory, but potentially abstract too). More generally, I
    hypothesize that the concept of visual imagery evaluates in reality
    the continuous spectrum of representational structures in
    <italic>sensory</italic> dimensions of psychological spaces.
    Mirroring visual imagery, spatial imagery could also be a rough
    psychometric evaluation of the continuous spectrum of structural
    differences in <italic>conceptual/abstract</italic> dimensions of
    psychological spaces. In this view, the psychological space of
    aphantasics would constrain internal representations to particularly
    abstract forms from a very early stage, thus selectively limiting
    the item properties thereafter encoded in long-term memory. In other
    terms, <bold>I hypothesize that aphantasia would not be
    characterized by an episodic memory deficit, but by an episodic
    memory <italic>selectivity</italic> caused by the specific
    characteristics of their representational structures and
    psychological spaces.</bold> This selectivity would have, as we
    already hypothesized several times, benefits and drawbacks.</p>
    <p>Gardenfors
    (<xref alt="2004" rid="ref-gardenforsConceptualSpacesFramework2004-nb-article" ref-type="bibr">2004</xref>)
    proposed that differences in psychological (in his terms,
    conceptual) spaces could arise from various sources, whether innate,
    due to learning, or broader cultural or social differences. All
    these hypotheses could be coherent to explain the sources of
    aphantasia. Nevertheless, the study of these sources should be the
    subject of very large-scale or longitudinal studies, which are out
    of the scope of this project.</p>
    <p>Here, we shall rather attempt to <bold>develop a method to
    characterize the differences in aphantasics’ representational
    structures and psychological spaces.</bold></p>
  </sec>
</sec>
<sec id="methodology-nb-article">
  <title>2. Methodology</title>
  <p>Roads and Love
  (<xref alt="2024" rid="ref-roads2024-nb-article" ref-type="bibr">2024</xref>), in
  a recent review on the state and perspectives of similarity research,
  highlighted two challenges that studies in this field had to face: (1)
  The high cost of collecting behavioral data on a large number of
  stimuli; (2) The lack of software packages being a high barrier to
  entry, making the task of coding models difficult for the
  uninitiated.</p>
  <p>To solve these problems, we present here two solutions,
  respectively for (1) experimental design and (2) data analysis:</p>
  <list list-type="order">
    <list-item>
      <p>A recent method to efficiently acquire similarity judgements,
      the “multiple arrangement of items” and “inverse multidimensional
      scaling” developed by Kriegeskorte and Mur
      (<xref alt="2012" rid="ref-kriegeskorteInverseMDSInferring2012-nb-article" ref-type="bibr">2012</xref>).</p>
    </list-item>
    <list-item>
      <p>An accessible and robust Python toolbox provided by Sasaki et
      al.
      (<xref alt="2023" rid="ref-sasakiToolboxGromovWassersteinOptimal2023-nb-article" ref-type="bibr">2023</xref>)
      to conduct unsupervised alignment analysis using
      Gromov-Wasserstein optimal transport.</p>
    </list-item>
  </list>
  <sec id="experimental-design-nb-article">
    <title>2.1 Experimental design</title>
    <sec id="multi-arrangement-and-inverse-multidimensional-scaling-nb-article">
      <title>Multi-arrangement and inverse multidimensional
      scaling</title>
      <p>Assuming a geometric model of representational similarities,
      Kriegeskorte and Mur
      (<xref alt="2012" rid="ref-kriegeskorteInverseMDSInferring2012-nb-article" ref-type="bibr">2012</xref>)
      developed a multi-arrangement (MA) method to efficiently acquire
      (dis)similarity judgments for large sets of objects. The subject
      has to perform multiple arrangements of item subsets adaptively
      designed for optimal measurement efficiency and for estimating the
      representational dissimilarity matrix (RDM) by combining the
      evidence from the subset arrangements.</p>
      <p>The procedure is illustrated in
      <xref alt="Figure 2.1" rid="fig-multi-arrangement-nb-article">Figure 2.1</xref>.</p>
      <fig id="fig-multi-arrangement-nb-article">
        <caption><p>Figure 2.1: <bold>Acquiring similarity judgements
        with the multi-arrangement method. (A)</bold> Subjects are asked
        to arrange items according to their similarity, using mouse
        drag-and-drop on a computer. The similarity measure is taken as
        the distances between the items: similar items are closer, while
        dissimilar items are further apart. The upper part of the figure
        shows screenshots at different moments of the acquisition for
        one subject. Columns are trials and rows show the object
        arrangements over time, running from the start (top row) to the
        end (last row). The first trial contains all items; subsequent
        trials contain subsets of items that are adaptively selected to
        optimally estimate judged similarity for each subject.
        <bold>(B)</bold> Once acquisition of the final judgements is
        completed, inter-item distances in the final trial arrangements
        are combined over trials by rescaling and averaging to yield a
        single dissimilarity estimate for each object pair. The process
        is illustrated in this figure for two example item pairs: a
        boy’s face and a hand (red), and carrots and a stop sign (blue).
        Their single-trial dissimilarity estimates (arrows) are combined
        into a single dissimilarity estimate, which is placed at the
        corresponding entry of the RDM (lower panel). Mirror-symmetric
        entries are indicated by lighter colors. Figure taken from Mur
        et al.
        (<xref alt="2013" rid="ref-murHumanObjectSimilarityJudgments2013-nb-article" ref-type="bibr">2013</xref>).</p></caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="images/multi-arrangement-method-mur-2013.png" />
      </fig>
      <p>A key strength of this method that sets it as particularly
      effective is the “adaptive” part. The goal of the process is to
      acquire similarity judgements as precisely as possible while
      minimizing the total amount of trials. To do so, starting from the
      second trial, selected subsets of the items to be compared are
      presented to the subject: these items are the ones that were very
      close on-screen in previous trials and thus had their distance
      evaluated with lower accuracy by the subject. As the subject has
      to fill the entire “arena” with the items, these subsequent trials
      will necessarily increase the level of precision in the similarity
      judgement between pairs of items. The second key benefit of this
      method is the time and effort gain compared to others. For
      example, to compare every pair of items among 64 different items
      would require <inline-formula><alternatives>
      <tex-math><![CDATA[\frac{64 \times (64-1)}{2} = 2016]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>64</mml:mn><mml:mo>×</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mn>64</mml:mn><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>=</mml:mo><mml:mn>2016</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
      comparisons (i.e. trials). This would be extremely time-consuming,
      while also losing the <italic>context-independence</italic>
      afforded by the MA method due to the presence of other items
      around every time the subject mentally performs a pairwise
      comparison.</p>
      <p>Historically, when referring to the projection of the
      representations of stimuli (e.g., coordinates in geometric space)
      from a high-dimensional space into a lower-dimensional space,
      inference algorithms were commonly called multidimensional scaling
      (<xref alt="Roads and Love 2024" rid="ref-roads2024-nb-article" ref-type="bibr">Roads
      and Love 2024</xref>). By analogy, the process of combining
      several lower-dimensional (2D) similarity judgements on-screen to
      form one higher dimensional similarity representation (in the RDM)
      can be conceptually seen as “inverse” multidimensional scaling,
      hence the name given to the method by Kriegeskorte and Mur
      (<xref alt="2012" rid="ref-kriegeskorteInverseMDSInferring2012-nb-article" ref-type="bibr">2012</xref>).</p>
    </sec>
    <sec id="principle-nb-article">
      <title>Principle</title>
      <p>The idea is simple: for a given set of items that have distinct
      and very pictorial visual properties, we would ask a wide range of
      aphantasics, phantasics or hyperphantasics to imagine, mentally
      compare and make similarity judgements between the items. To
      compare these representations with actual perceptual
      representations, the subjects would also perform the same task
      afterwards, this time with actual pictures to compare. Subjects
      would also fill our usual psychometric imagery questionnaires.</p>
      <p>To “compare imagined items”, we could use a “word” version of
      the MA paradigm. An example from Majewska et al.
      (<xref alt="2020" rid="ref-majewskaSpatialMultiarrangementClustering2020-nb-article" ref-type="bibr">2020</xref>)
      - <italic>who used the method to build large-scale semantic
      similarity resources for Natural Language Processing
      systems</italic> - is represented in
      <xref alt="Figure 2.2" rid="fig-majewska-nb-article">Figure 2.2</xref>.</p>
      <fig id="fig-majewska-nb-article">
        <caption><p>Figure 2.2: Arena layout of the MA protocol used by
        Majewska et al.
        (<xref alt="2020" rid="ref-majewskaSpatialMultiarrangementClustering2020-nb-article" ref-type="bibr">2020</xref>)
        to acquire similarity judgements on word pairs.</p></caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="images/majewska-spam.png" />
      </fig>
    </sec>
  </sec>
  <sec id="hypotheses-nb-article">
    <title>2.2 Hypotheses</title>
    <sec id="aphantasic-and-phantasic-psychological-spaces-nb-article">
      <title>Aphantasic and phantasic psychological spaces</title>
      <p>The most representative members of a category are called
      prototypical members.</p>
      <p>Prototype theory builds on the observation that among the
      instances of a property, some are more representative than others.
      The most representative one is the prototype of the property.</p>
      <p>Thus, following the concepts illustrated by Gardenfors, we
      would expect that aphantasics, when doing shape similarity
      judgements, would be more inclined to group items close to the
      prototypical items due to a lower definition of the mental image.
      In comparison, phantasics would have a much more distributed
      conceptual space of item shapes due to their higher-resolution
      mental images of said items.</p>
    </sec>
    <sec id="subjective-imagery-and-psychological-spaces-nb-article">
      <title>Subjective imagery and psychological spaces</title>
      <p>In the proposed view of visual imagery as the subjective
      expression of a given type of psychological space, we mentioned
      earlier that <italic>spatial</italic> imagery could also
      constitute a subjective expression of other dimensions of
      psychological spaces. Hence, the <italic>verbal</italic> dimension
      of the simplified model of imagery we outlined in my thesis
      project could also represent different dimensions.</p>
      <p>This conception leads to the following theoretical hypothesis:
      provided that our visual-spatial-verbal model correctly fits
      subjective imagery, the imagery profile of individuals should map
      on their psychological spaces.</p>
      <p>Operationally, this would be evaluated by the fact that
      <bold>individuals with similar imagery profiles</bold> (visual,
      spatial, verbal, or any combination of the three) <bold>should
      have similar representations</bold> in their given psychological
      space, <bold>quantifiable by the degree of alignment between their
      similarity structures.</bold></p>
      <fig>
        <caption><p>The two conditions for one subject.</p></caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="images/my-protocol-1.png" />
      </fig>
      <fig>
        <caption><p>The comparison between the representational
        structure of aphantasics and phantasics. This figure illustrates
        the principle, but in reality all pairs of subjects will be
        compared to assess their representational structure alignment.
        This is computationnally heavy, but analytically very
        powerful.</p></caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="images/my-protocol-2.png" />
      </fig>
    </sec>
  </sec>
</sec>
<sec id="study-data-simulation-and-analysis-nb-article">
  <title>3. Study data simulation and analysis</title>
  <sec id="nb-code-cell-1-nb-article" specific-use="notebook-code">
  <code language="r script"># ═══ Packages ═════════════════════════════════════════════════════════════════

if (!require(librarian)) install.packages(librarian) </code>
  <code id="annotated-cell-2-nb-article" language="r script">library(librarian)                                     

# now putting packages on our library's shelves:
shelf(
  # ─── data management ─────────────────
  holodeck,       # simulating multivariate data
  cluster,        # dissimilarity matrices
  
  # ─── modelling ───────────────────────
  mclust,         # mixture clustering
  
  #  data visualization ──────────────
  # palettes
  viridis,        # colour-blind friendly palettes
  # interactive
  plotly,         # interactive plots
  ggdendro,       # dendrograms
  seriation,      # dissimilarity plots
  webshot2,       # HTML screenshots for Word render
  webshot,
  
  # ─── essential package collections ───
  doParallel,     # parallel execution
  easystats,      # data analysis ecosystem
  reticulate,     # R to Python                    
  tidyverse,      # modern R ecosystem
)

# ─── Global cosmetic theme ───
theme_set(theme_modern(base_size = 14))

pal_okabe_ito &lt;- c(
  &quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;,                            
  &quot;#F5C710&quot;, &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;, &quot;#6c0009&quot;)      

# We'll need 9 colors at some point
pal_okabe_ito_extended &lt;- c(                                 
  &quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;,                           
  &quot;#F5C710&quot;, &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;, &quot;#6c0009&quot;, &quot;#414487FF&quot;)

# We'll need 30 colors at another moment
cool_30_colors &lt;- c(                                                   
  &quot;#3d51b4&quot;, &quot;#414487FF&quot;, &quot;#003d73&quot;, &quot;#440154FF&quot;, &quot;#6c0009&quot;, &quot;#b64e4e&quot;,
  &quot;#D55E00&quot;, &quot;#E69F00&quot;, &quot;#F5C710&quot;, &quot;#FDE725FF&quot;, &quot;#f2bb7b&quot;, &quot;#f1afad&quot;, &quot;#CC79A7&quot;, 
  &quot;#e57774&quot;, &quot;#7AD151FF&quot;, &quot;#57b571&quot;, &quot;#318a4a&quot;, &quot;#009E73&quot;, &quot;#22A884FF&quot;, 
  &quot;#2A788EFF&quot;, &quot;#0072B2&quot;, &quot;#2da6b5&quot;, &quot;#56B4E9&quot;, &quot;#889be0&quot;, &quot;#6677e0&quot;,   
  &quot;#3d51b4&quot;, &quot;#414487FF&quot;, &quot;#003d73&quot;, &quot;#440154FF&quot;, &quot;#6c0009&quot;, &quot;#b64e4e&quot;  
  )

path = &quot;notebooks/data/&quot;

df &lt;- read_rds(paste0(path, &quot;df.RDS&quot;))
df_embeds &lt;- read_rds(paste0(path, &quot;df_embeds.RDS&quot;))
# Categorical and visual embeddings
df_embeds_categ  &lt;- read_rds(paste0(path, &quot;df_embeds_categ.RDS&quot;))
df_embeds_visual &lt;- read_rds(paste0(path, &quot;df_embeds_visual.RDS&quot;))
# Subject embeddings per sub-group
df_embed_c_sub  &lt;- read_rds(paste0(path, &quot;df_embed_c_sub.RDS&quot;))
df_embed_cs_sub &lt;- read_rds(paste0(path, &quot;df_embed_cs_sub.RDS&quot;))
df_embed_v_sub  &lt;- read_rds(paste0(path, &quot;df_embed_v_sub.RDS&quot;))
df_embed_vs_sub &lt;- read_rds(paste0(path, &quot;df_embed_vs_sub.RDS&quot;))
# Accuracy of the unsupervised alignment (bad = not tidy data)
df_accuracy_all_bad &lt;- read_rds(paste0(path, &quot;df_accuracy_all_bad.RDS&quot;))
df_accuracy_cat_bad &lt;- read_rds(paste0(path, &quot;df_accuracy_cat_bad.RDS&quot;))
# Coordinates of the aligned embeddings from the Python output
coordinates_aligned_embeddings &lt;- read_rds(paste0(path, &quot;coordinates_aligned_embeddings.RDS&quot;))</code>
  <sec id="nb-code-cell-1-output-0-nb-article" specific-use="notebook-output">
  <preformat>Le chargement a nécessité le package : librarian</preformat>
  </sec>
  </sec>
  <sec id="visual-spatial-verbal-model-of-cognitive-profiles-nb-article">
    <title>3.1 Visual-spatial-verbal model of cognitive profiles</title>
    <p>One of the objectives of the study would be to link the
    subjective cogntive profiles of individuals with their
    representational structures. To evaluate these profiles, we are
    going to use psychometric questionnaires evaluating the
    visual-object, spatial, and verbal dimensions of imagery which will
    yield three scores, one for each dimension.</p>
    <p>We are going to simulate 30 participants presenting four
    different cognitive profiles, that I defined as, respectively,
    <italic>verbal</italic> aphantasics, <italic>spatial</italic>
    aphantasics, <italic>spatial</italic> phantasics, and
    <italic>visual</italic> phantasics. Their imagery abilities are
    summarised in
    <xref alt="Table 3.1" rid="tbl-imageries-nb-article">Table 3.1</xref>.</p>
    <p>To simulate these four sub-groups, we use the
    <monospace>holodeck</monospace> R package to generate multivariate
    normal distributions of scores on these three dimensions for each
    sub-group. For instance, verbal aphantasics have normally
    distributed visual imagery scores centered around a mean of 0
    (normalized, so negative scores are possible), 0.4 for spatial
    imagery, and 0.7 for verbal style; Spatial aphantasics have means of
    0 for visual, 0.75 spatial, and 0.3 for verbal; etc. The numbers are
    arbitrary, but have been chosen by trial-and-error to obtain a model
    that is both well-defined and not exagerrated. The 30 subjects’
    imagery profiles are represented in the three dimensional space of
    the visual-spatial-verbal dimensions in
    <xref alt="Figure 3.1" rid="fig-plot-osv-model-nb-article">Figure 3.1</xref>.</p>
    <fig id="tbl-imageries-nb-article">
      <caption><p>Table 3.1: Imagery abilities of the four hypothesized
      cognitive profiles.</p></caption>
      <table-wrap>
        <table>
          <thead>
            <tr>
              <th>Cognitive profile</th>
              <th align="center">Visual imagery</th>
              <th align="center">Spatial imagery</th>
              <th align="center">Verbal style</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Verbal aphantasic</td>
              <td align="center">–</td>
              <td align="center">-</td>
              <td align="center">++</td>
            </tr>
            <tr>
              <td>Spatial aphantasic</td>
              <td align="center">–</td>
              <td align="center">++</td>
              <td align="center">-</td>
            </tr>
            <tr>
              <td>Spatial phantasic</td>
              <td align="center">+</td>
              <td align="center">++</td>
              <td align="center">-</td>
            </tr>
            <tr>
              <td>Visual phantasic</td>
              <td align="center">++</td>
              <td align="center">-</td>
              <td align="center">+</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </fig>
    <p>Down below is the code to generate these scores.</p>
    <sec id="osv-model-nb-article" specific-use="notebook-code">
    <code language="python"># The function takes the variance and covariance of the imagery distributions
# as arguments
generate_osv_model &lt;- function(var, cov){
  df &lt;- 
    tibble(group = rep(c(&quot;aph&quot;, &quot;phant&quot;), each = 8)) |&gt; 
    group_by(group) |&gt; 
    mutate(
      spatial_group = c(rep(&quot;spa_low&quot;, 4), rep(&quot;spa_high&quot;, 4)),
      vis_spa_group = paste0(group, &quot;_&quot;, spatial_group),
      verbal_group = &quot;verbal_low&quot;,
      verbal_group  = case_when(
        vis_spa_group == &quot;aph_spa_low&quot; ~ &quot;verbal_high&quot;, 
        vis_spa_group == &quot;phant_spa_low&quot; ~ &quot;verbal_mid&quot;,
        TRUE ~ verbal_group)
    ) |&gt; 
    group_by(vis_spa_group) |&gt; 
    # ─── visual ───
    sim_discr(
      n_vars = 1, 
      var = var, 
      cov = cov, 
      # aph_s, aph_v, phant_s, phant_v
      group_means = c(0, 0, 0.6, 0.87), 
      name = &quot;v&quot;) |&gt; 
    # ─── spatial ───
    sim_discr(
      n_vars = 1,  
      var = var, 
      cov = cov, 
      # aph_s, aph_v, phant_s, phant_v
      group_means = c(0.75, 0.4, 0.7, 0.3), 
      name = &quot;s&quot;) |&gt;
    # ─── verbal ───
    sim_discr(
      n_vars = 1,  
      var = var, 
      cov = cov, 
      # aph_s, aph_v, phant_s, phant_v
      group_means = c(0.3, 0.7, 0.3, 0.5), 
      name = &quot;i&quot;) |&gt;
    rename(
      visual_imagery  = v_1,
      spatial_imagery = s_1,
      verbal_profile  = i_1
      )
}

df &lt;- generate_osv_model(0.03, 0)</code>
    </sec>
    <sec id="cell-fig-plot-osv-model-nb-article" specific-use="notebook-code">
    <code language="r script">plotting_osv_model &lt;- function(df, grouping_variable, size){
  df |&gt; 
    plot_ly(
      x = ~visual_imagery,
      y = ~spatial_imagery,
      z = ~verbal_profile,
      color = ~df[[grouping_variable]],
      text  = ~df[[grouping_variable]],
      colors = c(&quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;, &quot;#F5C710&quot;),
      type = &quot;scatter3d&quot;,
      mode = &quot;markers+text&quot;,
      marker = list(size = size),
      textfont = list(size = size + 4)
    ) |&gt; 
    layout(
      scene = list(
        xaxis = list(
          title = list(text = &quot;Visual imagery&quot;, font = list(color = &quot;grey&quot;)),
          tickfont = list(color = &quot;grey&quot;)
          ),
        yaxis = list(
          title = list(text = &quot;Spatial imagery&quot;, font = list(color = &quot;grey&quot;)),
          tickfont = list(color = &quot;grey&quot;)
          ),
        zaxis = list(
          title = list(text = &quot;Verbal profile&quot;, font = list(color = &quot;grey&quot;)),
          tickfont = list(color = &quot;grey&quot;)
          )
      ),
      legend = list(title = list(text = &quot;Group&quot;)),
      paper_bgcolor = &quot;transparent&quot;
      )
}

df |&gt; 
  mutate(vis_spa_group = case_when(
    vis_spa_group == &quot;aph_spa_high&quot; ~ &quot;Aph. spatial&quot;,
    vis_spa_group == &quot;aph_spa_low&quot;  ~ &quot;Aph. verbal&quot;,
    vis_spa_group == &quot;phant_spa_high&quot; ~ &quot;Phant. spatial&quot;,
    vis_spa_group == &quot;phant_spa_low&quot;  ~ &quot;Phant. visual&quot;
  )) |&gt; 
  plotting_osv_model(grouping_variable = &quot;vis_spa_group&quot;, size = 4)</code>
    <fig id="fig-plot-osv-model-nb-article">
      <caption><p>Figure 3.1: Imagery profiles generated for 30 subjects
      on the three object, spatial, and verbal dimensions.</p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-plot-osv-model-1.png" />
    </fig>
    <sec id="cell-fig-plot-osv-model-output-0-nb-article" specific-use="notebook-output">
    <preformat>PhantomJS not found. You can install it with webshot::install_phantomjs(). If it is installed, please make sure the phantomjs executable can be found via the PATH variable.</preformat>
    </sec>
    </sec>
  </sec>
</sec>
</body>



<back>
<ref-list>
  <title></title>
  <ref id="ref-roads2024-nb-article">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Roads</surname><given-names>Brett D.</given-names></name>
        <name><surname>Love</surname><given-names>Bradley C.</given-names></name>
      </person-group>
      <article-title>Modeling similarity and psychological space</article-title>
      <source>Annual Review of Psychology</source>
      <year iso-8601-date="2024-01">2024</year><month>01</month>
      <volume>75</volume>
      <issue>1</issue>
      <issn>0066-4308, 1545-2085</issn>
      <uri>https://www.annualreviews.org/doi/10.1146/annurev-psych-040323-115131</uri>
      <pub-id pub-id-type="doi">10.1146/annurev-psych-040323-115131</pub-id>
      <fpage>215</fpage>
      <lpage>240</lpage>
    </element-citation>
  </ref>
  <ref id="ref-shepardSecondorderIsomorphismInternal1970-nb-article">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Shepard</surname><given-names>Roger N</given-names></name>
        <name><surname>Chipman</surname><given-names>Susan</given-names></name>
      </person-group>
      <article-title>Second-order isomorphism of internal representations: Shapes of states</article-title>
      <source>Cognitive Psychology</source>
      <year iso-8601-date="1970-01">1970</year><month>01</month>
      <volume>1</volume>
      <issue>1</issue>
      <issn>0010-0285</issn>
      <uri>https://www.sciencedirect.com/science/article/pii/0010028570900022</uri>
      <pub-id pub-id-type="doi">10.1016/0010-0285(70)90002-2</pub-id>
      <fpage>1</fpage>
      <lpage>17</lpage>
    </element-citation>
  </ref>
  <ref id="ref-sasakiToolboxGromovWassersteinOptimal2023-nb-article">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Sasaki</surname><given-names>Masaru</given-names></name>
        <name><surname>Takeda</surname><given-names>Ken</given-names></name>
        <name><surname>Abe</surname><given-names>Kota</given-names></name>
        <name><surname>Oizumi</surname><given-names>Masafumi</given-names></name>
      </person-group>
      <article-title>Toolbox for gromov-wasserstein optimal transport: Application to unsupervised alignment in neuroscience</article-title>
      <year iso-8601-date="2023">2023</year>
      <uri>https://www.biorxiv.org/content/10.1101/2023.09.15.558038v1</uri>
      <pub-id pub-id-type="doi">10.1101/2023.09.15.558038</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-bainbridgeQuantifyingAphantasiaDrawing2021-nb-article">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bainbridge</surname><given-names>Wilma A.</given-names></name>
        <name><surname>Pounder</surname><given-names>Zoë</given-names></name>
        <name><surname>Eardley</surname><given-names>Alison F.</given-names></name>
        <name><surname>Baker</surname><given-names>Chris I.</given-names></name>
      </person-group>
      <article-title>Quantifying aphantasia through drawing: Those without visual imagery show deficits in object but not spatial memory</article-title>
      <source>Cortex</source>
      <year iso-8601-date="2021-02-01">2021</year><month>02</month><day>01</day>
      <volume>135</volume>
      <uri>https://www.sciencedirect.com/science/article/pii/S0010945220304317</uri>
      <pub-id pub-id-type="doi">10.1016/j.cortex.2020.11.014</pub-id>
      <fpage>159</fpage>
      <lpage>172</lpage>
    </element-citation>
  </ref>
  <ref id="ref-kriegeskorteInverseMDSInferring2012-nb-article">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name>
        <name><surname>Mur</surname><given-names>Marieke</given-names></name>
      </person-group>
      <article-title>Inverse MDS: Inferring dissimilarity structure from multiple item arrangements</article-title>
      <source>Frontiers in Psychology</source>
      <year iso-8601-date="2012">2012</year>
      <volume>3</volume>
      <uri>https://www.frontiersin.org/articles/10.3389/fpsyg.2012.00245</uri>
    </element-citation>
  </ref>
  <ref id="ref-decockSimilarityGoodman2011-nb-article">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Decock</surname><given-names>Lieven</given-names></name>
        <name><surname>Douven</surname><given-names>Igor</given-names></name>
      </person-group>
      <article-title>Similarity After Goodman</article-title>
      <source>Review of Philosophy and Psychology</source>
      <year iso-8601-date="2011-03-01">2011</year><month>03</month><day>01</day>
      <volume>2</volume>
      <issue>1</issue>
      <uri>https://doi.org/10.1007/s13164-010-0035-y</uri>
      <pub-id pub-id-type="doi">10.1007/s13164-010-0035-y</pub-id>
      <fpage>61</fpage>
      <lpage>75</lpage>
    </element-citation>
  </ref>
  <ref id="ref-goodmanSevenStricturesSimilarity1972-nb-article">
    <element-citation publication-type="chapter">
      <person-group person-group-type="author">
        <name><surname>Goodman</surname><given-names>Nelson</given-names></name>
      </person-group>
      <article-title>Seven strictures on similarity</article-title>
      <publisher-name>Bobs-Merril</publisher-name>
      <year iso-8601-date="1972">1972</year>
    </element-citation>
  </ref>
  <ref id="ref-gardenforsConceptualSpacesFramework2004-nb-article">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Gardenfors</surname><given-names>Peter</given-names></name>
      </person-group>
      <article-title>Conceptual spaces as a framework for knowledge representation</article-title>
      <year iso-8601-date="2004">2004</year>
    </element-citation>
  </ref>
  <ref id="ref-murHumanObjectSimilarityJudgments2013-nb-article">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Mur</surname><given-names>Marieke</given-names></name>
        <name><surname>Meys</surname><given-names>Mirjam</given-names></name>
        <name><surname>Bodurka</surname><given-names>Jerzy</given-names></name>
        <name><surname>Goebel</surname><given-names>Rainer</given-names></name>
        <name><surname>Bandettini</surname><given-names>Peter</given-names></name>
        <name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name>
      </person-group>
      <article-title>Human object-similarity judgments reflect and transcend the primate-IT object representation</article-title>
      <source>Frontiers in Psychology</source>
      <year iso-8601-date="2013">2013</year>
      <volume>4</volume>
      <uri>https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00128</uri>
    </element-citation>
  </ref>
  <ref id="ref-majewskaSpatialMultiarrangementClustering2020-nb-article">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Majewska</surname><given-names>O.</given-names></name>
        <name><surname>McCarthy</surname><given-names>D.</given-names></name>
        <name><surname>Bosch</surname><given-names>J. van den</given-names></name>
        <name><surname>Kriegeskorte</surname><given-names>N.</given-names></name>
        <name><surname>Vulic</surname><given-names>I.</given-names></name>
        <name><surname>Korhonen</surname><given-names>A.</given-names></name>
      </person-group>
      <source>Spatial multi-arrangement for clustering and multi-way similarity dataset construction</source>
      <publisher-name>European Language Resources Association</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <uri>https://www.repository.cam.ac.uk/handle/1810/306834</uri>
    </element-citation>
  </ref>
</ref-list>
<fn-group>
  <fn id="fn1-nb-article">
    <label>1</label><p>A claim dismissed since then by propositions of
    robust mathematical models of similarity, e.g. Gardenfors
    (<xref alt="2004" rid="ref-gardenforsConceptualSpacesFramework2004-nb-article" ref-type="bibr">2004</xref>),
    Decock and Douven
    (<xref alt="2011" rid="ref-decockSimilarityGoodman2011-nb-article" ref-type="bibr">2011</xref>).</p>
  </fn>
</fn-group>
</back>


</sub-article>
<sub-article article-type="notebook" id="nb-4-nb-1">
<front-stub>
<title-group>
<article-title>Simulation code</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Delem</surname>
<given-names>Maël</given-names>
</name>
<string-name>Maël Delem</string-name>

</contrib>
</contrib-group>
</front-stub>

<body>
<boxed-text>
  <disp-quote>
    <p><bold>Packages and setup</bold></p>
    <p>Down below is the code to load necessary packages used for the
    simulation and analysis, along with some setups for the whole
    document (<italic>hover over the numbers on the far right for
    additional explanation of code and mechanics</italic>).</p>
    <sec id="nb-code-cell-1-nb-1" specific-use="notebook-code">
    <code id="annotated-cell-2-nb-1" language="r script">
# ═══ Packages ═════════════════════════════════════════════════════════════════

if (!require(librarian)) install.packages(librarian)
    </code>
    <def-list>
      <def-item>
        <term>Line 4</term>
        <def>
          <p>The package <monospace>librairian</monospace> eases package
          management with the “shelf” function, which automatically: (1)
          checks if a package is installed; (2) installs it if need be;
          (3) loads the package like the “library()” function would.</p>
        </def>
      </def-item>
      <def-item>
        <term>Line 28</term>
        <def>
          <p><monospace>reticulate</monospace> allows to translate and
          transfer objects and functions from R to Python and
          vice-versa, and was thus of primary importance for the
          successful use of the Python toolbox on our simulated
          data.</p>
        </def>
      </def-item>
      <def-item>
        <term>Lines 35,51</term>
        <def>
          <p>These are personal custom color palettes meant to extend my
          favourite palette, the color-atypical friendly Okabe-Ito color
          palette. The palette originally has only eight colors, but I
          will need nine, then up to 30 for later graphs, so I extended
          it with a hand-picked selection of mine.</p>
        </def>
      </def-item>
      <def-item>
        <term>Lines 55,69</term>
        <def>
          <p>These are R objects that were the results of a previous run
          of the simulation.</p>
        </def>
      </def-item>
    </def-list>
    <code id="annotated-cell-3-nb-1" language="r script">
library(librarian)

# now putting packages on our library's shelves:
shelf(
  # ─── data management ─────────────────
  holodeck,       # simulating multivariate data
  cluster,        # dissimilarity matrices
  
  # ─── modelling ───────────────────────
  mclust,         # mixture clustering
  
  #  data visualization ──────────────
  # palettes
  viridis,        # colour-blind friendly palettes
  # interactive
  plotly,         # interactive plots
  ggdendro,       # dendrograms
  seriation,      # dissimilarity plots
  webshot2,       # HTML screenshots for Word render
  webshot,
  
  # ─── essential package collections ───
  doParallel,     # parallel execution
  easystats,      # data analysis ecosystem
  reticulate,     # R to Python
  tidyverse,      # modern R ecosystem
)

# ─── Global cosmetic theme ───
theme_set(theme_modern(base_size = 14))

pal_okabe_ito &lt;- c(
  &quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;,                            
  &quot;#F5C710&quot;, &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;, &quot;#6c0009&quot;)      

# We'll need 9 colors at some point
pal_okabe_ito_extended &lt;- c(                                 
  &quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;,                           
  &quot;#F5C710&quot;, &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;, &quot;#6c0009&quot;, &quot;#414487FF&quot;)

# We'll need 30 colors at another moment
cool_30_colors &lt;- c(                                                   
  &quot;#3d51b4&quot;, &quot;#414487FF&quot;, &quot;#003d73&quot;, &quot;#440154FF&quot;, &quot;#6c0009&quot;, &quot;#b64e4e&quot;,
  &quot;#D55E00&quot;, &quot;#E69F00&quot;, &quot;#F5C710&quot;, &quot;#FDE725FF&quot;, &quot;#f2bb7b&quot;, &quot;#f1afad&quot;, &quot;#CC79A7&quot;, 
  &quot;#e57774&quot;, &quot;#7AD151FF&quot;, &quot;#57b571&quot;, &quot;#318a4a&quot;, &quot;#009E73&quot;, &quot;#22A884FF&quot;, 
  &quot;#2A788EFF&quot;, &quot;#0072B2&quot;, &quot;#2da6b5&quot;, &quot;#56B4E9&quot;, &quot;#889be0&quot;, &quot;#6677e0&quot;,   
  &quot;#3d51b4&quot;, &quot;#414487FF&quot;, &quot;#003d73&quot;, &quot;#440154FF&quot;, &quot;#6c0009&quot;, &quot;#b64e4e&quot;  
  )

path = &quot;data/&quot;

df &lt;- read_rds(paste0(path, &quot;df.RDS&quot;))
df_embeds &lt;- read_rds(paste0(path, &quot;df_embeds.RDS&quot;))
# Categorical and visual embeddings
df_embeds_categ  &lt;- read_rds(paste0(path, &quot;df_embeds_categ.RDS&quot;))
df_embeds_visual &lt;- read_rds(paste0(path, &quot;df_embeds_visual.RDS&quot;))
# Subject embeddings per sub-group
df_embed_c_sub  &lt;- read_rds(paste0(path, &quot;df_embed_c_sub.RDS&quot;))
df_embed_cs_sub &lt;- read_rds(paste0(path, &quot;df_embed_cs_sub.RDS&quot;))
df_embed_v_sub  &lt;- read_rds(paste0(path, &quot;df_embed_v_sub.RDS&quot;))
df_embed_vs_sub &lt;- read_rds(paste0(path, &quot;df_embed_vs_sub.RDS&quot;))
# Accuracy of the unsupervised alignment (bad = not tidy data)
df_accuracy_all_bad &lt;- read_rds(paste0(path, &quot;df_accuracy_all_bad.RDS&quot;))
df_accuracy_cat_bad &lt;- read_rds(paste0(path, &quot;df_accuracy_cat_bad.RDS&quot;))
# Coordinates of the aligned embeddings from the Python output
coordinates_aligned_embeddings &lt;- read_rds(paste0(path, &quot;coordinates_aligned_embeddings.RDS&quot;))
    </code>
    <sec id="nb-code-cell-1-output-0-nb-1" specific-use="notebook-output">
    <preformat>Le chargement a nécessité le package : librarian</preformat>
    </sec>
    </sec>
  </disp-quote>
</boxed-text>
<sec id="nb-code-cell-2-nb-1" specific-use="notebook-code">
<code language="r script">
# The function takes the variance and covariance of the imagery distributions
# as arguments
generate_osv_model &lt;- function(var, cov){
  df &lt;- 
    tibble(group = rep(c(&quot;aph&quot;, &quot;phant&quot;), each = 8)) |&gt; 
    group_by(group) |&gt; 
    mutate(
      spatial_group = c(rep(&quot;spa_low&quot;, 4), rep(&quot;spa_high&quot;, 4)),
      vis_spa_group = paste0(group, &quot;_&quot;, spatial_group),
      verbal_group = &quot;verbal_low&quot;,
      verbal_group  = case_when(
        vis_spa_group == &quot;aph_spa_low&quot; ~ &quot;verbal_high&quot;, 
        vis_spa_group == &quot;phant_spa_low&quot; ~ &quot;verbal_mid&quot;,
        TRUE ~ verbal_group)
    ) |&gt; 
    group_by(vis_spa_group) |&gt; 
    # ─── visual ───
    sim_discr(
      n_vars = 1, 
      var = var, 
      cov = cov, 
      # aph_s, aph_v, phant_s, phant_v
      group_means = c(0, 0, 0.6, 0.87), 
      name = &quot;v&quot;) |&gt; 
    # ─── spatial ───
    sim_discr(
      n_vars = 1,  
      var = var, 
      cov = cov, 
      # aph_s, aph_v, phant_s, phant_v
      group_means = c(0.75, 0.4, 0.7, 0.3), 
      name = &quot;s&quot;) |&gt;
    # ─── verbal ───
    sim_discr(
      n_vars = 1,  
      var = var, 
      cov = cov, 
      # aph_s, aph_v, phant_s, phant_v
      group_means = c(0.3, 0.7, 0.3, 0.5), 
      name = &quot;i&quot;) |&gt;
    rename(
      visual_imagery  = v_1,
      spatial_imagery = s_1,
      verbal_profile  = i_1
      )
}

df &lt;- generate_osv_model(0.03, 0)
</code>
</sec>
</body>



<back>
</back>


</sub-article>
<sub-article article-type="notebook" id="nb-8-nb-2">
<front-stub>
<title-group>
<article-title>Unsupervised alignment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Delem</surname>
<given-names>Maël</given-names>
</name>
<string-name>Maël Delem</string-name>

</contrib>
</contrib-group>
</front-stub>

<body>
<boxed-text>
  <disp-quote>
    <p><bold>Packages and setup</bold></p>
    <p>Down below is the code to load necessary packages used for the
    simulation and analysis, along with some setups for the whole
    document (<italic>hover over the numbers on the far right for
    additional explanation of code and mechanics</italic>).</p>
    <sec id="nb-code-cell-1-nb-2" specific-use="notebook-code">
    <code id="annotated-cell-1-nb-2" language="r script">
# ═══ Packages ═════════════════════════════════════════════════════════════════

if (!require(librarian)) install.packages(librarian)
    </code>
    <def-list>
      <def-item>
        <term>Line 4</term>
        <def>
          <p>The package <monospace>librairian</monospace> eases package
          management with the “shelf” function, which automatically: (1)
          checks if a package is installed; (2) installs it if need be;
          (3) loads the package like the “library()” function would.</p>
        </def>
      </def-item>
      <def-item>
        <term>Line 28</term>
        <def>
          <p><monospace>reticulate</monospace> allows to translate and
          transfer objects and functions from R to Python and
          vice-versa, and was thus of primary importance for the
          successful use of the Python toolbox on our simulated
          data.</p>
        </def>
      </def-item>
      <def-item>
        <term>Lines 35,51</term>
        <def>
          <p>These are personal custom color palettes meant to extend my
          favourite palette, the color-atypical friendly Okabe-Ito color
          palette. The palette originally has only eight colors, but I
          will need nine, then up to 30 for later graphs, so I extended
          it with a hand-picked selection of mine.</p>
        </def>
      </def-item>
    </def-list>
    <code id="annotated-cell-2-nb-2" language="r script">
library(librarian)

# now putting packages on our library's shelves:
shelf(
  # ─── data management ─────────────────
  holodeck,       # simulating multivariate data
  cluster,        # dissimilarity matrices
  
  # ─── modelling ───────────────────────
  mclust,         # mixture clustering
  
  #  data visualization ──────────────
  # palettes
  viridis,        # colour-blind friendly palettes
  # interactive
  plotly,         # interactive plots
  ggdendro,       # dendrograms
  seriation,      # dissimilarity plots
  webshot2,       # HTML screenshots for Word render
  webshot,
  
  # ─── essential package collections ───
  doParallel,     # parallel execution
  easystats,      # data analysis ecosystem
  reticulate,     # R to Python
  tidyverse,      # modern R ecosystem
)

# ─── Global cosmetic theme ───
theme_set(theme_modern(base_size = 14))

pal_okabe_ito &lt;- c(
  &quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;,                            
  &quot;#F5C710&quot;, &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;, &quot;#6c0009&quot;)      

# We'll need 9 colors at some point
pal_okabe_ito_extended &lt;- c(                                 
  &quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;,                           
  &quot;#F5C710&quot;, &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;, &quot;#6c0009&quot;, &quot;#414487FF&quot;)

# We'll need 30 colors at another moment
cool_30_colors &lt;- c(                                                   
  &quot;#3d51b4&quot;, &quot;#414487FF&quot;, &quot;#003d73&quot;, &quot;#440154FF&quot;, &quot;#6c0009&quot;, &quot;#b64e4e&quot;,
  &quot;#D55E00&quot;, &quot;#E69F00&quot;, &quot;#F5C710&quot;, &quot;#FDE725FF&quot;, &quot;#f2bb7b&quot;, &quot;#f1afad&quot;, &quot;#CC79A7&quot;, 
  &quot;#e57774&quot;, &quot;#7AD151FF&quot;, &quot;#57b571&quot;, &quot;#318a4a&quot;, &quot;#009E73&quot;, &quot;#22A884FF&quot;, 
  &quot;#2A788EFF&quot;, &quot;#0072B2&quot;, &quot;#2da6b5&quot;, &quot;#56B4E9&quot;, &quot;#889be0&quot;, &quot;#6677e0&quot;,   
  &quot;#3d51b4&quot;, &quot;#414487FF&quot;, &quot;#003d73&quot;, &quot;#440154FF&quot;, &quot;#6c0009&quot;, &quot;#b64e4e&quot;  
  )
    </code>
    <sec id="nb-code-cell-1-output-0-nb-2" specific-use="notebook-output">
    <preformat>Le chargement a nécessité le package : librarian</preformat>
    </sec>
    </sec>
  </disp-quote>
</boxed-text>
</body>



<back>
</back>


</sub-article>

</article>